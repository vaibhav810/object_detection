2018-05-01 10:56:09.381347: step 0, loss = 7116.50 (0.7 examples/sec; 21.655 sec/batch)
2018-05-01 10:57:07.938051: step 10, loss = 155.42 (3.1 examples/sec; 5.147 sec/batch)
2018-05-01 10:57:59.257023: step 20, loss = 81.51 (3.2 examples/sec; 5.070 sec/batch)
2018-05-01 10:58:50.291176: step 30, loss = 44.13 (3.2 examples/sec; 5.046 sec/batch)
2018-05-01 10:59:41.715687: step 40, loss = 41.68 (3.1 examples/sec; 5.137 sec/batch)
2018-05-01 11:00:33.279490: step 50, loss = 33.53 (3.1 examples/sec; 5.177 sec/batch)
2018-05-01 11:01:24.321754: step 60, loss = 33.97 (3.1 examples/sec; 5.182 sec/batch)
2018-05-01 11:02:12.685814: step 70, loss = 34.57 (3.2 examples/sec; 5.031 sec/batch)
2018-05-01 11:03:03.839085: step 80, loss = 34.74 (3.1 examples/sec; 5.190 sec/batch)
2018-05-01 11:03:54.597663: step 90, loss = 30.10 (3.1 examples/sec; 5.086 sec/batch)
2018-05-01 11:04:46.413351: step 100, loss = 31.75 (3.2 examples/sec; 5.071 sec/batch)
2018-05-01 11:05:41.736815: step 110, loss = 29.55 (3.0 examples/sec; 5.307 sec/batch)
2018-05-01 11:06:32.422275: step 120, loss = 29.83 (3.2 examples/sec; 5.004 sec/batch)
2018-05-01 11:07:23.664620: step 130, loss = 32.85 (3.2 examples/sec; 5.011 sec/batch)
2018-05-01 11:08:14.896345: step 140, loss = 31.44 (3.1 examples/sec; 5.102 sec/batch)
2018-05-01 11:09:06.675742: step 150, loss = 29.61 (3.2 examples/sec; 5.054 sec/batch)
2018-05-01 11:09:57.664729: step 160, loss = 30.31 (3.1 examples/sec; 5.111 sec/batch)
2018-05-01 11:10:48.822877: step 170, loss = 31.28 (3.1 examples/sec; 5.177 sec/batch)
2018-05-01 11:11:39.989666: step 180, loss = 31.81 (3.2 examples/sec; 5.056 sec/batch)
2018-05-01 11:12:28.221734: step 190, loss = 29.80 (3.2 examples/sec; 5.023 sec/batch)
2018-05-01 11:13:19.121714: step 200, loss = 27.57 (3.1 examples/sec; 5.213 sec/batch)
2018-05-01 11:14:14.250204: step 210, loss = 29.12 (3.1 examples/sec; 5.082 sec/batch)
2018-05-01 11:15:05.063802: step 220, loss = 29.83 (3.1 examples/sec; 5.129 sec/batch)
2018-05-01 11:15:56.463586: step 230, loss = 29.84 (3.1 examples/sec; 5.157 sec/batch)
2018-05-01 11:16:47.954309: step 240, loss = 31.39 (3.1 examples/sec; 5.104 sec/batch)
2018-05-01 11:17:39.335420: step 250, loss = 30.81 (3.1 examples/sec; 5.198 sec/batch)
2018-05-01 11:18:30.465195: step 260, loss = 29.46 (3.1 examples/sec; 5.138 sec/batch)
2018-05-01 11:19:21.597250: step 270, loss = 29.11 (3.1 examples/sec; 5.162 sec/batch)
2018-05-01 11:20:13.147952: step 280, loss = 30.95 (3.2 examples/sec; 5.011 sec/batch)
2018-05-01 11:21:04.735519: step 290, loss = 29.84 (3.2 examples/sec; 5.062 sec/batch)
2018-05-01 11:21:55.805029: step 300, loss = 31.63 (3.2 examples/sec; 4.963 sec/batch)
2018-05-01 11:22:47.908808: step 310, loss = 31.59 (3.1 examples/sec; 5.113 sec/batch)
2018-05-01 11:23:38.869454: step 320, loss = 30.30 (3.1 examples/sec; 5.101 sec/batch)
2018-05-01 11:24:29.937547: step 330, loss = 29.28 (3.1 examples/sec; 5.105 sec/batch)
2018-05-01 11:25:20.897911: step 340, loss = 32.50 (3.2 examples/sec; 4.998 sec/batch)
2018-05-01 11:26:11.871257: step 350, loss = 28.75 (3.1 examples/sec; 5.103 sec/batch)
2018-05-01 11:27:03.598724: step 360, loss = 30.58 (3.2 examples/sec; 5.058 sec/batch)
2018-05-01 11:27:54.690243: step 370, loss = 29.61 (3.2 examples/sec; 5.039 sec/batch)
2018-05-01 11:28:46.050767: step 380, loss = 30.55 (3.2 examples/sec; 5.026 sec/batch)
2018-05-01 11:29:37.404065: step 390, loss = 28.14 (3.1 examples/sec; 5.085 sec/batch)
2018-05-01 11:30:28.178788: step 400, loss = 29.36 (3.2 examples/sec; 5.015 sec/batch)
2018-05-01 11:31:23.084162: step 410, loss = 30.28 (3.2 examples/sec; 5.074 sec/batch)
2018-05-01 11:32:14.761549: step 420, loss = 31.16 (2.9 examples/sec; 5.437 sec/batch)
2018-05-01 11:33:03.481544: step 430, loss = 30.60 (3.2 examples/sec; 5.062 sec/batch)
2018-05-01 11:33:54.569681: step 440, loss = 30.99 (3.1 examples/sec; 5.178 sec/batch)
2018-05-01 11:34:46.097118: step 450, loss = 29.10 (3.2 examples/sec; 5.047 sec/batch)
2018-05-01 11:35:37.671026: step 460, loss = 31.03 (3.1 examples/sec; 5.099 sec/batch)
2018-05-01 11:36:28.846050: step 470, loss = 31.30 (3.1 examples/sec; 5.098 sec/batch)
2018-05-01 11:37:20.103720: step 480, loss = 28.95 (3.1 examples/sec; 5.134 sec/batch)
2018-05-01 11:38:11.707291: step 490, loss = 30.76 (3.1 examples/sec; 5.166 sec/batch)
2018-05-01 11:39:03.036809: step 500, loss = 29.33 (3.2 examples/sec; 5.065 sec/batch)
2018-05-01 11:39:57.269113: step 510, loss = 30.58 (3.1 examples/sec; 5.083 sec/batch)
2018-05-01 11:40:47.583085: step 520, loss = 29.05 (3.1 examples/sec; 5.126 sec/batch)
2018-05-01 11:41:41.216713: step 530, loss = 29.07 (3.1 examples/sec; 5.229 sec/batch)
2018-05-01 11:42:35.014735: step 540, loss = 32.00 (3.0 examples/sec; 5.251 sec/batch)
2018-05-01 11:43:27.026886: step 550, loss = 33.10 (2.9 examples/sec; 5.508 sec/batch)
2018-05-01 11:44:37.757212: step 560, loss = 29.24 (2.1 examples/sec; 7.607 sec/batch)
2018-05-01 11:45:57.804708: step 570, loss = 31.08 (2.0 examples/sec; 7.839 sec/batch)
2018-05-01 11:47:17.920471: step 580, loss = 31.36 (2.0 examples/sec; 7.851 sec/batch)
2018-05-01 11:48:22.335060: step 590, loss = 29.65 (3.1 examples/sec; 5.130 sec/batch)
2018-05-01 11:49:13.697042: step 600, loss = 30.95 (3.2 examples/sec; 5.065 sec/batch)
2018-05-01 11:50:14.895327: step 610, loss = 27.57 (3.0 examples/sec; 5.261 sec/batch)
2018-05-01 11:51:21.859369: step 620, loss = 28.58 (2.2 examples/sec; 7.353 sec/batch)
2018-05-01 11:52:24.969039: step 630, loss = 30.07 (2.4 examples/sec; 6.618 sec/batch)
2018-05-01 11:53:17.274779: step 640, loss = 27.71 (3.2 examples/sec; 5.049 sec/batch)
2018-05-01 11:54:09.182693: step 650, loss = 30.08 (2.9 examples/sec; 5.476 sec/batch)
2018-05-01 11:54:57.877029: step 660, loss = 29.56 (3.2 examples/sec; 4.950 sec/batch)
2018-05-01 11:55:48.792302: step 670, loss = 29.95 (3.1 examples/sec; 5.220 sec/batch)
2018-05-01 11:56:39.732581: step 680, loss = 30.33 (3.0 examples/sec; 5.259 sec/batch)
2018-05-01 11:57:30.272483: step 690, loss = 34.00 (3.2 examples/sec; 5.054 sec/batch)
2018-05-01 11:58:20.603947: step 700, loss = 28.38 (3.1 examples/sec; 5.105 sec/batch)
2018-05-01 11:59:14.626646: step 710, loss = 31.66 (3.2 examples/sec; 4.953 sec/batch)
2018-05-01 12:00:05.409566: step 720, loss = 28.86 (3.2 examples/sec; 5.040 sec/batch)
2018-05-01 12:00:56.742657: step 730, loss = 29.93 (3.1 examples/sec; 5.203 sec/batch)
2018-05-01 12:01:47.669866: step 740, loss = 28.16 (3.1 examples/sec; 5.097 sec/batch)
2018-05-01 12:02:39.114668: step 750, loss = 29.16 (3.0 examples/sec; 5.391 sec/batch)
2018-05-01 12:03:30.115839: step 760, loss = 30.02 (3.1 examples/sec; 5.175 sec/batch)
2018-05-01 12:04:21.418346: step 770, loss = 28.17 (3.2 examples/sec; 5.021 sec/batch)
2018-05-01 12:05:09.880804: step 780, loss = 27.90 (3.2 examples/sec; 5.079 sec/batch)
2018-05-01 12:06:00.551889: step 790, loss = 27.05 (3.2 examples/sec; 5.007 sec/batch)
2018-05-01 12:06:51.483288: step 800, loss = 28.83 (3.1 examples/sec; 5.148 sec/batch)
2018-05-01 12:07:45.764684: step 810, loss = 29.80 (3.1 examples/sec; 5.159 sec/batch)
2018-05-01 12:08:37.123424: step 820, loss = 30.59 (3.0 examples/sec; 5.320 sec/batch)
2018-05-01 12:09:27.798182: step 830, loss = 30.32 (3.3 examples/sec; 4.878 sec/batch)
2018-05-01 12:10:19.181183: step 840, loss = 27.77 (3.2 examples/sec; 5.018 sec/batch)
2018-05-01 12:11:09.822492: step 850, loss = 27.90 (3.2 examples/sec; 5.024 sec/batch)
2018-05-01 12:12:00.873012: step 860, loss = 31.14 (3.1 examples/sec; 5.119 sec/batch)
2018-05-01 12:12:51.617245: step 870, loss = 28.82 (3.3 examples/sec; 4.914 sec/batch)
2018-05-01 12:13:42.029157: step 880, loss = 29.51 (3.1 examples/sec; 5.131 sec/batch)
2018-05-01 12:14:32.807340: step 890, loss = 31.17 (3.1 examples/sec; 5.125 sec/batch)
2018-05-01 12:15:20.662067: step 900, loss = 29.54 (3.9 examples/sec; 4.061 sec/batch)
2018-05-01 12:16:14.395438: step 910, loss = 29.39 (3.2 examples/sec; 4.948 sec/batch)
2018-05-01 12:17:04.929896: step 920, loss = 30.44 (3.2 examples/sec; 5.064 sec/batch)
2018-05-01 12:17:55.632910: step 930, loss = 29.14 (3.1 examples/sec; 5.094 sec/batch)
2018-05-01 12:18:46.324938: step 940, loss = 28.39 (3.1 examples/sec; 5.161 sec/batch)
2018-05-01 12:19:36.561678: step 950, loss = 31.75 (3.2 examples/sec; 4.953 sec/batch)
2018-05-01 12:20:27.287365: step 960, loss = 30.29 (3.2 examples/sec; 5.032 sec/batch)
2018-05-01 12:21:18.117850: step 970, loss = 31.83 (3.1 examples/sec; 5.118 sec/batch)
2018-05-01 12:22:09.145239: step 980, loss = 28.98 (3.1 examples/sec; 5.158 sec/batch)
2018-05-01 12:22:59.424086: step 990, loss = 28.23 (3.2 examples/sec; 5.016 sec/batch)
2018-05-01 12:23:50.384702: step 1000, loss = 29.57 (3.2 examples/sec; 5.071 sec/batch)
2018-05-01 12:24:43.974817: step 1010, loss = 30.28 (3.3 examples/sec; 4.782 sec/batch)
2018-05-01 12:25:31.768426: step 1020, loss = 29.45 (3.8 examples/sec; 4.217 sec/batch)
2018-05-01 12:26:22.636969: step 1030, loss = 28.61 (3.1 examples/sec; 5.147 sec/batch)
2018-05-01 12:27:13.125886: step 1040, loss = 28.63 (3.1 examples/sec; 5.210 sec/batch)
2018-05-01 12:28:03.921293: step 1050, loss = 31.07 (3.2 examples/sec; 5.071 sec/batch)
2018-05-01 12:28:55.140724: step 1060, loss = 28.95 (3.2 examples/sec; 5.040 sec/batch)
2018-05-01 12:29:45.726530: step 1070, loss = 29.42 (3.2 examples/sec; 5.032 sec/batch)
2018-05-01 12:30:35.463590: step 1080, loss = 28.96 (3.1 examples/sec; 5.096 sec/batch)
2018-05-01 12:31:25.923210: step 1090, loss = 28.08 (3.2 examples/sec; 4.972 sec/batch)
2018-05-01 12:32:17.138446: step 1100, loss = 30.30 (3.1 examples/sec; 5.229 sec/batch)
2018-05-01 12:33:11.279276: step 1110, loss = 29.68 (3.2 examples/sec; 5.036 sec/batch)
2018-05-01 12:34:02.029615: step 1120, loss = 29.74 (3.0 examples/sec; 5.307 sec/batch)
2018-05-01 12:34:52.645489: step 1130, loss = 27.05 (3.2 examples/sec; 4.961 sec/batch)
2018-05-01 12:35:40.479502: step 1140, loss = 28.29 (4.0 examples/sec; 4.006 sec/batch)
2018-05-01 12:36:30.732503: step 1150, loss = 29.28 (3.2 examples/sec; 5.047 sec/batch)
2018-05-01 13:35:57.111382: step 0, loss = 8126.33 (0.7 examples/sec; 21.528 sec/batch)
2018-05-01 13:36:55.761512: step 10, loss = 181.51 (3.0 examples/sec; 5.301 sec/batch)
2018-05-01 13:37:47.560076: step 20, loss = 65.83 (3.1 examples/sec; 5.141 sec/batch)
2018-05-01 13:38:39.162631: step 30, loss = 46.84 (3.1 examples/sec; 5.225 sec/batch)
2018-05-01 13:39:30.076312: step 40, loss = 37.90 (3.2 examples/sec; 4.932 sec/batch)
2018-05-01 13:40:21.778700: step 50, loss = 34.66 (3.1 examples/sec; 5.113 sec/batch)
2018-05-01 13:41:13.005284: step 60, loss = 34.32 (3.1 examples/sec; 5.128 sec/batch)
2018-05-01 13:42:01.584912: step 70, loss = 34.90 (3.1 examples/sec; 5.087 sec/batch)
2018-05-01 13:42:53.099458: step 80, loss = 34.55 (3.1 examples/sec; 5.139 sec/batch)
2018-05-01 13:43:44.564305: step 90, loss = 32.14 (3.1 examples/sec; 5.204 sec/batch)
2018-05-01 13:44:36.146645: step 100, loss = 30.96 (3.1 examples/sec; 5.097 sec/batch)
2018-05-01 13:45:31.490471: step 110, loss = 31.90 (3.1 examples/sec; 5.141 sec/batch)
2018-05-01 13:46:22.686991: step 120, loss = 31.20 (3.1 examples/sec; 5.110 sec/batch)
2018-05-01 13:47:14.576417: step 130, loss = 32.39 (3.1 examples/sec; 5.104 sec/batch)
2018-05-01 13:48:06.139203: step 140, loss = 29.31 (3.1 examples/sec; 5.088 sec/batch)
2018-05-01 13:48:58.019226: step 150, loss = 31.00 (3.1 examples/sec; 5.199 sec/batch)
2018-05-01 13:49:49.391557: step 160, loss = 28.39 (3.2 examples/sec; 5.001 sec/batch)
2018-05-01 13:50:40.666929: step 170, loss = 30.87 (3.1 examples/sec; 5.219 sec/batch)
2018-05-01 13:51:31.455700: step 180, loss = 30.68 (3.1 examples/sec; 5.157 sec/batch)
2018-05-01 13:52:20.273817: step 190, loss = 29.65 (3.1 examples/sec; 5.238 sec/batch)
2018-05-01 13:53:11.799578: step 200, loss = 30.66 (3.1 examples/sec; 5.150 sec/batch)
2018-05-01 13:54:06.376784: step 210, loss = 26.68 (3.2 examples/sec; 4.942 sec/batch)
2018-05-01 13:54:58.127450: step 220, loss = 28.02 (3.1 examples/sec; 5.102 sec/batch)
2018-05-01 13:55:49.188981: step 230, loss = 29.38 (3.2 examples/sec; 5.016 sec/batch)
2018-05-01 13:56:40.620011: step 240, loss = 29.97 (3.1 examples/sec; 5.095 sec/batch)
2018-05-01 13:57:32.004529: step 250, loss = 30.79 (3.1 examples/sec; 5.152 sec/batch)
2018-05-01 13:58:23.129015: step 260, loss = 31.83 (3.1 examples/sec; 5.173 sec/batch)
2018-05-01 13:59:14.787523: step 270, loss = 32.76 (3.2 examples/sec; 5.079 sec/batch)
2018-05-01 14:00:06.026287: step 280, loss = 32.84 (3.1 examples/sec; 5.103 sec/batch)
2018-05-01 14:00:57.590063: step 290, loss = 29.92 (3.1 examples/sec; 5.140 sec/batch)
2018-05-01 14:01:49.399888: step 300, loss = 28.61 (3.1 examples/sec; 5.084 sec/batch)
2018-05-01 14:02:42.752543: step 310, loss = 31.87 (3.0 examples/sec; 5.261 sec/batch)
2018-05-01 14:03:34.499367: step 320, loss = 31.50 (3.1 examples/sec; 5.175 sec/batch)
2018-05-01 14:04:25.681774: step 330, loss = 29.81 (3.1 examples/sec; 5.223 sec/batch)
2018-05-01 14:05:17.315956: step 340, loss = 29.62 (3.2 examples/sec; 5.055 sec/batch)
2018-05-01 14:06:09.350842: step 350, loss = 30.01 (3.1 examples/sec; 5.084 sec/batch)
2018-05-01 14:07:00.333924: step 360, loss = 29.57 (3.1 examples/sec; 5.107 sec/batch)
2018-05-01 14:07:52.371228: step 370, loss = 29.59 (3.1 examples/sec; 5.176 sec/batch)
2018-05-01 14:08:44.293663: step 380, loss = 30.58 (3.0 examples/sec; 5.342 sec/batch)
2018-05-01 14:09:36.017597: step 390, loss = 28.21 (3.1 examples/sec; 5.189 sec/batch)
2018-05-01 14:10:27.279242: step 400, loss = 28.58 (3.2 examples/sec; 5.009 sec/batch)
2018-05-01 14:11:22.112244: step 410, loss = 28.20 (3.2 examples/sec; 5.055 sec/batch)
2018-05-01 14:12:10.455996: step 420, loss = 28.39 (4.0 examples/sec; 3.988 sec/batch)
2018-05-01 14:13:01.307931: step 430, loss = 29.02 (3.2 examples/sec; 5.020 sec/batch)
2018-05-01 14:13:51.719974: step 440, loss = 29.97 (3.2 examples/sec; 5.071 sec/batch)
2018-05-01 14:14:42.667065: step 450, loss = 30.56 (3.1 examples/sec; 5.111 sec/batch)
2018-05-01 14:15:33.627440: step 460, loss = 31.03 (3.0 examples/sec; 5.264 sec/batch)
2018-05-01 14:16:24.591543: step 470, loss = 28.36 (3.2 examples/sec; 5.002 sec/batch)
2018-05-01 14:17:15.813279: step 480, loss = 30.20 (3.3 examples/sec; 4.918 sec/batch)
2018-05-01 14:18:06.657874: step 490, loss = 29.00 (3.1 examples/sec; 5.115 sec/batch)
2018-05-01 14:18:57.414522: step 500, loss = 29.26 (3.2 examples/sec; 4.998 sec/batch)
2018-05-01 14:19:51.670847: step 510, loss = 29.92 (3.1 examples/sec; 5.149 sec/batch)
2018-05-01 14:20:42.869276: step 520, loss = 32.34 (3.2 examples/sec; 5.002 sec/batch)
2018-05-01 14:21:33.823986: step 530, loss = 30.67 (3.1 examples/sec; 5.095 sec/batch)
2018-05-01 14:22:22.795568: step 540, loss = 30.92 (4.0 examples/sec; 3.976 sec/batch)
2018-05-01 14:23:13.578908: step 550, loss = 29.70 (3.0 examples/sec; 5.261 sec/batch)
2018-05-01 14:24:03.561551: step 560, loss = 29.34 (3.2 examples/sec; 5.026 sec/batch)
2018-05-01 14:24:54.587259: step 570, loss = 28.60 (3.1 examples/sec; 5.145 sec/batch)
2018-05-01 14:25:45.402886: step 580, loss = 29.14 (3.1 examples/sec; 5.160 sec/batch)
2018-05-01 14:26:36.555105: step 590, loss = 30.90 (3.2 examples/sec; 4.977 sec/batch)
2018-05-01 14:27:27.525310: step 600, loss = 28.19 (3.2 examples/sec; 4.930 sec/batch)
2018-05-01 14:28:21.708320: step 610, loss = 31.10 (3.2 examples/sec; 5.065 sec/batch)
2018-05-01 14:29:12.815738: step 620, loss = 27.47 (3.2 examples/sec; 5.026 sec/batch)
2018-05-01 14:30:03.482403: step 630, loss = 30.07 (3.2 examples/sec; 5.034 sec/batch)
2018-05-01 14:30:54.887015: step 640, loss = 30.22 (3.1 examples/sec; 5.187 sec/batch)
2018-05-01 14:31:46.351419: step 650, loss = 28.39 (3.1 examples/sec; 5.176 sec/batch)
2018-05-01 14:32:34.164052: step 660, loss = 28.55 (4.1 examples/sec; 3.891 sec/batch)
2018-05-01 14:33:24.322320: step 670, loss = 28.23 (3.3 examples/sec; 4.920 sec/batch)
2018-05-01 14:34:15.040036: step 680, loss = 29.92 (3.1 examples/sec; 5.106 sec/batch)
2018-05-01 14:35:06.073681: step 690, loss = 27.60 (3.1 examples/sec; 5.155 sec/batch)
2018-05-01 14:35:56.732116: step 700, loss = 28.48 (3.1 examples/sec; 5.155 sec/batch)
2018-05-01 14:36:49.876292: step 710, loss = 26.51 (3.2 examples/sec; 4.974 sec/batch)
2018-05-01 14:37:40.906085: step 720, loss = 28.66 (3.1 examples/sec; 5.183 sec/batch)
2018-05-01 14:38:31.272631: step 730, loss = 28.35 (3.2 examples/sec; 5.027 sec/batch)
2018-05-01 14:39:22.316417: step 740, loss = 30.63 (3.0 examples/sec; 5.249 sec/batch)
2018-05-01 14:40:12.830348: step 750, loss = 30.14 (3.1 examples/sec; 5.124 sec/batch)
2018-05-01 14:41:02.913003: step 760, loss = 28.74 (3.3 examples/sec; 4.883 sec/batch)
2018-05-01 14:41:53.505139: step 770, loss = 28.42 (3.0 examples/sec; 5.350 sec/batch)
2018-05-01 14:42:44.644651: step 780, loss = 30.31 (3.2 examples/sec; 4.949 sec/batch)
2018-05-01 14:43:33.167282: step 790, loss = 29.06 (3.2 examples/sec; 5.066 sec/batch)
2018-05-01 14:44:24.170940: step 800, loss = 27.13 (3.2 examples/sec; 5.034 sec/batch)
2018-05-01 14:45:18.771506: step 810, loss = 28.95 (3.0 examples/sec; 5.331 sec/batch)
2018-05-01 14:46:09.854065: step 820, loss = 30.70 (3.1 examples/sec; 5.110 sec/batch)
2018-05-01 14:47:00.673795: step 830, loss = 28.67 (3.0 examples/sec; 5.322 sec/batch)
2018-05-01 14:47:51.827755: step 840, loss = 26.99 (3.0 examples/sec; 5.304 sec/batch)
2018-05-01 14:48:42.781254: step 850, loss = 30.72 (3.1 examples/sec; 5.105 sec/batch)
2018-05-01 14:49:33.511957: step 860, loss = 28.68 (3.1 examples/sec; 5.199 sec/batch)
2018-05-01 14:50:24.086572: step 870, loss = 31.83 (3.2 examples/sec; 4.959 sec/batch)
2018-05-01 14:51:14.761543: step 880, loss = 27.84 (3.2 examples/sec; 5.037 sec/batch)
2018-05-01 14:52:05.460577: step 890, loss = 28.86 (3.2 examples/sec; 5.007 sec/batch)
2018-05-01 14:52:55.885610: step 900, loss = 26.95 (3.2 examples/sec; 5.040 sec/batch)
2018-05-01 14:53:49.170894: step 910, loss = 31.31 (3.1 examples/sec; 5.099 sec/batch)
2018-05-01 14:54:40.179227: step 920, loss = 27.91 (3.2 examples/sec; 5.030 sec/batch)
2018-05-01 14:55:30.815342: step 930, loss = 28.40 (3.1 examples/sec; 5.159 sec/batch)
2018-05-01 14:56:21.970950: step 940, loss = 29.79 (3.2 examples/sec; 5.056 sec/batch)
2018-05-01 14:57:13.079517: step 950, loss = 29.03 (3.1 examples/sec; 5.100 sec/batch)
2018-05-01 14:58:03.881928: step 960, loss = 28.75 (3.1 examples/sec; 5.157 sec/batch)
2018-05-01 14:58:54.518150: step 970, loss = 26.95 (3.2 examples/sec; 5.054 sec/batch)
2018-05-01 14:59:45.096882: step 980, loss = 29.29 (3.2 examples/sec; 5.062 sec/batch)
2018-05-01 15:00:35.304895: step 990, loss = 28.06 (3.3 examples/sec; 4.875 sec/batch)
2018-05-01 15:01:26.548347: step 1000, loss = 30.11 (3.1 examples/sec; 5.177 sec/batch)
2018-05-01 15:02:21.820653: step 1010, loss = 28.18 (3.1 examples/sec; 5.211 sec/batch)
2018-05-01 15:03:11.092467: step 1020, loss = 29.43 (4.2 examples/sec; 3.852 sec/batch)
2018-05-01 15:04:00.831656: step 1030, loss = 27.61 (3.1 examples/sec; 5.154 sec/batch)
2018-05-01 15:04:51.736248: step 1040, loss = 26.71 (3.1 examples/sec; 5.134 sec/batch)
2018-05-01 15:05:42.394838: step 1050, loss = 28.26 (3.1 examples/sec; 5.093 sec/batch)
2018-05-01 15:06:33.017577: step 1060, loss = 28.65 (3.3 examples/sec; 4.844 sec/batch)
2018-05-01 15:07:23.705013: step 1070, loss = 30.26 (3.2 examples/sec; 4.982 sec/batch)
2018-05-01 15:08:14.078852: step 1080, loss = 29.48 (3.1 examples/sec; 5.100 sec/batch)
2018-05-01 15:09:05.037699: step 1090, loss = 29.69 (3.1 examples/sec; 5.095 sec/batch)
2018-05-01 15:09:55.744852: step 1100, loss = 30.16 (3.1 examples/sec; 5.153 sec/batch)
2018-05-01 15:10:51.146139: step 1110, loss = 28.12 (3.1 examples/sec; 5.126 sec/batch)
2018-05-01 15:11:41.489690: step 1120, loss = 28.20 (3.2 examples/sec; 5.026 sec/batch)
2018-05-01 15:12:32.664368: step 1130, loss = 28.95 (3.1 examples/sec; 5.124 sec/batch)
2018-05-01 15:13:21.935130: step 1140, loss = 28.12 (4.1 examples/sec; 3.855 sec/batch)
2018-05-01 15:14:11.452305: step 1150, loss = 29.25 (3.0 examples/sec; 5.302 sec/batch)
2018-05-01 15:15:02.535057: step 1160, loss = 31.76 (3.1 examples/sec; 5.094 sec/batch)
2018-05-01 15:15:52.917414: step 1170, loss = 28.77 (3.1 examples/sec; 5.244 sec/batch)
2018-05-01 15:16:43.982834: step 1180, loss = 28.53 (3.2 examples/sec; 5.065 sec/batch)
2018-05-01 15:17:34.693980: step 1190, loss = 30.35 (3.1 examples/sec; 5.149 sec/batch)
2018-05-01 15:18:25.638532: step 1200, loss = 28.83 (3.1 examples/sec; 5.153 sec/batch)
2018-05-01 15:19:20.498688: step 1210, loss = 28.68 (3.2 examples/sec; 5.040 sec/batch)
2018-05-01 15:20:10.319110: step 1220, loss = 30.00 (3.3 examples/sec; 4.914 sec/batch)
2018-05-01 15:21:01.365252: step 1230, loss = 30.92 (3.1 examples/sec; 5.138 sec/batch)
2018-05-01 15:21:51.693711: step 1240, loss = 29.86 (3.2 examples/sec; 5.074 sec/batch)
2018-05-01 15:22:42.245652: step 1250, loss = 28.94 (3.1 examples/sec; 5.121 sec/batch)
2018-05-01 15:23:31.763462: step 1260, loss = 27.50 (4.2 examples/sec; 3.829 sec/batch)
2018-05-01 15:24:21.929197: step 1270, loss = 28.94 (3.2 examples/sec; 4.958 sec/batch)
2018-05-01 15:25:12.073527: step 1280, loss = 26.39 (3.2 examples/sec; 4.929 sec/batch)
2018-05-01 15:26:01.994434: step 1290, loss = 30.61 (3.1 examples/sec; 5.180 sec/batch)
2018-05-01 15:26:58.123812: step 1300, loss = 28.65 (2.6 examples/sec; 6.125 sec/batch)
2018-05-01 15:28:01.832824: step 1310, loss = 28.49 (2.8 examples/sec; 5.753 sec/batch)
2018-05-01 15:29:03.114391: step 1320, loss = 29.36 (2.5 examples/sec; 6.289 sec/batch)
2018-05-01 15:30:04.085579: step 1330, loss = 29.03 (2.6 examples/sec; 6.260 sec/batch)
2018-05-01 15:31:04.977469: step 1340, loss = 27.55 (2.5 examples/sec; 6.320 sec/batch)
2018-05-01 15:32:05.027737: step 1350, loss = 28.47 (2.6 examples/sec; 6.117 sec/batch)
2018-05-01 15:33:06.049717: step 1360, loss = 27.77 (2.7 examples/sec; 5.936 sec/batch)
2018-05-01 15:34:06.715837: step 1370, loss = 28.75 (2.7 examples/sec; 5.935 sec/batch)
2018-05-01 15:35:05.066276: step 1380, loss = 30.21 (2.6 examples/sec; 6.269 sec/batch)
2018-05-01 15:35:56.325013: step 1390, loss = 30.02 (3.1 examples/sec; 5.096 sec/batch)
2018-05-01 15:36:46.913400: step 1400, loss = 31.10 (3.1 examples/sec; 5.208 sec/batch)
2018-05-01 15:37:41.581512: step 1410, loss = 28.09 (3.2 examples/sec; 5.062 sec/batch)
2018-05-01 15:38:32.310725: step 1420, loss = 28.34 (3.2 examples/sec; 5.070 sec/batch)
2018-05-01 15:39:22.825204: step 1430, loss = 29.07 (3.1 examples/sec; 5.090 sec/batch)
2018-05-01 15:40:13.636494: step 1440, loss = 29.40 (3.3 examples/sec; 4.908 sec/batch)
2018-05-01 15:41:04.347893: step 1450, loss = 30.67 (3.2 examples/sec; 5.066 sec/batch)
2018-05-01 15:41:55.572863: step 1460, loss = 29.33 (3.2 examples/sec; 4.980 sec/batch)
2018-05-01 15:42:47.022124: step 1470, loss = 27.98 (3.1 examples/sec; 5.084 sec/batch)
2018-05-01 15:43:37.201116: step 1480, loss = 29.51 (3.2 examples/sec; 5.053 sec/batch)
2018-05-01 15:44:28.469052: step 1490, loss = 29.85 (3.1 examples/sec; 5.106 sec/batch)
2018-05-01 15:45:16.382738: step 1500, loss = 27.85 (3.2 examples/sec; 5.075 sec/batch)
2018-05-01 15:46:11.050491: step 1510, loss = 29.54 (3.1 examples/sec; 5.102 sec/batch)
2018-05-01 15:47:01.805418: step 1520, loss = 28.46 (3.2 examples/sec; 4.956 sec/batch)
2018-05-01 15:47:52.300573: step 1530, loss = 30.68 (3.2 examples/sec; 4.934 sec/batch)
2018-05-01 15:48:43.241327: step 1540, loss = 28.18 (3.1 examples/sec; 5.121 sec/batch)
2018-05-01 15:49:34.030845: step 1550, loss = 27.42 (3.1 examples/sec; 5.127 sec/batch)
2018-05-01 15:50:24.748365: step 1560, loss = 27.59 (3.2 examples/sec; 4.970 sec/batch)
2018-05-01 15:51:15.901731: step 1570, loss = 28.33 (3.3 examples/sec; 4.922 sec/batch)
2018-05-01 15:52:07.331943: step 1580, loss = 26.86 (3.2 examples/sec; 5.058 sec/batch)
2018-05-01 15:52:58.337022: step 1590, loss = 31.38 (3.2 examples/sec; 5.072 sec/batch)
2018-05-01 15:53:48.985045: step 1600, loss = 28.02 (3.2 examples/sec; 4.964 sec/batch)
2018-05-01 15:54:42.683914: step 1610, loss = 27.58 (3.1 examples/sec; 5.166 sec/batch)
2018-05-01 15:55:31.187536: step 1620, loss = 29.12 (3.2 examples/sec; 5.031 sec/batch)
2018-05-01 15:56:21.392855: step 1630, loss = 29.12 (3.2 examples/sec; 5.011 sec/batch)
2018-05-01 15:57:12.638414: step 1640, loss = 30.56 (3.0 examples/sec; 5.253 sec/batch)
2018-05-01 15:58:03.555218: step 1650, loss = 30.06 (3.2 examples/sec; 4.964 sec/batch)
2018-05-01 15:58:53.744212: step 1660, loss = 28.37 (3.2 examples/sec; 5.013 sec/batch)
2018-05-01 15:59:44.157596: step 1670, loss = 27.62 (3.2 examples/sec; 5.007 sec/batch)
2018-05-01 16:00:35.306678: step 1680, loss = 31.00 (3.2 examples/sec; 5.015 sec/batch)
2018-05-01 16:01:25.559144: step 1690, loss = 29.13 (3.2 examples/sec; 4.943 sec/batch)
2018-05-01 16:02:15.456523: step 1700, loss = 27.07 (3.1 examples/sec; 5.124 sec/batch)
2018-05-01 16:03:09.793302: step 1710, loss = 26.12 (3.2 examples/sec; 5.017 sec/batch)
2018-05-01 16:04:00.101258: step 1720, loss = 32.56 (3.2 examples/sec; 4.986 sec/batch)
2018-05-01 16:04:50.938338: step 1730, loss = 30.26 (3.2 examples/sec; 5.075 sec/batch)
2018-05-01 16:05:39.718522: step 1740, loss = 28.55 (3.1 examples/sec; 5.226 sec/batch)
2018-05-01 16:06:30.887902: step 1750, loss = 28.87 (3.2 examples/sec; 4.985 sec/batch)
2018-05-01 16:07:21.868156: step 1760, loss = 30.33 (3.0 examples/sec; 5.325 sec/batch)
2018-05-01 16:08:13.278056: step 1770, loss = 27.73 (3.2 examples/sec; 5.063 sec/batch)
2018-05-01 16:09:04.343800: step 1780, loss = 27.53 (3.0 examples/sec; 5.278 sec/batch)
2018-05-01 16:09:54.770847: step 1790, loss = 27.66 (3.1 examples/sec; 5.135 sec/batch)
2018-05-01 16:10:45.182628: step 1800, loss = 28.02 (3.2 examples/sec; 4.984 sec/batch)
2018-05-01 16:11:39.339785: step 1810, loss = 29.05 (3.1 examples/sec; 5.135 sec/batch)
2018-05-01 16:12:30.679468: step 1820, loss = 30.59 (3.2 examples/sec; 5.070 sec/batch)
2018-05-01 16:13:20.966784: step 1830, loss = 30.43 (3.2 examples/sec; 4.954 sec/batch)
2018-05-01 16:14:11.974010: step 1840, loss = 27.46 (3.1 examples/sec; 5.081 sec/batch)
2018-05-01 16:15:02.256894: step 1850, loss = 27.07 (3.1 examples/sec; 5.173 sec/batch)
2018-05-01 16:15:50.643160: step 1860, loss = 29.81 (3.1 examples/sec; 5.083 sec/batch)
2018-05-01 16:16:41.150939: step 1870, loss = 29.23 (3.3 examples/sec; 4.864 sec/batch)
2018-05-01 16:17:32.166084: step 1880, loss = 30.08 (3.1 examples/sec; 5.156 sec/batch)
2018-05-01 16:18:22.897202: step 1890, loss = 26.18 (3.0 examples/sec; 5.284 sec/batch)
2018-05-01 16:19:13.786856: step 1900, loss = 29.27 (3.2 examples/sec; 4.968 sec/batch)
2018-05-01 16:20:07.494880: step 1910, loss = 27.63 (3.1 examples/sec; 5.147 sec/batch)
2018-05-01 16:20:57.932341: step 1920, loss = 29.31 (3.1 examples/sec; 5.226 sec/batch)
2018-05-01 16:21:48.805255: step 1930, loss = 29.23 (3.2 examples/sec; 5.004 sec/batch)
2018-05-01 16:22:39.820800: step 1940, loss = 29.86 (3.2 examples/sec; 5.035 sec/batch)
2018-05-01 16:23:30.468025: step 1950, loss = 29.69 (3.2 examples/sec; 5.014 sec/batch)
2018-05-01 16:24:21.522354: step 1960, loss = 29.69 (3.2 examples/sec; 5.001 sec/batch)
2018-05-01 16:25:12.014864: step 1970, loss = 28.08 (3.1 examples/sec; 5.166 sec/batch)
2018-05-01 16:26:00.863521: step 1980, loss = 26.94 (3.2 examples/sec; 5.036 sec/batch)
2018-05-01 16:26:51.498267: step 1990, loss = 27.37 (3.1 examples/sec; 5.134 sec/batch)
2018-05-01 16:27:41.572759: step 2000, loss = 29.89 (3.2 examples/sec; 5.032 sec/batch)
2018-05-01 16:28:36.549850: step 2010, loss = 28.88 (3.1 examples/sec; 5.175 sec/batch)
2018-05-01 16:29:27.486061: step 2020, loss = 27.09 (3.2 examples/sec; 5.072 sec/batch)
2018-05-01 16:30:18.019967: step 2030, loss = 29.21 (3.2 examples/sec; 4.987 sec/batch)
2018-05-01 16:31:08.662551: step 2040, loss = 28.50 (3.2 examples/sec; 4.997 sec/batch)
2018-05-01 16:31:59.213762: step 2050, loss = 28.55 (3.1 examples/sec; 5.236 sec/batch)
2018-05-01 16:32:50.293780: step 2060, loss = 27.55 (3.3 examples/sec; 4.910 sec/batch)
2018-05-01 16:33:41.576799: step 2070, loss = 29.88 (3.1 examples/sec; 5.213 sec/batch)
2018-05-01 16:34:32.379462: step 2080, loss = 26.84 (3.2 examples/sec; 5.028 sec/batch)
2018-05-01 16:35:23.516996: step 2090, loss = 27.67 (3.2 examples/sec; 5.021 sec/batch)
2018-05-01 16:36:12.581316: step 2100, loss = 29.08 (3.2 examples/sec; 5.023 sec/batch)
2018-05-01 16:37:06.743450: step 2110, loss = 29.74 (3.1 examples/sec; 5.091 sec/batch)
2018-05-01 16:37:57.865941: step 2120, loss = 30.31 (3.2 examples/sec; 5.045 sec/batch)
2018-05-01 16:38:49.097523: step 2130, loss = 27.80 (3.1 examples/sec; 5.102 sec/batch)
2018-05-01 16:39:39.779281: step 2140, loss = 29.34 (3.1 examples/sec; 5.093 sec/batch)
2018-05-01 16:40:30.926795: step 2150, loss = 27.72 (3.2 examples/sec; 5.000 sec/batch)
2018-05-01 16:41:21.911233: step 2160, loss = 26.90 (3.1 examples/sec; 5.134 sec/batch)
2018-05-01 16:42:12.555502: step 2170, loss = 29.32 (3.2 examples/sec; 5.035 sec/batch)
2018-05-01 16:43:03.980622: step 2180, loss = 28.08 (3.2 examples/sec; 5.001 sec/batch)
2018-05-01 16:43:55.275281: step 2190, loss = 31.73 (3.0 examples/sec; 5.279 sec/batch)
2018-05-01 16:44:45.619371: step 2200, loss = 27.05 (3.1 examples/sec; 5.082 sec/batch)
2018-05-01 16:45:39.911973: step 2210, loss = 27.13 (3.2 examples/sec; 4.997 sec/batch)
2018-05-01 16:46:27.584095: step 2220, loss = 30.68 (3.2 examples/sec; 4.952 sec/batch)
2018-05-01 16:47:18.456744: step 2230, loss = 27.22 (3.2 examples/sec; 4.943 sec/batch)
2018-05-01 16:48:09.170307: step 2240, loss = 28.79 (3.3 examples/sec; 4.893 sec/batch)
2018-05-01 16:48:59.797067: step 2250, loss = 28.62 (3.2 examples/sec; 5.075 sec/batch)
2018-05-01 16:49:50.631818: step 2260, loss = 28.37 (3.1 examples/sec; 5.223 sec/batch)
2018-05-01 16:50:40.858469: step 2270, loss = 26.25 (3.2 examples/sec; 4.962 sec/batch)
2018-05-01 16:51:31.357472: step 2280, loss = 29.40 (3.2 examples/sec; 5.018 sec/batch)
2018-05-01 16:52:21.176433: step 2290, loss = 27.79 (3.3 examples/sec; 4.907 sec/batch)
2018-05-01 16:53:10.854393: step 2300, loss = 26.99 (3.2 examples/sec; 5.061 sec/batch)
2018-05-01 16:54:05.567234: step 2310, loss = 27.96 (3.1 examples/sec; 5.140 sec/batch)
2018-05-01 16:54:56.323527: step 2320, loss = 28.51 (3.0 examples/sec; 5.276 sec/batch)
2018-05-01 16:55:46.714076: step 2330, loss = 27.88 (3.2 examples/sec; 5.077 sec/batch)
2018-05-01 16:56:34.698934: step 2340, loss = 27.59 (3.2 examples/sec; 4.978 sec/batch)
2018-05-01 16:57:25.283434: step 2350, loss = 29.78 (3.1 examples/sec; 5.182 sec/batch)
2018-05-01 16:58:16.161385: step 2360, loss = 27.89 (3.2 examples/sec; 4.991 sec/batch)
2018-05-01 16:59:06.762528: step 2370, loss = 30.02 (3.1 examples/sec; 5.105 sec/batch)
2018-05-01 16:59:57.879601: step 2380, loss = 27.60 (3.0 examples/sec; 5.252 sec/batch)
2018-05-01 17:00:49.319180: step 2390, loss = 28.41 (3.2 examples/sec; 5.020 sec/batch)
2018-05-01 17:01:39.673280: step 2400, loss = 28.19 (3.3 examples/sec; 4.863 sec/batch)
2018-05-01 17:02:34.032605: step 2410, loss = 28.97 (3.2 examples/sec; 4.979 sec/batch)
2018-05-01 17:03:24.272703: step 2420, loss = 30.28 (3.3 examples/sec; 4.916 sec/batch)
2018-05-01 17:04:15.217245: step 2430, loss = 29.27 (3.1 examples/sec; 5.120 sec/batch)
2018-05-01 17:05:05.497238: step 2440, loss = 29.52 (3.0 examples/sec; 5.259 sec/batch)
2018-05-01 17:05:56.047311: step 2450, loss = 29.69 (3.2 examples/sec; 5.006 sec/batch)
2018-05-01 17:06:43.537862: step 2460, loss = 27.47 (3.1 examples/sec; 5.127 sec/batch)
2018-05-01 17:07:33.939664: step 2470, loss = 30.78 (3.2 examples/sec; 4.941 sec/batch)
2018-05-01 17:08:24.072985: step 2480, loss = 30.58 (3.2 examples/sec; 5.020 sec/batch)
2018-05-01 17:09:14.739023: step 2490, loss = 28.61 (3.1 examples/sec; 5.086 sec/batch)
2018-05-01 17:10:04.777453: step 2500, loss = 29.62 (3.1 examples/sec; 5.087 sec/batch)
2018-05-01 17:10:58.506482: step 2510, loss = 27.09 (3.2 examples/sec; 5.007 sec/batch)
2018-05-01 17:11:48.285829: step 2520, loss = 27.01 (3.3 examples/sec; 4.888 sec/batch)
2018-05-01 17:12:38.744277: step 2530, loss = 28.19 (3.2 examples/sec; 4.938 sec/batch)
2018-05-01 17:13:29.078840: step 2540, loss = 31.51 (3.1 examples/sec; 5.092 sec/batch)
2018-05-01 17:14:19.291819: step 2550, loss = 27.83 (3.2 examples/sec; 4.946 sec/batch)
2018-05-01 17:15:09.071227: step 2560, loss = 29.68 (3.2 examples/sec; 5.021 sec/batch)
2018-05-01 17:16:00.053511: step 2570, loss = 26.47 (3.2 examples/sec; 5.045 sec/batch)
2018-05-01 17:16:47.399279: step 2580, loss = 28.46 (3.2 examples/sec; 4.947 sec/batch)
2018-05-01 17:17:37.056776: step 2590, loss = 28.06 (3.3 examples/sec; 4.854 sec/batch)
2018-05-01 17:18:26.653382: step 2600, loss = 27.61 (3.4 examples/sec; 4.739 sec/batch)
2018-05-01 17:19:20.843210: step 2610, loss = 27.69 (3.1 examples/sec; 5.103 sec/batch)
2018-05-01 17:20:11.009428: step 2620, loss = 30.55 (3.3 examples/sec; 4.895 sec/batch)
2018-05-01 17:21:01.334317: step 2630, loss = 26.98 (3.2 examples/sec; 4.994 sec/batch)
2018-05-01 17:21:51.329309: step 2640, loss = 28.71 (3.3 examples/sec; 4.896 sec/batch)
2018-05-01 17:22:42.051338: step 2650, loss = 26.47 (3.3 examples/sec; 4.879 sec/batch)
2018-05-01 17:23:32.280552: step 2660, loss = 27.39 (3.3 examples/sec; 4.862 sec/batch)
2018-05-01 17:24:22.046572: step 2670, loss = 27.37 (3.2 examples/sec; 4.943 sec/batch)
2018-05-01 17:25:12.798428: step 2680, loss = 28.89 (3.1 examples/sec; 5.118 sec/batch)
2018-05-01 17:26:02.287565: step 2690, loss = 29.09 (3.2 examples/sec; 4.931 sec/batch)
2018-05-01 17:26:49.525593: step 2700, loss = 27.65 (3.1 examples/sec; 5.146 sec/batch)
2018-05-01 17:27:42.631866: step 2710, loss = 29.38 (3.2 examples/sec; 5.002 sec/batch)
2018-05-01 17:28:32.512223: step 2720, loss = 27.11 (3.2 examples/sec; 4.951 sec/batch)
2018-05-01 17:29:22.391288: step 2730, loss = 29.03 (3.3 examples/sec; 4.917 sec/batch)
2018-05-01 17:30:13.060401: step 2740, loss = 29.67 (3.1 examples/sec; 5.170 sec/batch)
2018-05-01 17:31:03.360287: step 2750, loss = 27.50 (3.3 examples/sec; 4.884 sec/batch)
2018-05-01 17:31:52.835705: step 2760, loss = 29.00 (3.2 examples/sec; 4.927 sec/batch)
2018-05-01 17:32:43.374300: step 2770, loss = 26.62 (3.1 examples/sec; 5.200 sec/batch)
2018-05-01 17:33:34.415210: step 2780, loss = 26.54 (3.3 examples/sec; 4.902 sec/batch)
2018-05-01 17:34:24.802226: step 2790, loss = 26.74 (3.2 examples/sec; 5.045 sec/batch)
2018-05-01 17:35:14.282476: step 2800, loss = 30.26 (3.2 examples/sec; 5.068 sec/batch)
2018-05-01 17:36:08.329874: step 2810, loss = 29.25 (3.3 examples/sec; 4.898 sec/batch)
2018-05-01 17:36:56.056078: step 2820, loss = 26.36 (3.2 examples/sec; 5.071 sec/batch)
2018-05-01 17:37:46.355524: step 2830, loss = 29.58 (3.1 examples/sec; 5.095 sec/batch)
2018-05-01 17:38:36.790007: step 2840, loss = 28.99 (3.1 examples/sec; 5.129 sec/batch)
2018-05-01 17:39:26.790101: step 2850, loss = 27.46 (3.2 examples/sec; 4.989 sec/batch)
2018-05-01 17:40:16.865328: step 2860, loss = 30.82 (3.2 examples/sec; 5.036 sec/batch)
2018-05-01 17:41:07.026504: step 2870, loss = 27.09 (3.2 examples/sec; 4.971 sec/batch)
2018-05-01 17:41:57.106547: step 2880, loss = 28.49 (3.0 examples/sec; 5.248 sec/batch)
2018-05-01 17:42:47.211584: step 2890, loss = 29.35 (3.1 examples/sec; 5.159 sec/batch)
2018-05-01 17:43:36.460404: step 2900, loss = 29.75 (3.3 examples/sec; 4.795 sec/batch)
2018-05-01 17:44:29.896233: step 2910, loss = 24.99 (3.1 examples/sec; 5.150 sec/batch)
2018-05-01 17:45:20.269379: step 2920, loss = 29.04 (3.3 examples/sec; 4.821 sec/batch)
2018-05-01 17:46:09.452596: step 2930, loss = 28.31 (3.2 examples/sec; 5.028 sec/batch)
2018-05-01 17:46:57.219570: step 2940, loss = 30.76 (4.2 examples/sec; 3.831 sec/batch)
2018-05-01 17:47:46.949936: step 2950, loss = 27.81 (3.3 examples/sec; 4.907 sec/batch)
2018-05-01 17:48:37.493377: step 2960, loss = 29.02 (3.2 examples/sec; 4.966 sec/batch)
2018-05-01 17:49:27.779673: step 2970, loss = 29.91 (3.1 examples/sec; 5.080 sec/batch)
2018-05-01 17:50:17.671109: step 2980, loss = 26.36 (3.1 examples/sec; 5.122 sec/batch)
2018-05-01 17:51:08.334823: step 2990, loss = 27.69 (3.1 examples/sec; 5.142 sec/batch)
2018-05-01 17:51:58.558770: step 3000, loss = 25.47 (3.2 examples/sec; 4.987 sec/batch)
2018-05-01 17:52:52.146013: step 3010, loss = 30.65 (3.2 examples/sec; 5.053 sec/batch)
2018-05-01 17:53:42.761898: step 3020, loss = 29.37 (3.2 examples/sec; 5.011 sec/batch)
2018-05-01 17:54:32.996451: step 3030, loss = 30.54 (3.2 examples/sec; 4.973 sec/batch)
2018-05-01 17:55:22.685044: step 3040, loss = 27.16 (3.2 examples/sec; 4.989 sec/batch)
2018-05-01 17:56:13.103641: step 3050, loss = 29.34 (3.1 examples/sec; 5.087 sec/batch)
2018-05-01 17:57:01.965893: step 3060, loss = 29.66 (4.3 examples/sec; 3.759 sec/batch)
2018-05-01 17:57:50.779127: step 3070, loss = 26.51 (3.3 examples/sec; 4.918 sec/batch)
2018-05-01 17:58:41.340802: step 3080, loss = 29.71 (3.2 examples/sec; 4.939 sec/batch)
2018-05-01 17:59:31.019909: step 3090, loss = 28.36 (3.2 examples/sec; 5.068 sec/batch)
2018-05-01 18:00:20.871695: step 3100, loss = 26.20 (3.2 examples/sec; 5.064 sec/batch)
2018-05-01 18:01:14.926331: step 3110, loss = 27.23 (3.1 examples/sec; 5.192 sec/batch)
2018-05-01 18:02:04.995137: step 3120, loss = 26.22 (3.2 examples/sec; 5.030 sec/batch)
2018-05-01 18:02:54.804349: step 3130, loss = 28.92 (3.1 examples/sec; 5.220 sec/batch)
2018-05-01 18:03:45.084245: step 3140, loss = 30.43 (3.2 examples/sec; 5.043 sec/batch)
2018-05-01 18:04:34.930976: step 3150, loss = 28.38 (3.3 examples/sec; 4.820 sec/batch)
2018-05-01 18:05:24.767391: step 3160, loss = 30.03 (3.2 examples/sec; 4.971 sec/batch)
2018-05-01 18:06:14.751082: step 3170, loss = 27.53 (3.2 examples/sec; 5.040 sec/batch)
2018-05-01 18:07:05.017346: step 3180, loss = 27.80 (3.2 examples/sec; 5.018 sec/batch)
2018-05-01 18:07:52.502357: step 3190, loss = 25.95 (3.2 examples/sec; 4.934 sec/batch)
2018-05-01 18:08:42.208181: step 3200, loss = 28.15 (3.2 examples/sec; 5.046 sec/batch)
2018-05-01 18:09:36.066608: step 3210, loss = 28.11 (3.2 examples/sec; 5.020 sec/batch)
2018-05-01 18:10:26.063820: step 3220, loss = 29.61 (3.3 examples/sec; 4.874 sec/batch)
2018-05-01 18:11:15.780006: step 3230, loss = 24.56 (3.3 examples/sec; 4.863 sec/batch)
2018-05-01 18:12:05.376480: step 3240, loss = 28.78 (3.3 examples/sec; 4.877 sec/batch)
2018-05-01 18:12:55.762487: step 3250, loss = 26.45 (3.2 examples/sec; 4.943 sec/batch)
2018-05-01 18:13:46.548099: step 3260, loss = 28.95 (3.0 examples/sec; 5.250 sec/batch)
2018-05-01 18:14:36.710677: step 3270, loss = 26.19 (3.1 examples/sec; 5.113 sec/batch)
2018-05-01 18:15:26.356992: step 3280, loss = 30.65 (3.4 examples/sec; 4.774 sec/batch)
2018-05-01 18:16:15.665527: step 3290, loss = 27.91 (3.2 examples/sec; 5.052 sec/batch)
2018-05-01 18:17:06.091298: step 3300, loss = 28.66 (3.1 examples/sec; 5.188 sec/batch)
2018-05-01 18:17:57.404293: step 3310, loss = 27.81 (3.2 examples/sec; 4.932 sec/batch)
2018-05-01 18:18:47.481795: step 3320, loss = 29.17 (3.3 examples/sec; 4.817 sec/batch)
2018-05-01 18:19:36.485871: step 3330, loss = 27.46 (3.2 examples/sec; 4.952 sec/batch)
2018-05-01 18:20:26.228543: step 3340, loss = 26.97 (3.1 examples/sec; 5.140 sec/batch)
2018-05-01 18:21:16.569772: step 3350, loss = 27.35 (3.3 examples/sec; 4.917 sec/batch)
2018-05-01 18:22:06.219054: step 3360, loss = 26.81 (3.2 examples/sec; 5.023 sec/batch)
2018-05-01 18:22:56.771343: step 3370, loss = 27.21 (3.2 examples/sec; 5.050 sec/batch)
2018-05-01 18:23:47.265176: step 3380, loss = 28.76 (3.2 examples/sec; 4.946 sec/batch)
2018-05-01 18:24:37.661523: step 3390, loss = 26.06 (3.2 examples/sec; 5.023 sec/batch)
2018-05-01 18:25:27.727953: step 3400, loss = 27.06 (3.1 examples/sec; 5.136 sec/batch)
2018-05-01 18:26:21.125525: step 3410, loss = 29.48 (3.3 examples/sec; 4.905 sec/batch)
2018-05-01 18:27:11.593220: step 3420, loss = 26.40 (3.1 examples/sec; 5.182 sec/batch)
2018-05-01 18:27:59.337733: step 3430, loss = 26.85 (3.3 examples/sec; 4.873 sec/batch)
2018-05-01 18:28:48.832885: step 3440, loss = 28.39 (3.4 examples/sec; 4.706 sec/batch)
2018-05-01 18:29:38.248227: step 3450, loss = 27.25 (3.2 examples/sec; 5.056 sec/batch)
2018-05-01 18:30:28.129174: step 3460, loss = 27.42 (3.1 examples/sec; 5.134 sec/batch)
2018-05-01 18:31:18.680437: step 3470, loss = 29.27 (3.2 examples/sec; 5.007 sec/batch)
2018-05-01 18:32:08.905835: step 3480, loss = 28.22 (3.2 examples/sec; 5.020 sec/batch)
2018-05-01 18:32:59.369312: step 3490, loss = 29.04 (3.1 examples/sec; 5.101 sec/batch)
2018-05-01 18:33:49.616308: step 3500, loss = 29.36 (3.2 examples/sec; 4.997 sec/batch)
2018-05-01 18:34:44.021283: step 3510, loss = 27.21 (3.0 examples/sec; 5.296 sec/batch)
2018-05-01 18:35:34.284714: step 3520, loss = 27.15 (3.2 examples/sec; 4.937 sec/batch)
2018-05-01 18:36:23.697795: step 3530, loss = 29.39 (3.3 examples/sec; 4.840 sec/batch)
2018-05-01 18:37:13.406743: step 3540, loss = 26.09 (3.2 examples/sec; 4.940 sec/batch)
2018-05-01 18:37:59.556218: step 3550, loss = 26.86 (3.4 examples/sec; 4.686 sec/batch)
2018-05-01 18:38:49.612948: step 3560, loss = 28.93 (3.2 examples/sec; 4.944 sec/batch)
2018-05-01 18:39:38.521652: step 3570, loss = 25.16 (3.3 examples/sec; 4.816 sec/batch)
2018-05-01 18:40:27.930902: step 3580, loss = 25.69 (3.2 examples/sec; 4.931 sec/batch)
2018-05-01 18:41:18.050974: step 3590, loss = 28.21 (3.2 examples/sec; 4.956 sec/batch)
2018-05-01 18:42:07.092248: step 3600, loss = 26.44 (3.2 examples/sec; 4.961 sec/batch)
2018-05-01 18:43:00.522355: step 3610, loss = 26.15 (3.2 examples/sec; 4.986 sec/batch)
2018-05-01 18:43:51.034910: step 3620, loss = 29.81 (3.3 examples/sec; 4.876 sec/batch)
2018-05-01 18:44:40.662690: step 3630, loss = 25.49 (3.2 examples/sec; 4.949 sec/batch)
2018-05-01 18:45:30.659391: step 3640, loss = 26.53 (3.2 examples/sec; 5.004 sec/batch)
2018-05-01 18:46:20.612105: step 3650, loss = 26.83 (3.2 examples/sec; 4.937 sec/batch)
2018-05-01 18:47:10.119163: step 3660, loss = 30.10 (3.2 examples/sec; 4.997 sec/batch)
2018-05-01 18:48:00.436933: step 3670, loss = 28.90 (3.3 examples/sec; 4.821 sec/batch)
2018-05-01 18:48:47.770732: step 3680, loss = 27.39 (3.2 examples/sec; 4.953 sec/batch)
2018-05-01 18:49:37.571650: step 3690, loss = 28.52 (3.1 examples/sec; 5.135 sec/batch)
2018-05-01 18:50:26.893896: step 3700, loss = 30.05 (3.2 examples/sec; 4.927 sec/batch)
2018-05-01 18:51:19.673464: step 3710, loss = 28.97 (3.3 examples/sec; 4.865 sec/batch)
2018-05-01 18:52:09.762375: step 3720, loss = 29.02 (3.0 examples/sec; 5.354 sec/batch)
2018-05-01 18:53:00.028477: step 3730, loss = 27.13 (3.1 examples/sec; 5.122 sec/batch)
2018-05-01 18:53:49.378805: step 3740, loss = 29.47 (3.3 examples/sec; 4.859 sec/batch)
2018-05-01 18:54:39.316456: step 3750, loss = 26.88 (3.2 examples/sec; 5.067 sec/batch)
2018-05-01 18:55:29.214572: step 3760, loss = 27.88 (3.3 examples/sec; 4.810 sec/batch)
2018-05-01 18:56:19.404514: step 3770, loss = 25.00 (3.3 examples/sec; 4.919 sec/batch)
2018-05-01 18:57:09.913122: step 3780, loss = 28.02 (3.2 examples/sec; 5.033 sec/batch)
2018-05-01 18:57:59.760283: step 3790, loss = 26.88 (3.2 examples/sec; 5.075 sec/batch)
2018-05-01 18:58:46.759707: step 3800, loss = 25.87 (3.2 examples/sec; 4.965 sec/batch)
2018-05-01 18:59:40.168540: step 3810, loss = 25.44 (3.1 examples/sec; 5.148 sec/batch)
2018-05-01 19:00:30.556982: step 3820, loss = 26.81 (3.2 examples/sec; 4.942 sec/batch)
2018-05-01 19:01:20.326308: step 3830, loss = 27.16 (3.3 examples/sec; 4.915 sec/batch)
2018-05-01 19:02:10.733975: step 3840, loss = 29.96 (3.1 examples/sec; 5.093 sec/batch)
2018-05-01 19:03:01.089225: step 3850, loss = 27.42 (3.3 examples/sec; 4.911 sec/batch)
2018-05-01 19:03:50.426194: step 3860, loss = 26.94 (3.3 examples/sec; 4.824 sec/batch)
2018-05-01 19:04:40.791381: step 3870, loss = 27.23 (3.2 examples/sec; 4.988 sec/batch)
2018-05-01 19:05:30.604651: step 3880, loss = 28.04 (3.2 examples/sec; 4.938 sec/batch)
2018-05-01 19:06:20.126636: step 3890, loss = 26.82 (3.2 examples/sec; 5.000 sec/batch)
2018-05-01 19:07:09.711375: step 3900, loss = 26.02 (3.3 examples/sec; 4.853 sec/batch)
2018-05-01 19:08:03.659850: step 3910, loss = 23.02 (3.2 examples/sec; 4.953 sec/batch)
2018-05-01 19:08:50.795315: step 3920, loss = 24.58 (3.2 examples/sec; 5.014 sec/batch)
2018-05-01 19:09:40.675192: step 3930, loss = 26.22 (3.3 examples/sec; 4.908 sec/batch)
2018-05-01 19:10:29.695224: step 3940, loss = 24.34 (3.1 examples/sec; 5.088 sec/batch)
2018-05-01 19:11:19.585534: step 3950, loss = 29.47 (3.2 examples/sec; 4.974 sec/batch)
2018-05-01 19:12:08.440773: step 3960, loss = 28.60 (3.4 examples/sec; 4.665 sec/batch)
2018-05-01 19:12:57.737794: step 3970, loss = 26.64 (3.4 examples/sec; 4.770 sec/batch)
2018-05-01 19:13:47.070174: step 3980, loss = 28.38 (3.2 examples/sec; 4.937 sec/batch)
2018-05-01 19:14:36.917484: step 3990, loss = 26.77 (3.1 examples/sec; 5.239 sec/batch)
2018-05-01 19:15:26.666161: step 4000, loss = 25.29 (3.3 examples/sec; 4.842 sec/batch)
2018-05-01 19:16:19.544830: step 4010, loss = 30.91 (3.2 examples/sec; 5.026 sec/batch)
2018-05-01 19:17:09.720896: step 4020, loss = 29.31 (3.0 examples/sec; 5.291 sec/batch)
2018-05-01 19:17:59.974320: step 4030, loss = 29.09 (3.2 examples/sec; 5.026 sec/batch)
2018-05-01 19:18:47.911184: step 4040, loss = 27.15 (4.3 examples/sec; 3.714 sec/batch)
2018-05-01 19:19:36.381958: step 4050, loss = 25.64 (3.3 examples/sec; 4.845 sec/batch)
2018-05-01 19:20:25.876393: step 4060, loss = 27.39 (3.2 examples/sec; 4.969 sec/batch)
2018-05-01 19:21:15.725462: step 4070, loss = 27.48 (3.2 examples/sec; 4.932 sec/batch)
2018-05-01 19:22:05.500750: step 4080, loss = 27.58 (3.2 examples/sec; 4.961 sec/batch)
2018-05-01 19:22:54.983109: step 4090, loss = 28.05 (3.1 examples/sec; 5.086 sec/batch)
2018-05-01 19:23:45.308300: step 4100, loss = 27.14 (3.1 examples/sec; 5.080 sec/batch)
2018-05-01 19:24:38.394032: step 4110, loss = 27.11 (3.4 examples/sec; 4.739 sec/batch)
2018-05-01 19:25:28.249079: step 4120, loss = 27.37 (3.2 examples/sec; 4.947 sec/batch)
2018-05-01 19:26:18.649699: step 4130, loss = 28.82 (3.2 examples/sec; 4.976 sec/batch)
2018-05-01 19:27:08.970774: step 4140, loss = 24.96 (3.2 examples/sec; 4.926 sec/batch)
2018-05-01 19:27:58.995016: step 4150, loss = 28.31 (3.2 examples/sec; 5.009 sec/batch)
2018-05-01 19:28:49.122715: step 4160, loss = 28.45 (3.2 examples/sec; 4.975 sec/batch)
2018-05-01 19:29:36.584588: step 4170, loss = 26.51 (3.2 examples/sec; 5.078 sec/batch)
2018-05-01 19:30:28.294594: step 4180, loss = 29.93 (3.1 examples/sec; 5.094 sec/batch)
2018-05-01 19:31:18.191826: step 4190, loss = 27.00 (3.1 examples/sec; 5.195 sec/batch)
2018-05-01 19:32:07.889810: step 4200, loss = 26.81 (3.2 examples/sec; 4.979 sec/batch)
2018-05-01 19:33:01.955225: step 4210, loss = 26.46 (3.2 examples/sec; 5.024 sec/batch)
2018-05-01 19:33:52.297261: step 4220, loss = 27.48 (3.2 examples/sec; 5.075 sec/batch)
2018-05-01 19:34:42.569901: step 4230, loss = 27.35 (3.1 examples/sec; 5.091 sec/batch)
2018-05-01 19:35:32.531816: step 4240, loss = 26.87 (3.2 examples/sec; 4.948 sec/batch)
2018-05-01 19:36:22.822312: step 4250, loss = 27.11 (3.2 examples/sec; 4.992 sec/batch)
2018-05-01 19:37:12.508461: step 4260, loss = 25.78 (3.1 examples/sec; 5.101 sec/batch)
2018-05-01 19:38:02.421573: step 4270, loss = 27.39 (3.2 examples/sec; 4.949 sec/batch)
2018-05-01 19:38:52.774573: step 4280, loss = 26.44 (3.2 examples/sec; 5.036 sec/batch)
2018-05-01 19:39:40.593610: step 4290, loss = 27.39 (3.1 examples/sec; 5.141 sec/batch)
2018-05-01 19:40:31.671104: step 4300, loss = 30.86 (3.2 examples/sec; 5.068 sec/batch)
2018-05-01 19:41:25.129159: step 4310, loss = 28.02 (3.2 examples/sec; 4.987 sec/batch)
2018-05-01 19:42:14.922453: step 4320, loss = 28.90 (3.2 examples/sec; 5.024 sec/batch)
2018-05-01 19:43:04.680752: step 4330, loss = 26.19 (3.3 examples/sec; 4.875 sec/batch)
2018-05-01 19:43:53.707908: step 4340, loss = 27.34 (3.2 examples/sec; 4.998 sec/batch)
2018-05-01 19:44:43.452882: step 4350, loss = 25.24 (3.2 examples/sec; 5.066 sec/batch)
2018-05-01 19:45:33.140944: step 4360, loss = 27.08 (3.3 examples/sec; 4.900 sec/batch)
2018-05-01 19:46:23.343510: step 4370, loss = 27.75 (3.2 examples/sec; 4.944 sec/batch)
2018-05-01 19:47:13.158648: step 4380, loss = 28.34 (3.1 examples/sec; 5.129 sec/batch)
2018-05-01 19:48:02.581649: step 4390, loss = 26.06 (3.3 examples/sec; 4.917 sec/batch)
2018-05-01 19:48:52.860718: step 4400, loss = 24.32 (3.1 examples/sec; 5.183 sec/batch)
2018-05-01 19:49:43.877487: step 4410, loss = 26.82 (3.2 examples/sec; 5.014 sec/batch)
2018-05-01 19:50:34.018078: step 4420, loss = 27.46 (3.2 examples/sec; 4.951 sec/batch)
2018-05-01 19:51:24.175290: step 4430, loss = 29.51 (3.2 examples/sec; 4.954 sec/batch)
2018-05-01 19:52:14.348751: step 4440, loss = 26.64 (3.2 examples/sec; 4.969 sec/batch)
2018-05-01 19:53:04.047628: step 4450, loss = 28.79 (3.4 examples/sec; 4.734 sec/batch)
2018-05-01 19:53:53.337843: step 4460, loss = 25.60 (3.2 examples/sec; 4.962 sec/batch)
2018-05-01 19:54:43.799545: step 4470, loss = 26.10 (3.2 examples/sec; 4.957 sec/batch)
2018-05-01 19:55:33.860405: step 4480, loss = 26.46 (3.2 examples/sec; 5.008 sec/batch)
2018-05-01 19:56:23.751863: step 4490, loss = 31.83 (3.3 examples/sec; 4.793 sec/batch)
2018-05-01 19:57:13.831394: step 4500, loss = 22.86 (3.2 examples/sec; 5.038 sec/batch)
2018-05-01 19:58:07.247155: step 4510, loss = 26.13 (3.1 examples/sec; 5.103 sec/batch)
2018-05-01 19:58:57.101771: step 4520, loss = 27.46 (3.3 examples/sec; 4.777 sec/batch)
2018-05-01 19:59:44.207377: step 4530, loss = 28.38 (3.2 examples/sec; 5.042 sec/batch)
2018-05-01 20:00:34.125222: step 4540, loss = 26.25 (3.1 examples/sec; 5.104 sec/batch)
2018-05-01 20:01:24.034068: step 4550, loss = 30.20 (3.2 examples/sec; 4.944 sec/batch)
2018-05-01 20:02:14.508749: step 4560, loss = 27.18 (3.2 examples/sec; 4.932 sec/batch)
2018-05-01 20:03:05.041878: step 4570, loss = 25.41 (3.2 examples/sec; 5.075 sec/batch)
2018-05-01 20:03:54.694025: step 4580, loss = 27.74 (3.2 examples/sec; 5.007 sec/batch)
2018-05-01 20:04:44.788183: step 4590, loss = 27.67 (3.1 examples/sec; 5.080 sec/batch)
2018-05-01 20:05:34.185047: step 4600, loss = 25.37 (3.2 examples/sec; 4.936 sec/batch)
2018-05-01 20:06:27.795981: step 4610, loss = 27.85 (3.2 examples/sec; 4.991 sec/batch)
2018-05-01 20:07:18.001248: step 4620, loss = 25.51 (3.2 examples/sec; 5.064 sec/batch)
2018-05-01 20:08:07.530840: step 4630, loss = 25.46 (3.2 examples/sec; 5.011 sec/batch)
2018-05-01 20:08:56.884367: step 4640, loss = 27.05 (3.3 examples/sec; 4.912 sec/batch)
2018-05-01 20:09:44.837988: step 4650, loss = 25.46 (4.3 examples/sec; 3.753 sec/batch)
2018-05-01 20:10:33.497392: step 4660, loss = 25.57 (3.2 examples/sec; 5.006 sec/batch)
2018-05-01 20:11:23.526707: step 4670, loss = 30.32 (3.2 examples/sec; 4.991 sec/batch)
2018-05-01 20:12:13.551138: step 4680, loss = 26.32 (3.1 examples/sec; 5.091 sec/batch)
2018-05-01 20:13:04.088912: step 4690, loss = 28.98 (3.2 examples/sec; 5.061 sec/batch)
2018-05-01 20:13:53.233138: step 4700, loss = 27.82 (3.2 examples/sec; 4.990 sec/batch)
2018-05-01 20:14:45.943485: step 4710, loss = 27.45 (3.3 examples/sec; 4.915 sec/batch)
2018-05-01 20:15:35.528648: step 4720, loss = 25.00 (3.3 examples/sec; 4.872 sec/batch)
2018-05-01 20:16:24.860218: step 4730, loss = 26.22 (3.2 examples/sec; 4.945 sec/batch)
2018-05-01 20:17:14.948880: step 4740, loss = 28.30 (3.2 examples/sec; 5.023 sec/batch)
2018-05-01 20:18:05.366998: step 4750, loss = 24.60 (3.3 examples/sec; 4.893 sec/batch)
2018-05-01 20:18:54.847173: step 4760, loss = 25.82 (3.1 examples/sec; 5.106 sec/batch)
2018-05-01 20:19:44.163909: step 4770, loss = 25.14 (3.2 examples/sec; 4.930 sec/batch)
2018-05-01 20:20:32.141225: step 4780, loss = 25.10 (3.2 examples/sec; 4.967 sec/batch)
2018-05-01 20:21:22.430555: step 4790, loss = 27.25 (3.2 examples/sec; 4.947 sec/batch)
2018-05-01 20:22:12.439869: step 4800, loss = 24.50 (3.1 examples/sec; 5.208 sec/batch)
2018-05-01 20:23:06.170836: step 4810, loss = 29.00 (3.2 examples/sec; 5.012 sec/batch)
2018-05-01 20:23:56.389955: step 4820, loss = 26.04 (3.1 examples/sec; 5.131 sec/batch)
2018-05-01 20:24:46.725376: step 4830, loss = 27.79 (3.2 examples/sec; 5.079 sec/batch)
2018-05-01 20:25:36.961904: step 4840, loss = 26.35 (3.2 examples/sec; 5.007 sec/batch)
2018-05-01 20:26:26.601582: step 4850, loss = 25.72 (3.3 examples/sec; 4.863 sec/batch)
2018-05-01 20:27:16.031402: step 4860, loss = 24.46 (3.3 examples/sec; 4.883 sec/batch)
2018-05-01 20:28:05.956132: step 4870, loss = 23.74 (3.2 examples/sec; 5.032 sec/batch)
2018-05-01 20:28:56.618549: step 4880, loss = 25.50 (3.1 examples/sec; 5.083 sec/batch)
2018-05-01 20:29:46.864132: step 4890, loss = 24.73 (3.2 examples/sec; 5.053 sec/batch)
2018-05-01 20:30:34.994099: step 4900, loss = 28.06 (3.3 examples/sec; 4.899 sec/batch)
2018-05-01 20:31:28.362577: step 4910, loss = 26.75 (3.2 examples/sec; 5.024 sec/batch)
2018-05-01 20:32:18.718994: step 4920, loss = 26.33 (3.3 examples/sec; 4.829 sec/batch)
2018-05-01 20:33:09.161295: step 4930, loss = 28.24 (3.2 examples/sec; 5.051 sec/batch)
2018-05-01 20:33:59.706940: step 4940, loss = 24.57 (3.1 examples/sec; 5.117 sec/batch)
2018-05-01 20:34:49.931880: step 4950, loss = 25.07 (3.2 examples/sec; 5.036 sec/batch)
2018-05-01 20:35:39.554562: step 4960, loss = 26.24 (3.3 examples/sec; 4.777 sec/batch)
2018-05-01 20:36:30.045857: step 4970, loss = 27.15 (3.2 examples/sec; 5.012 sec/batch)
2018-05-01 20:37:19.939652: step 4980, loss = 27.14 (3.2 examples/sec; 5.001 sec/batch)
2018-05-01 20:38:09.885808: step 4990, loss = 23.26 (3.3 examples/sec; 4.882 sec/batch)
2018-05-01 20:38:59.751528: step 5000, loss = 29.74 (3.1 examples/sec; 5.190 sec/batch)
2018-05-01 20:39:53.057949: step 5010, loss = 28.06 (3.2 examples/sec; 4.962 sec/batch)
2018-05-01 20:40:40.387627: step 5020, loss = 25.68 (3.2 examples/sec; 4.975 sec/batch)
2018-05-01 20:41:30.563062: step 5030, loss = 23.36 (3.2 examples/sec; 5.018 sec/batch)
2018-05-01 20:42:20.314325: step 5040, loss = 28.29 (3.3 examples/sec; 4.860 sec/batch)
2018-05-01 20:43:09.586215: step 5050, loss = 26.50 (3.4 examples/sec; 4.667 sec/batch)
2018-05-01 20:43:59.764419: step 5060, loss = 26.61 (3.2 examples/sec; 4.956 sec/batch)
2018-05-01 20:44:49.819072: step 5070, loss = 26.04 (3.2 examples/sec; 4.981 sec/batch)
2018-05-01 20:45:39.735858: step 5080, loss = 26.67 (3.2 examples/sec; 4.977 sec/batch)
2018-05-01 20:46:30.463188: step 5090, loss = 26.47 (3.2 examples/sec; 5.065 sec/batch)
2018-05-01 20:47:20.582481: step 5100, loss = 27.58 (3.2 examples/sec; 5.045 sec/batch)
2018-05-01 20:48:14.391731: step 5110, loss = 26.52 (3.3 examples/sec; 4.830 sec/batch)
2018-05-01 20:49:04.042324: step 5120, loss = 25.94 (3.2 examples/sec; 5.061 sec/batch)
2018-05-01 20:49:54.153065: step 5130, loss = 27.73 (3.2 examples/sec; 5.030 sec/batch)
2018-05-01 20:50:41.577901: step 5140, loss = 24.80 (3.8 examples/sec; 4.250 sec/batch)
2018-05-01 20:51:31.260729: step 5150, loss = 24.46 (3.3 examples/sec; 4.875 sec/batch)
2018-05-01 20:52:20.973079: step 5160, loss = 25.65 (3.2 examples/sec; 5.027 sec/batch)
2018-05-01 20:53:10.987929: step 5170, loss = 24.67 (3.1 examples/sec; 5.105 sec/batch)
2018-05-01 20:54:00.631987: step 5180, loss = 24.95 (3.3 examples/sec; 4.803 sec/batch)
2018-05-01 20:54:50.839517: step 5190, loss = 25.29 (3.2 examples/sec; 4.956 sec/batch)
2018-05-01 20:55:40.943450: step 5200, loss = 29.25 (3.2 examples/sec; 5.064 sec/batch)
2018-05-01 20:56:34.334918: step 5210, loss = 25.29 (3.2 examples/sec; 4.964 sec/batch)
2018-05-01 20:57:24.392267: step 5220, loss = 23.81 (3.2 examples/sec; 5.016 sec/batch)
2018-05-01 20:58:14.932175: step 5230, loss = 25.91 (3.1 examples/sec; 5.109 sec/batch)
2018-05-01 20:59:05.314035: step 5240, loss = 30.00 (3.2 examples/sec; 4.988 sec/batch)
2018-05-01 20:59:54.945266: step 5250, loss = 26.17 (3.3 examples/sec; 4.793 sec/batch)
2018-05-01 21:00:44.457420: step 5260, loss = 25.31 (3.9 examples/sec; 4.077 sec/batch)
2018-05-01 21:01:33.153056: step 5270, loss = 22.02 (3.2 examples/sec; 5.052 sec/batch)
2018-05-01 21:02:23.352227: step 5280, loss = 24.08 (3.2 examples/sec; 4.992 sec/batch)
2018-05-01 21:03:13.577926: step 5290, loss = 25.21 (3.2 examples/sec; 4.973 sec/batch)
2018-05-01 21:04:03.185055: step 5300, loss = 26.78 (3.1 examples/sec; 5.106 sec/batch)
2018-05-01 21:04:55.923080: step 5310, loss = 23.33 (3.3 examples/sec; 4.892 sec/batch)
2018-05-01 21:05:45.971626: step 5320, loss = 24.41 (3.3 examples/sec; 4.908 sec/batch)
2018-05-01 21:06:36.084701: step 5330, loss = 24.01 (3.2 examples/sec; 5.031 sec/batch)
2018-05-01 21:07:26.225017: step 5340, loss = 25.94 (3.2 examples/sec; 5.055 sec/batch)
2018-05-01 21:08:16.069416: step 5350, loss = 24.38 (3.2 examples/sec; 4.988 sec/batch)
2018-05-01 21:09:06.498385: step 5360, loss = 26.37 (3.2 examples/sec; 4.928 sec/batch)
2018-05-01 21:09:56.579531: step 5370, loss = 24.87 (3.1 examples/sec; 5.091 sec/batch)
2018-05-01 21:10:45.927793: step 5380, loss = 28.64 (3.2 examples/sec; 4.951 sec/batch)
2018-05-01 21:11:33.472440: step 5390, loss = 27.44 (3.3 examples/sec; 4.910 sec/batch)
2018-05-01 21:12:23.676303: step 5400, loss = 26.52 (3.2 examples/sec; 5.062 sec/batch)
2018-05-01 21:13:17.417022: step 5410, loss = 24.14 (3.1 examples/sec; 5.142 sec/batch)
2018-05-01 21:14:07.177993: step 5420, loss = 24.55 (3.1 examples/sec; 5.120 sec/batch)
2018-05-01 21:14:57.074319: step 5430, loss = 24.36 (3.2 examples/sec; 5.003 sec/batch)
2018-05-01 21:15:46.594537: step 5440, loss = 27.33 (3.2 examples/sec; 5.030 sec/batch)
2018-05-01 21:16:35.777160: step 5450, loss = 24.55 (3.3 examples/sec; 4.866 sec/batch)
2018-05-01 21:17:25.419418: step 5460, loss = 25.39 (3.2 examples/sec; 5.064 sec/batch)
2018-05-01 21:18:15.874947: step 5470, loss = 27.00 (3.3 examples/sec; 4.891 sec/batch)
2018-05-01 21:19:06.107854: step 5480, loss = 25.25 (3.2 examples/sec; 5.033 sec/batch)
2018-05-01 21:19:54.939906: step 5490, loss = 24.25 (3.3 examples/sec; 4.915 sec/batch)
2018-05-01 21:20:44.770952: step 5500, loss = 27.10 (3.2 examples/sec; 5.011 sec/batch)
2018-05-01 21:21:35.268623: step 5510, loss = 24.31 (3.3 examples/sec; 4.891 sec/batch)
2018-05-01 21:22:25.184039: step 5520, loss = 24.86 (3.2 examples/sec; 4.974 sec/batch)
2018-05-01 21:23:15.228413: step 5530, loss = 23.74 (3.2 examples/sec; 5.061 sec/batch)
2018-05-01 21:24:05.060807: step 5540, loss = 30.08 (3.3 examples/sec; 4.794 sec/batch)
2018-05-01 21:24:55.787866: step 5550, loss = 23.24 (3.1 examples/sec; 5.132 sec/batch)
2018-05-01 21:25:46.480280: step 5560, loss = 27.95 (3.3 examples/sec; 4.897 sec/batch)
2018-05-01 21:26:37.356414: step 5570, loss = 25.29 (3.2 examples/sec; 4.978 sec/batch)
2018-05-01 21:27:27.053190: step 5580, loss = 28.18 (3.3 examples/sec; 4.777 sec/batch)
2018-05-01 21:28:16.168067: step 5590, loss = 24.91 (3.3 examples/sec; 4.907 sec/batch)
2018-05-01 21:29:06.667900: step 5600, loss = 26.12 (3.2 examples/sec; 5.045 sec/batch)
2018-05-01 21:29:59.422971: step 5610, loss = 27.94 (3.4 examples/sec; 4.717 sec/batch)
2018-05-01 21:30:49.423867: step 5620, loss = 26.11 (3.2 examples/sec; 4.936 sec/batch)
2018-05-01 21:31:36.926158: step 5630, loss = 31.08 (3.2 examples/sec; 5.018 sec/batch)
2018-05-01 21:32:26.597242: step 5640, loss = 25.12 (3.2 examples/sec; 5.067 sec/batch)
2018-05-01 21:33:16.256766: step 5650, loss = 29.07 (3.2 examples/sec; 5.007 sec/batch)
2018-05-01 21:34:06.402112: step 5660, loss = 25.88 (3.2 examples/sec; 5.061 sec/batch)
2018-05-01 21:34:56.562509: step 5670, loss = 29.00 (3.2 examples/sec; 5.034 sec/batch)
2018-05-01 21:35:46.503038: step 5680, loss = 25.82 (3.3 examples/sec; 4.906 sec/batch)
2018-05-01 21:36:36.898801: step 5690, loss = 26.10 (3.3 examples/sec; 4.870 sec/batch)
2018-05-01 21:37:26.766292: step 5700, loss = 24.15 (3.3 examples/sec; 4.823 sec/batch)
2018-05-01 21:38:20.356224: step 5710, loss = 24.44 (3.2 examples/sec; 5.045 sec/batch)
2018-05-01 21:39:10.943773: step 5720, loss = 25.31 (3.2 examples/sec; 4.969 sec/batch)
2018-05-01 21:40:00.966446: step 5730, loss = 24.06 (3.1 examples/sec; 5.120 sec/batch)
2018-05-01 21:40:50.818689: step 5740, loss = 26.86 (3.1 examples/sec; 5.186 sec/batch)
2018-05-01 21:41:38.088727: step 5750, loss = 27.70 (3.2 examples/sec; 4.929 sec/batch)
2018-05-01 21:42:27.535716: step 5760, loss = 27.22 (3.3 examples/sec; 4.865 sec/batch)
2018-05-01 21:43:18.012736: step 5770, loss = 24.95 (3.2 examples/sec; 5.049 sec/batch)
2018-05-01 21:44:07.828615: step 5780, loss = 25.50 (3.2 examples/sec; 5.053 sec/batch)
2018-05-01 21:44:57.719040: step 5790, loss = 27.55 (3.2 examples/sec; 4.963 sec/batch)
2018-05-01 21:45:47.996474: step 5800, loss = 25.15 (3.2 examples/sec; 4.986 sec/batch)
2018-05-01 21:46:41.127643: step 5810, loss = 24.85 (3.2 examples/sec; 5.026 sec/batch)
2018-05-01 21:47:30.238572: step 5820, loss = 28.76 (3.3 examples/sec; 4.807 sec/batch)
2018-05-01 21:48:19.872936: step 5830, loss = 28.13 (3.3 examples/sec; 4.837 sec/batch)
2018-05-01 21:49:09.272541: step 5840, loss = 27.15 (3.3 examples/sec; 4.916 sec/batch)
2018-05-01 21:49:59.467374: step 5850, loss = 22.00 (3.1 examples/sec; 5.125 sec/batch)
2018-05-01 21:50:49.070868: step 5860, loss = 22.57 (3.4 examples/sec; 4.754 sec/batch)
2018-05-01 21:51:36.015476: step 5870, loss = 28.02 (4.2 examples/sec; 3.818 sec/batch)
2018-05-01 21:52:24.754408: step 5880, loss = 26.24 (3.4 examples/sec; 4.669 sec/batch)
2018-05-01 21:53:14.170465: step 5890, loss = 25.48 (3.3 examples/sec; 4.805 sec/batch)
2018-05-01 21:54:04.742627: step 5900, loss = 23.33 (3.2 examples/sec; 4.988 sec/batch)
2018-05-01 21:54:58.603171: step 5910, loss = 24.12 (3.3 examples/sec; 4.902 sec/batch)
2018-05-01 21:55:48.310712: step 5920, loss = 25.16 (3.2 examples/sec; 5.017 sec/batch)
2018-05-01 21:56:37.979148: step 5930, loss = 26.13 (3.2 examples/sec; 4.996 sec/batch)
2018-05-01 21:57:27.566171: step 5940, loss = 23.71 (3.1 examples/sec; 5.090 sec/batch)
2018-05-01 21:58:17.643415: step 5950, loss = 25.31 (3.1 examples/sec; 5.227 sec/batch)
2018-05-01 21:59:07.683737: step 5960, loss = 25.15 (3.2 examples/sec; 5.019 sec/batch)
2018-05-01 21:59:57.339815: step 5970, loss = 27.28 (3.1 examples/sec; 5.145 sec/batch)
2018-05-01 22:00:47.360088: step 5980, loss = 25.74 (3.2 examples/sec; 4.992 sec/batch)
2018-05-01 22:01:37.080882: step 5990, loss = 26.45 (3.2 examples/sec; 5.058 sec/batch)
2018-05-01 22:02:24.364630: step 6000, loss = 25.51 (3.2 examples/sec; 5.063 sec/batch)
2018-05-01 22:03:17.457519: step 6010, loss = 25.25 (3.3 examples/sec; 4.799 sec/batch)
2018-05-01 22:04:07.424454: step 6020, loss = 27.40 (3.1 examples/sec; 5.130 sec/batch)
2018-05-01 22:04:57.329682: step 6030, loss = 24.31 (3.1 examples/sec; 5.159 sec/batch)
2018-05-01 22:05:47.208987: step 6040, loss = 24.06 (3.2 examples/sec; 5.069 sec/batch)
2018-05-01 22:06:37.059369: step 6050, loss = 26.07 (3.2 examples/sec; 4.986 sec/batch)
2018-05-01 22:07:26.878896: step 6060, loss = 27.49 (3.2 examples/sec; 5.036 sec/batch)
2018-05-01 22:08:16.335553: step 6070, loss = 24.73 (3.2 examples/sec; 4.935 sec/batch)
2018-05-01 22:09:07.022499: step 6080, loss = 27.23 (3.3 examples/sec; 4.917 sec/batch)
2018-05-01 22:09:57.562198: step 6090, loss = 25.36 (3.2 examples/sec; 4.986 sec/batch)
2018-05-01 22:10:47.728753: step 6100, loss = 26.67 (3.1 examples/sec; 5.111 sec/batch)
2018-05-01 22:11:40.988901: step 6110, loss = 25.47 (3.4 examples/sec; 4.770 sec/batch)
2018-05-01 22:12:27.843557: step 6120, loss = 26.79 (3.2 examples/sec; 4.931 sec/batch)
2018-05-01 22:13:17.841779: step 6130, loss = 25.18 (3.3 examples/sec; 4.823 sec/batch)
2018-05-01 22:14:07.163346: step 6140, loss = 23.63 (3.2 examples/sec; 4.974 sec/batch)
2018-05-01 22:14:56.866948: step 6150, loss = 24.42 (3.3 examples/sec; 4.882 sec/batch)
2018-05-01 22:15:46.773758: step 6160, loss = 23.02 (3.1 examples/sec; 5.096 sec/batch)
2018-05-01 22:16:36.246741: step 6170, loss = 25.27 (3.2 examples/sec; 5.036 sec/batch)
2018-05-01 22:17:26.091229: step 6180, loss = 25.19 (3.1 examples/sec; 5.161 sec/batch)
2018-05-01 22:18:15.893770: step 6190, loss = 25.49 (3.2 examples/sec; 4.990 sec/batch)
2018-05-01 22:19:05.685371: step 6200, loss = 22.43 (3.3 examples/sec; 4.865 sec/batch)
2018-05-01 22:19:58.782747: step 6210, loss = 25.66 (3.2 examples/sec; 4.947 sec/batch)
2018-05-01 22:20:48.691729: step 6220, loss = 25.52 (3.2 examples/sec; 5.054 sec/batch)
2018-05-01 22:21:38.314687: step 6230, loss = 25.40 (3.1 examples/sec; 5.201 sec/batch)
2018-05-01 22:22:25.530279: step 6240, loss = 24.67 (3.2 examples/sec; 4.964 sec/batch)
2018-05-01 22:23:14.426944: step 6250, loss = 24.38 (3.3 examples/sec; 4.842 sec/batch)
2018-05-01 22:24:03.827545: step 6260, loss = 25.06 (3.2 examples/sec; 4.926 sec/batch)
2018-05-01 22:24:53.494653: step 6270, loss = 28.48 (3.1 examples/sec; 5.100 sec/batch)
2018-05-01 22:25:42.962265: step 6280, loss = 26.84 (3.2 examples/sec; 5.058 sec/batch)
2018-05-01 22:26:32.257336: step 6290, loss = 26.31 (3.2 examples/sec; 4.949 sec/batch)
2018-05-01 22:27:21.996398: step 6300, loss = 25.17 (3.2 examples/sec; 4.966 sec/batch)
2018-05-01 22:28:16.085004: step 6310, loss = 25.46 (3.1 examples/sec; 5.097 sec/batch)
2018-05-01 22:29:06.077032: step 6320, loss = 23.30 (3.1 examples/sec; 5.172 sec/batch)
2018-05-01 22:29:55.398415: step 6330, loss = 28.17 (3.2 examples/sec; 4.934 sec/batch)
2018-05-01 22:30:45.148485: step 6340, loss = 26.51 (3.2 examples/sec; 4.944 sec/batch)
2018-05-01 22:31:34.894234: step 6350, loss = 28.21 (3.2 examples/sec; 4.930 sec/batch)
2018-05-01 22:32:23.909491: step 6360, loss = 26.56 (4.1 examples/sec; 3.941 sec/batch)
2018-05-01 22:33:11.713905: step 6370, loss = 23.53 (3.3 examples/sec; 4.862 sec/batch)
2018-05-01 22:34:01.507527: step 6380, loss = 23.49 (3.4 examples/sec; 4.735 sec/batch)
2018-05-01 22:34:50.966529: step 6390, loss = 24.70 (3.3 examples/sec; 4.871 sec/batch)
2018-05-01 22:35:41.491456: step 6400, loss = 26.76 (3.1 examples/sec; 5.180 sec/batch)
2018-05-01 22:36:34.742352: step 6410, loss = 23.74 (3.1 examples/sec; 5.099 sec/batch)
2018-05-01 22:37:25.067000: step 6420, loss = 24.14 (3.2 examples/sec; 4.947 sec/batch)
2018-05-01 22:38:14.296569: step 6430, loss = 27.08 (3.3 examples/sec; 4.903 sec/batch)
2018-05-01 22:39:03.903859: step 6440, loss = 25.79 (3.4 examples/sec; 4.751 sec/batch)
2018-05-01 22:39:53.821177: step 6450, loss = 24.07 (3.2 examples/sec; 5.058 sec/batch)
2018-05-01 22:40:43.810298: step 6460, loss = 23.38 (3.2 examples/sec; 4.996 sec/batch)
2018-05-01 22:41:33.413757: step 6470, loss = 24.16 (3.2 examples/sec; 5.024 sec/batch)
2018-05-01 22:42:22.668427: step 6480, loss = 24.69 (3.3 examples/sec; 4.860 sec/batch)
2018-05-01 22:43:09.989806: step 6490, loss = 26.04 (3.2 examples/sec; 4.937 sec/batch)
2018-05-01 22:43:59.049742: step 6500, loss = 24.44 (3.2 examples/sec; 5.016 sec/batch)
2018-05-01 22:44:51.699629: step 6510, loss = 25.81 (3.2 examples/sec; 5.012 sec/batch)
2018-05-01 22:45:40.961817: step 6520, loss = 25.19 (3.2 examples/sec; 5.012 sec/batch)
2018-05-01 22:46:29.758386: step 6530, loss = 22.71 (3.3 examples/sec; 4.880 sec/batch)
2018-05-01 22:47:18.565540: step 6540, loss = 25.69 (3.3 examples/sec; 4.815 sec/batch)
2018-05-01 22:48:07.429611: step 6550, loss = 23.64 (3.4 examples/sec; 4.688 sec/batch)
2018-05-01 22:48:57.871857: step 6560, loss = 23.70 (3.1 examples/sec; 5.129 sec/batch)
2018-05-01 22:49:47.634854: step 6570, loss = 24.16 (3.1 examples/sec; 5.104 sec/batch)
2018-05-01 22:50:37.829110: step 6580, loss = 23.97 (3.3 examples/sec; 4.905 sec/batch)
2018-05-01 22:51:27.943717: step 6590, loss = 25.95 (3.2 examples/sec; 4.994 sec/batch)
2018-05-01 22:52:17.156999: step 6600, loss = 25.81 (3.2 examples/sec; 4.958 sec/batch)
2018-05-01 22:53:07.562761: step 6610, loss = 25.67 (3.2 examples/sec; 4.955 sec/batch)
2018-05-01 22:53:57.998537: step 6620, loss = 25.26 (3.2 examples/sec; 4.965 sec/batch)
2018-05-01 22:54:47.746435: step 6630, loss = 27.55 (3.3 examples/sec; 4.827 sec/batch)
2018-05-01 22:55:36.821539: step 6640, loss = 25.87 (3.3 examples/sec; 4.843 sec/batch)
2018-05-01 22:56:26.646301: step 6650, loss = 25.26 (3.1 examples/sec; 5.083 sec/batch)
2018-05-01 22:57:16.953597: step 6660, loss = 29.46 (3.3 examples/sec; 4.801 sec/batch)
2018-05-01 22:58:06.329220: step 6670, loss = 22.84 (3.2 examples/sec; 4.950 sec/batch)
2018-05-01 22:58:56.593881: step 6680, loss = 26.60 (3.4 examples/sec; 4.741 sec/batch)
2018-05-01 22:59:45.791035: step 6690, loss = 24.82 (3.3 examples/sec; 4.880 sec/batch)
2018-05-01 23:00:35.662731: step 6700, loss = 25.33 (3.2 examples/sec; 5.031 sec/batch)
2018-05-01 23:01:27.954643: step 6710, loss = 26.70 (3.2 examples/sec; 5.040 sec/batch)
2018-05-01 23:02:17.313980: step 6720, loss = 26.21 (3.3 examples/sec; 4.857 sec/batch)
2018-05-01 23:03:05.343842: step 6730, loss = 25.85 (4.3 examples/sec; 3.702 sec/batch)
2018-05-01 23:03:54.802041: step 6740, loss = 25.56 (3.2 examples/sec; 4.990 sec/batch)
2018-05-01 23:04:44.495084: step 6750, loss = 23.67 (3.3 examples/sec; 4.918 sec/batch)
2018-05-01 23:05:33.802216: step 6760, loss = 24.77 (3.3 examples/sec; 4.911 sec/batch)
2018-05-01 23:06:22.994016: step 6770, loss = 24.66 (3.4 examples/sec; 4.694 sec/batch)
2018-05-01 23:07:12.838763: step 6780, loss = 24.31 (3.1 examples/sec; 5.143 sec/batch)
2018-05-01 23:08:02.573667: step 6790, loss = 25.51 (3.2 examples/sec; 5.021 sec/batch)
2018-05-01 23:08:51.895378: step 6800, loss = 25.98 (3.3 examples/sec; 4.882 sec/batch)
2018-05-01 23:09:45.309545: step 6810, loss = 26.06 (3.2 examples/sec; 4.966 sec/batch)
2018-05-01 23:10:35.454907: step 6820, loss = 25.60 (3.3 examples/sec; 4.837 sec/batch)
2018-05-01 23:11:25.449070: step 6830, loss = 27.06 (3.2 examples/sec; 5.005 sec/batch)
2018-05-01 23:12:14.533231: step 6840, loss = 23.96 (3.1 examples/sec; 5.174 sec/batch)
2018-05-01 23:13:03.806654: step 6850, loss = 25.67 (3.3 examples/sec; 4.869 sec/batch)
2018-05-01 23:13:50.320715: step 6860, loss = 24.95 (3.2 examples/sec; 5.021 sec/batch)
2018-05-01 23:14:39.591062: step 6870, loss = 25.87 (3.2 examples/sec; 4.978 sec/batch)
2018-05-01 23:15:29.091288: step 6880, loss = 25.08 (3.3 examples/sec; 4.871 sec/batch)
2018-05-01 23:16:19.482464: step 6890, loss = 25.60 (3.1 examples/sec; 5.171 sec/batch)
2018-05-01 23:17:09.174883: step 6900, loss = 25.69 (3.2 examples/sec; 5.018 sec/batch)
2018-05-01 23:18:02.587354: step 6910, loss = 29.74 (3.2 examples/sec; 4.969 sec/batch)
2018-05-01 23:18:51.683292: step 6920, loss = 27.03 (3.2 examples/sec; 5.044 sec/batch)
2018-05-01 23:19:41.762163: step 6930, loss = 23.93 (3.1 examples/sec; 5.093 sec/batch)
2018-05-01 23:20:31.492826: step 6940, loss = 24.21 (3.1 examples/sec; 5.090 sec/batch)
2018-05-01 23:21:21.457323: step 6950, loss = 22.13 (3.1 examples/sec; 5.229 sec/batch)
2018-05-01 23:22:11.207226: step 6960, loss = 23.30 (3.2 examples/sec; 5.039 sec/batch)
2018-05-01 23:23:00.870392: step 6970, loss = 25.16 (3.3 examples/sec; 4.859 sec/batch)
2018-05-01 23:23:47.755768: step 6980, loss = 25.68 (3.2 examples/sec; 5.016 sec/batch)
2018-05-01 23:24:37.503384: step 6990, loss = 24.89 (3.3 examples/sec; 4.846 sec/batch)
2018-05-01 23:25:26.679655: step 7000, loss = 26.79 (3.2 examples/sec; 4.952 sec/batch)
2018-05-01 23:26:20.221284: step 7010, loss = 26.18 (3.1 examples/sec; 5.098 sec/batch)
2018-05-01 23:27:10.369277: step 7020, loss = 26.82 (3.3 examples/sec; 4.896 sec/batch)
2018-05-01 23:28:00.105859: step 7030, loss = 25.80 (3.2 examples/sec; 5.042 sec/batch)
2018-05-01 23:28:49.514999: step 7040, loss = 23.69 (3.2 examples/sec; 5.006 sec/batch)
2018-05-01 23:29:38.288372: step 7050, loss = 24.63 (3.2 examples/sec; 4.956 sec/batch)
2018-05-01 23:30:27.562021: step 7060, loss = 23.58 (3.1 examples/sec; 5.089 sec/batch)
2018-05-01 23:31:16.558205: step 7070, loss = 23.73 (3.2 examples/sec; 4.958 sec/batch)
2018-05-01 23:32:06.650957: step 7080, loss = 23.20 (3.2 examples/sec; 5.005 sec/batch)
2018-05-01 23:32:56.322897: step 7090, loss = 25.17 (3.3 examples/sec; 4.910 sec/batch)
2018-05-01 23:33:44.616189: step 7100, loss = 23.67 (4.1 examples/sec; 3.877 sec/batch)
2018-05-01 23:34:37.461698: step 7110, loss = 22.68 (3.1 examples/sec; 5.083 sec/batch)
2018-05-01 23:35:26.571481: step 7120, loss = 23.80 (3.3 examples/sec; 4.915 sec/batch)
2018-05-01 23:36:16.476306: step 7130, loss = 24.94 (3.1 examples/sec; 5.097 sec/batch)
2018-05-01 23:37:05.292854: step 7140, loss = 23.47 (3.2 examples/sec; 4.943 sec/batch)
2018-05-01 23:37:55.459562: step 7150, loss = 25.80 (3.2 examples/sec; 4.982 sec/batch)
2018-05-01 23:38:45.231533: step 7160, loss = 27.25 (3.2 examples/sec; 4.958 sec/batch)
2018-05-01 23:39:34.660744: step 7170, loss = 22.08 (3.1 examples/sec; 5.105 sec/batch)
2018-05-01 23:40:23.938232: step 7180, loss = 24.41 (3.2 examples/sec; 4.926 sec/batch)
2018-05-01 23:41:13.358653: step 7190, loss = 23.81 (3.3 examples/sec; 4.913 sec/batch)
2018-05-01 23:42:03.191904: step 7200, loss = 25.74 (3.3 examples/sec; 4.912 sec/batch)
2018-05-01 23:42:56.802437: step 7210, loss = 25.94 (3.2 examples/sec; 5.075 sec/batch)
2018-05-01 23:43:46.937019: step 7220, loss = 24.89 (3.1 examples/sec; 5.166 sec/batch)
2018-05-01 23:44:34.084892: step 7230, loss = 22.59 (3.2 examples/sec; 4.998 sec/batch)
2018-05-01 23:45:23.427980: step 7240, loss = 22.79 (3.1 examples/sec; 5.090 sec/batch)
2018-05-01 23:46:12.984412: step 7250, loss = 25.91 (3.3 examples/sec; 4.846 sec/batch)
2018-05-01 23:47:02.598106: step 7260, loss = 25.77 (3.2 examples/sec; 5.022 sec/batch)
2018-05-01 23:47:52.681545: step 7270, loss = 25.44 (3.2 examples/sec; 4.980 sec/batch)
2018-05-01 23:48:42.411057: step 7280, loss = 26.75 (3.2 examples/sec; 5.014 sec/batch)
2018-05-01 23:49:31.947579: step 7290, loss = 25.34 (3.2 examples/sec; 5.047 sec/batch)
2018-05-01 23:50:21.302887: step 7300, loss = 25.82 (3.3 examples/sec; 4.893 sec/batch)
2018-05-01 23:51:14.298298: step 7310, loss = 26.85 (3.1 examples/sec; 5.157 sec/batch)
2018-05-01 23:52:04.351144: step 7320, loss = 24.38 (3.2 examples/sec; 4.925 sec/batch)
2018-05-01 23:52:54.244908: step 7330, loss = 24.77 (3.3 examples/sec; 4.922 sec/batch)
2018-05-01 23:53:43.774357: step 7340, loss = 22.63 (3.2 examples/sec; 5.045 sec/batch)
2018-05-01 23:54:30.645635: step 7350, loss = 22.53 (3.4 examples/sec; 4.722 sec/batch)
2018-05-01 23:55:20.285273: step 7360, loss = 23.39 (3.3 examples/sec; 4.885 sec/batch)
2018-05-01 23:56:09.472903: step 7370, loss = 24.35 (3.2 examples/sec; 5.055 sec/batch)
2018-05-01 23:56:59.639395: step 7380, loss = 26.01 (3.2 examples/sec; 4.972 sec/batch)
2018-05-01 23:57:49.577206: step 7390, loss = 24.76 (3.2 examples/sec; 4.962 sec/batch)
2018-05-01 23:58:38.779528: step 7400, loss = 23.04 (3.2 examples/sec; 4.998 sec/batch)
2018-05-01 23:59:31.779564: step 7410, loss = 27.20 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 00:00:21.624963: step 7420, loss = 26.41 (3.3 examples/sec; 4.905 sec/batch)
2018-05-02 00:01:10.835897: step 7430, loss = 25.71 (3.2 examples/sec; 4.936 sec/batch)
2018-05-02 00:02:00.773414: step 7440, loss = 22.80 (3.2 examples/sec; 4.972 sec/batch)
2018-05-02 00:02:50.420658: step 7450, loss = 24.29 (3.3 examples/sec; 4.887 sec/batch)
2018-05-02 00:03:39.497340: step 7460, loss = 23.42 (3.3 examples/sec; 4.866 sec/batch)
2018-05-02 00:04:28.163411: step 7470, loss = 22.29 (4.1 examples/sec; 3.877 sec/batch)
2018-05-02 00:05:16.476230: step 7480, loss = 22.92 (3.2 examples/sec; 5.033 sec/batch)
2018-05-02 00:06:06.427042: step 7490, loss = 23.04 (3.3 examples/sec; 4.906 sec/batch)
2018-05-02 00:06:57.148277: step 7500, loss = 23.37 (3.2 examples/sec; 5.072 sec/batch)
2018-05-02 00:07:50.142099: step 7510, loss = 25.87 (3.3 examples/sec; 4.795 sec/batch)
2018-05-02 00:08:40.692478: step 7520, loss = 26.03 (3.1 examples/sec; 5.086 sec/batch)
2018-05-02 00:09:30.291663: step 7530, loss = 26.23 (3.3 examples/sec; 4.864 sec/batch)
2018-05-02 00:10:19.445748: step 7540, loss = 24.03 (3.2 examples/sec; 4.962 sec/batch)
2018-05-02 00:11:09.091719: step 7550, loss = 27.02 (3.1 examples/sec; 5.240 sec/batch)
2018-05-02 00:11:58.638313: step 7560, loss = 23.42 (3.1 examples/sec; 5.225 sec/batch)
2018-05-02 00:12:48.646761: step 7570, loss = 27.26 (3.2 examples/sec; 4.981 sec/batch)
2018-05-02 00:13:38.170759: step 7580, loss = 22.44 (3.2 examples/sec; 4.953 sec/batch)
2018-05-02 00:14:28.347218: step 7590, loss = 22.72 (3.0 examples/sec; 5.320 sec/batch)
2018-05-02 00:15:15.840667: step 7600, loss = 25.69 (3.2 examples/sec; 4.972 sec/batch)
2018-05-02 00:16:08.976679: step 7610, loss = 26.20 (3.1 examples/sec; 5.145 sec/batch)
2018-05-02 00:16:58.343596: step 7620, loss = 25.37 (3.3 examples/sec; 4.839 sec/batch)
2018-05-02 00:17:47.430080: step 7630, loss = 22.74 (3.3 examples/sec; 4.877 sec/batch)
2018-05-02 00:18:36.976422: step 7640, loss = 23.23 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 00:19:26.582245: step 7650, loss = 24.57 (3.2 examples/sec; 5.063 sec/batch)
2018-05-02 00:20:16.679620: step 7660, loss = 23.63 (3.2 examples/sec; 5.047 sec/batch)
2018-05-02 00:21:06.483239: step 7670, loss = 22.75 (3.2 examples/sec; 4.975 sec/batch)
2018-05-02 00:21:56.300786: step 7680, loss = 24.81 (3.2 examples/sec; 5.009 sec/batch)
2018-05-02 00:22:45.721287: step 7690, loss = 24.81 (3.3 examples/sec; 4.849 sec/batch)
2018-05-02 00:23:36.056580: step 7700, loss = 24.69 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 00:24:29.132631: step 7710, loss = 26.23 (3.3 examples/sec; 4.891 sec/batch)
2018-05-02 00:25:16.373537: step 7720, loss = 23.60 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 00:26:06.115011: step 7730, loss = 24.73 (3.2 examples/sec; 5.000 sec/batch)
2018-05-02 00:26:55.290790: step 7740, loss = 23.39 (3.2 examples/sec; 5.014 sec/batch)
2018-05-02 00:27:44.475245: step 7750, loss = 22.21 (3.2 examples/sec; 5.000 sec/batch)
2018-05-02 00:28:33.484996: step 7760, loss = 24.32 (3.4 examples/sec; 4.760 sec/batch)
2018-05-02 00:29:23.027741: step 7770, loss = 24.76 (3.2 examples/sec; 5.020 sec/batch)
2018-05-02 00:30:12.537277: step 7780, loss = 23.19 (3.2 examples/sec; 5.053 sec/batch)
2018-05-02 00:31:02.331626: step 7790, loss = 24.24 (3.2 examples/sec; 4.978 sec/batch)
2018-05-02 00:31:55.393919: step 7800, loss = 25.49 (3.1 examples/sec; 5.114 sec/batch)
2018-05-02 00:32:48.903964: step 7810, loss = 23.71 (3.2 examples/sec; 5.075 sec/batch)
2018-05-02 00:33:38.752051: step 7820, loss = 22.50 (3.1 examples/sec; 5.093 sec/batch)
2018-05-02 00:34:28.441326: step 7830, loss = 26.04 (3.2 examples/sec; 4.979 sec/batch)
2018-05-02 00:35:14.812447: step 7840, loss = 23.58 (3.4 examples/sec; 4.714 sec/batch)
2018-05-02 00:36:04.759079: step 7850, loss = 24.87 (3.2 examples/sec; 5.033 sec/batch)
2018-05-02 00:36:54.443961: step 7860, loss = 23.65 (3.1 examples/sec; 5.103 sec/batch)
2018-05-02 00:37:44.743800: step 7870, loss = 24.92 (3.2 examples/sec; 5.054 sec/batch)
2018-05-02 00:38:34.143223: step 7880, loss = 25.02 (3.1 examples/sec; 5.105 sec/batch)
2018-05-02 00:39:23.844792: step 7890, loss = 23.65 (3.3 examples/sec; 4.861 sec/batch)
2018-05-02 00:40:13.883703: step 7900, loss = 23.67 (3.1 examples/sec; 5.187 sec/batch)
2018-05-02 00:41:07.087057: step 7910, loss = 24.84 (3.2 examples/sec; 4.986 sec/batch)
2018-05-02 00:41:57.199187: step 7920, loss = 24.04 (3.2 examples/sec; 5.046 sec/batch)
2018-05-02 00:42:47.180648: step 7930, loss = 23.71 (3.2 examples/sec; 4.971 sec/batch)
2018-05-02 00:43:36.258420: step 7940, loss = 20.51 (3.3 examples/sec; 4.813 sec/batch)
2018-05-02 00:44:26.285318: step 7950, loss = 26.41 (3.3 examples/sec; 4.919 sec/batch)
2018-05-02 00:45:15.525495: step 7960, loss = 24.87 (3.4 examples/sec; 4.691 sec/batch)
2018-05-02 00:46:02.894099: step 7970, loss = 25.16 (3.2 examples/sec; 5.010 sec/batch)
2018-05-02 00:46:52.460688: step 7980, loss = 25.77 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 00:47:42.625623: step 7990, loss = 23.28 (3.2 examples/sec; 5.060 sec/batch)
2018-05-02 00:48:32.813266: step 8000, loss = 25.21 (3.3 examples/sec; 4.818 sec/batch)
2018-05-02 00:49:25.208580: step 8010, loss = 23.03 (3.2 examples/sec; 5.013 sec/batch)
2018-05-02 00:50:14.719413: step 8020, loss = 25.45 (3.3 examples/sec; 4.874 sec/batch)
2018-05-02 00:51:04.779149: step 8030, loss = 23.49 (3.2 examples/sec; 5.010 sec/batch)
2018-05-02 00:51:54.617050: step 8040, loss = 25.30 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 00:52:44.021619: step 8050, loss = 24.79 (3.1 examples/sec; 5.242 sec/batch)
2018-05-02 00:53:33.403869: step 8060, loss = 24.47 (3.1 examples/sec; 5.104 sec/batch)
2018-05-02 00:54:22.916409: step 8070, loss = 24.27 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 00:55:12.185847: step 8080, loss = 22.89 (3.3 examples/sec; 4.892 sec/batch)
2018-05-02 00:55:59.537775: step 8090, loss = 23.93 (3.2 examples/sec; 5.029 sec/batch)
2018-05-02 00:56:49.068700: step 8100, loss = 23.51 (3.3 examples/sec; 4.888 sec/batch)
2018-05-02 00:57:42.422350: step 8110, loss = 23.33 (3.2 examples/sec; 4.941 sec/batch)
2018-05-02 00:58:31.822334: step 8120, loss = 22.64 (3.3 examples/sec; 4.860 sec/batch)
2018-05-02 00:59:21.077073: step 8130, loss = 23.38 (3.2 examples/sec; 4.993 sec/batch)
2018-05-02 01:00:10.772763: step 8140, loss = 24.39 (3.2 examples/sec; 5.076 sec/batch)
2018-05-02 01:01:00.548513: step 8150, loss = 22.14 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 01:01:50.209909: step 8160, loss = 23.72 (3.2 examples/sec; 4.957 sec/batch)
2018-05-02 01:02:39.476834: step 8170, loss = 25.85 (3.2 examples/sec; 4.979 sec/batch)
2018-05-02 01:03:29.370756: step 8180, loss = 22.86 (3.2 examples/sec; 4.973 sec/batch)
2018-05-02 01:04:18.392573: step 8190, loss = 22.08 (3.3 examples/sec; 4.882 sec/batch)
2018-05-02 01:05:07.563304: step 8200, loss = 22.41 (3.3 examples/sec; 4.796 sec/batch)
2018-05-02 01:05:57.540060: step 8210, loss = 23.35 (3.2 examples/sec; 5.004 sec/batch)
2018-05-02 01:06:46.803571: step 8220, loss = 25.44 (3.4 examples/sec; 4.764 sec/batch)
2018-05-02 01:07:36.652447: step 8230, loss = 23.29 (3.2 examples/sec; 4.973 sec/batch)
2018-05-02 01:08:27.266750: step 8240, loss = 23.91 (3.1 examples/sec; 5.220 sec/batch)
2018-05-02 01:09:16.653144: step 8250, loss = 22.39 (3.1 examples/sec; 5.096 sec/batch)
2018-05-02 01:10:06.188300: step 8260, loss = 24.30 (3.3 examples/sec; 4.821 sec/batch)
2018-05-02 01:10:56.274955: step 8270, loss = 22.72 (3.3 examples/sec; 4.922 sec/batch)
2018-05-02 01:11:45.872404: step 8280, loss = 23.25 (3.1 examples/sec; 5.085 sec/batch)
2018-05-02 01:12:35.303173: step 8290, loss = 26.74 (3.3 examples/sec; 4.881 sec/batch)
2018-05-02 01:13:24.965186: step 8300, loss = 25.26 (3.2 examples/sec; 4.944 sec/batch)
2018-05-02 01:14:19.423210: step 8310, loss = 25.08 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 01:15:09.211223: step 8320, loss = 24.29 (3.3 examples/sec; 4.879 sec/batch)
2018-05-02 01:15:58.897009: step 8330, loss = 26.16 (3.2 examples/sec; 5.035 sec/batch)
2018-05-02 01:16:46.637452: step 8340, loss = 23.42 (3.2 examples/sec; 5.005 sec/batch)
2018-05-02 01:17:36.357086: step 8350, loss = 23.50 (3.3 examples/sec; 4.868 sec/batch)
2018-05-02 01:18:26.032615: step 8360, loss = 25.32 (3.2 examples/sec; 5.061 sec/batch)
2018-05-02 01:19:15.754154: step 8370, loss = 24.71 (3.1 examples/sec; 5.104 sec/batch)
2018-05-02 01:20:05.175809: step 8380, loss = 23.57 (3.2 examples/sec; 4.946 sec/batch)
2018-05-02 01:20:55.032504: step 8390, loss = 22.60 (3.2 examples/sec; 4.979 sec/batch)
2018-05-02 01:21:45.538682: step 8400, loss = 22.03 (3.1 examples/sec; 5.086 sec/batch)
2018-05-02 01:22:38.757159: step 8410, loss = 22.49 (3.1 examples/sec; 5.099 sec/batch)
2018-05-02 01:23:28.480478: step 8420, loss = 24.62 (3.2 examples/sec; 5.020 sec/batch)
2018-05-02 01:24:18.439755: step 8430, loss = 23.84 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 01:25:07.984996: step 8440, loss = 23.96 (3.2 examples/sec; 4.947 sec/batch)
2018-05-02 01:25:57.166463: step 8450, loss = 23.42 (3.2 examples/sec; 5.072 sec/batch)
2018-05-02 01:26:43.824066: step 8460, loss = 23.72 (3.3 examples/sec; 4.815 sec/batch)
2018-05-02 01:27:33.051755: step 8470, loss = 23.79 (3.4 examples/sec; 4.735 sec/batch)
2018-05-02 01:28:22.843501: step 8480, loss = 22.22 (3.3 examples/sec; 4.906 sec/batch)
2018-05-02 01:29:12.783383: step 8490, loss = 23.51 (3.2 examples/sec; 4.972 sec/batch)
2018-05-02 01:30:02.681006: step 8500, loss = 25.37 (3.2 examples/sec; 4.951 sec/batch)
2018-05-02 01:30:55.897000: step 8510, loss = 26.76 (3.3 examples/sec; 4.920 sec/batch)
2018-05-02 01:31:45.306788: step 8520, loss = 22.43 (3.2 examples/sec; 4.944 sec/batch)
2018-05-02 01:32:35.290897: step 8530, loss = 24.06 (3.3 examples/sec; 4.834 sec/batch)
2018-05-02 01:33:25.094775: step 8540, loss = 25.21 (3.2 examples/sec; 5.037 sec/batch)
2018-05-02 01:34:14.416193: step 8550, loss = 27.57 (3.2 examples/sec; 4.984 sec/batch)
2018-05-02 01:35:03.946602: step 8560, loss = 23.36 (3.3 examples/sec; 4.917 sec/batch)
2018-05-02 01:35:53.622517: step 8570, loss = 27.02 (3.2 examples/sec; 5.025 sec/batch)
2018-05-02 01:36:40.425406: step 8580, loss = 24.22 (3.4 examples/sec; 4.658 sec/batch)
2018-05-02 01:37:29.710845: step 8590, loss = 23.29 (3.2 examples/sec; 5.031 sec/batch)
2018-05-02 01:38:18.978698: step 8600, loss = 23.10 (3.4 examples/sec; 4.768 sec/batch)
2018-05-02 01:39:12.669377: step 8610, loss = 20.93 (3.2 examples/sec; 4.933 sec/batch)
2018-05-02 01:40:01.935007: step 8620, loss = 23.93 (3.2 examples/sec; 4.957 sec/batch)
2018-05-02 01:40:52.014742: step 8630, loss = 24.73 (3.2 examples/sec; 5.006 sec/batch)
2018-05-02 01:41:41.667580: step 8640, loss = 26.81 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 01:42:31.471196: step 8650, loss = 24.78 (3.1 examples/sec; 5.185 sec/batch)
2018-05-02 01:43:21.384575: step 8660, loss = 22.60 (3.1 examples/sec; 5.080 sec/batch)
2018-05-02 01:44:10.663538: step 8670, loss = 22.51 (3.2 examples/sec; 4.965 sec/batch)
2018-05-02 01:45:00.715588: step 8680, loss = 24.47 (3.3 examples/sec; 4.855 sec/batch)
2018-05-02 01:45:50.569766: step 8690, loss = 23.19 (3.1 examples/sec; 5.087 sec/batch)
2018-05-02 01:46:39.136011: step 8700, loss = 24.23 (4.1 examples/sec; 3.870 sec/batch)
2018-05-02 01:47:31.954283: step 8710, loss = 22.06 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 01:48:22.370680: step 8720, loss = 23.38 (3.3 examples/sec; 4.811 sec/batch)
2018-05-02 01:49:12.261884: step 8730, loss = 23.33 (3.2 examples/sec; 4.972 sec/batch)
2018-05-02 01:50:02.691424: step 8740, loss = 22.12 (3.1 examples/sec; 5.117 sec/batch)
2018-05-02 01:50:52.137289: step 8750, loss = 22.06 (3.2 examples/sec; 4.945 sec/batch)
2018-05-02 01:51:42.149661: step 8760, loss = 23.45 (3.4 examples/sec; 4.704 sec/batch)
2018-05-02 01:52:31.254682: step 8770, loss = 24.62 (3.2 examples/sec; 4.939 sec/batch)
2018-05-02 01:53:21.487517: step 8780, loss = 22.79 (3.1 examples/sec; 5.112 sec/batch)
2018-05-02 01:54:11.028791: step 8790, loss = 24.92 (3.2 examples/sec; 4.948 sec/batch)
2018-05-02 01:55:00.935473: step 8800, loss = 22.38 (3.2 examples/sec; 5.007 sec/batch)
2018-05-02 01:55:54.052363: step 8810, loss = 26.28 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 01:56:44.228818: step 8820, loss = 21.99 (3.2 examples/sec; 5.059 sec/batch)
2018-05-02 01:57:31.538780: step 8830, loss = 23.11 (3.3 examples/sec; 4.922 sec/batch)
2018-05-02 01:58:21.565020: step 8840, loss = 21.58 (3.2 examples/sec; 4.991 sec/batch)
2018-05-02 01:59:10.946151: step 8850, loss = 24.28 (3.3 examples/sec; 4.875 sec/batch)
2018-05-02 02:00:01.092937: step 8860, loss = 23.15 (3.3 examples/sec; 4.817 sec/batch)
2018-05-02 02:00:50.379521: step 8870, loss = 23.42 (3.2 examples/sec; 5.000 sec/batch)
2018-05-02 02:01:40.368164: step 8880, loss = 23.63 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 02:02:29.433345: step 8890, loss = 23.12 (3.2 examples/sec; 4.932 sec/batch)
2018-05-02 02:03:18.956471: step 8900, loss = 24.78 (3.2 examples/sec; 4.947 sec/batch)
2018-05-02 02:04:12.255384: step 8910, loss = 23.26 (3.1 examples/sec; 5.086 sec/batch)
2018-05-02 02:05:02.099242: step 8920, loss = 23.29 (3.2 examples/sec; 4.929 sec/batch)
2018-05-02 02:05:51.295748: step 8930, loss = 24.62 (3.2 examples/sec; 5.019 sec/batch)
2018-05-02 02:06:40.971919: step 8940, loss = 22.96 (3.3 examples/sec; 4.898 sec/batch)
2018-05-02 02:07:27.836457: step 8950, loss = 23.49 (3.2 examples/sec; 4.960 sec/batch)
2018-05-02 02:08:17.989311: step 8960, loss = 24.40 (3.1 examples/sec; 5.179 sec/batch)
2018-05-02 02:09:07.743513: step 8970, loss = 23.59 (3.2 examples/sec; 4.940 sec/batch)
2018-05-02 02:09:57.470537: step 8980, loss = 23.60 (3.3 examples/sec; 4.868 sec/batch)
2018-05-02 02:10:47.201631: step 8990, loss = 23.74 (3.2 examples/sec; 5.022 sec/batch)
2018-05-02 02:11:37.001272: step 9000, loss = 22.13 (3.2 examples/sec; 4.996 sec/batch)
2018-05-02 02:12:30.874207: step 9010, loss = 25.92 (3.3 examples/sec; 4.904 sec/batch)
2018-05-02 02:13:20.551591: step 9020, loss = 22.36 (3.1 examples/sec; 5.084 sec/batch)
2018-05-02 02:14:10.031918: step 9030, loss = 23.35 (3.4 examples/sec; 4.769 sec/batch)
2018-05-02 02:15:00.023364: step 9040, loss = 24.52 (3.3 examples/sec; 4.831 sec/batch)
2018-05-02 02:15:49.405110: step 9050, loss = 24.66 (3.3 examples/sec; 4.880 sec/batch)
2018-05-02 02:16:38.734619: step 9060, loss = 23.89 (3.2 examples/sec; 4.992 sec/batch)
2018-05-02 02:17:26.211149: step 9070, loss = 23.21 (3.3 examples/sec; 4.902 sec/batch)
2018-05-02 02:18:15.447448: step 9080, loss = 22.73 (3.2 examples/sec; 5.066 sec/batch)
2018-05-02 02:19:05.013017: step 9090, loss = 21.94 (3.4 examples/sec; 4.752 sec/batch)
2018-05-02 02:19:54.809657: step 9100, loss = 25.88 (3.2 examples/sec; 4.954 sec/batch)
2018-05-02 02:20:48.153242: step 9110, loss = 24.50 (3.3 examples/sec; 4.897 sec/batch)
2018-05-02 02:21:37.294096: step 9120, loss = 25.36 (3.2 examples/sec; 4.945 sec/batch)
2018-05-02 02:22:27.322493: step 9130, loss = 24.92 (3.3 examples/sec; 4.909 sec/batch)
2018-05-02 02:23:17.042829: step 9140, loss = 22.35 (3.1 examples/sec; 5.085 sec/batch)
2018-05-02 02:24:06.480268: step 9150, loss = 22.75 (3.3 examples/sec; 4.844 sec/batch)
2018-05-02 02:24:55.532455: step 9160, loss = 23.31 (3.1 examples/sec; 5.110 sec/batch)
2018-05-02 02:25:45.175174: step 9170, loss = 23.31 (3.2 examples/sec; 5.030 sec/batch)
2018-05-02 02:26:33.988210: step 9180, loss = 21.92 (3.3 examples/sec; 4.781 sec/batch)
2018-05-02 02:27:22.883265: step 9190, loss = 23.61 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 02:28:10.510250: step 9200, loss = 25.80 (3.2 examples/sec; 4.955 sec/batch)
2018-05-02 02:29:04.704694: step 9210, loss = 22.04 (3.2 examples/sec; 5.056 sec/batch)
2018-05-02 02:29:54.221930: step 9220, loss = 22.61 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 02:30:43.990387: step 9230, loss = 24.84 (3.3 examples/sec; 4.901 sec/batch)
2018-05-02 02:31:33.334725: step 9240, loss = 25.08 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 02:32:22.816129: step 9250, loss = 23.08 (3.2 examples/sec; 5.009 sec/batch)
2018-05-02 02:33:11.511558: step 9260, loss = 24.43 (3.3 examples/sec; 4.808 sec/batch)
2018-05-02 02:34:00.995779: step 9270, loss = 23.44 (3.3 examples/sec; 4.896 sec/batch)
2018-05-02 02:34:50.348657: step 9280, loss = 21.88 (3.2 examples/sec; 4.926 sec/batch)
2018-05-02 02:35:39.026144: step 9290, loss = 24.97 (3.3 examples/sec; 4.864 sec/batch)
2018-05-02 02:36:28.861003: step 9300, loss = 20.46 (3.2 examples/sec; 5.058 sec/batch)
2018-05-02 02:37:22.747402: step 9310, loss = 24.31 (3.4 examples/sec; 4.750 sec/batch)
2018-05-02 02:38:10.343691: step 9320, loss = 23.22 (3.2 examples/sec; 4.940 sec/batch)
2018-05-02 02:38:59.872412: step 9330, loss = 25.14 (3.2 examples/sec; 5.035 sec/batch)
2018-05-02 02:39:49.821947: step 9340, loss = 22.91 (3.3 examples/sec; 4.909 sec/batch)
2018-05-02 02:40:38.828087: step 9350, loss = 23.71 (3.3 examples/sec; 4.791 sec/batch)
2018-05-02 02:41:28.256886: step 9360, loss = 23.32 (3.2 examples/sec; 5.005 sec/batch)
2018-05-02 02:42:17.588428: step 9370, loss = 23.64 (3.3 examples/sec; 4.809 sec/batch)
2018-05-02 02:43:06.969442: step 9380, loss = 23.29 (3.2 examples/sec; 4.998 sec/batch)
2018-05-02 02:43:57.092815: step 9390, loss = 23.65 (3.2 examples/sec; 5.030 sec/batch)
2018-05-02 02:44:46.680408: step 9400, loss = 23.84 (3.3 examples/sec; 4.864 sec/batch)
2018-05-02 02:45:39.880666: step 9410, loss = 22.37 (3.2 examples/sec; 4.972 sec/batch)
2018-05-02 02:46:29.950328: step 9420, loss = 20.85 (3.2 examples/sec; 4.957 sec/batch)
2018-05-02 02:47:19.708892: step 9430, loss = 25.52 (3.2 examples/sec; 5.000 sec/batch)
2018-05-02 02:48:06.783516: step 9440, loss = 21.53 (4.2 examples/sec; 3.804 sec/batch)
2018-05-02 02:48:56.572399: step 9450, loss = 23.73 (3.2 examples/sec; 4.953 sec/batch)
2018-05-02 02:49:46.305056: step 9460, loss = 24.35 (3.3 examples/sec; 4.850 sec/batch)
2018-05-02 02:50:35.234175: step 9470, loss = 24.94 (3.3 examples/sec; 4.885 sec/batch)
2018-05-02 02:51:24.554070: step 9480, loss = 21.50 (3.2 examples/sec; 4.960 sec/batch)
2018-05-02 02:52:13.902229: step 9490, loss = 24.88 (3.3 examples/sec; 4.883 sec/batch)
2018-05-02 02:53:02.898030: step 9500, loss = 23.95 (3.3 examples/sec; 4.897 sec/batch)
2018-05-02 02:53:55.843751: step 9510, loss = 23.82 (3.2 examples/sec; 4.994 sec/batch)
2018-05-02 02:54:45.900853: step 9520, loss = 22.79 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 02:55:35.515700: step 9530, loss = 24.60 (3.2 examples/sec; 5.065 sec/batch)
2018-05-02 02:56:25.941701: step 9540, loss = 25.32 (3.1 examples/sec; 5.243 sec/batch)
2018-05-02 02:57:15.697126: step 9550, loss = 21.94 (3.2 examples/sec; 5.027 sec/batch)
2018-05-02 02:58:05.552861: step 9560, loss = 20.67 (3.2 examples/sec; 4.966 sec/batch)
2018-05-02 02:58:52.919233: step 9570, loss = 22.61 (3.3 examples/sec; 4.918 sec/batch)
2018-05-02 02:59:42.781747: step 9580, loss = 24.51 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 03:00:32.609513: step 9590, loss = 23.23 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 03:01:21.989346: step 9600, loss = 24.55 (3.3 examples/sec; 4.807 sec/batch)
2018-05-02 03:02:14.335602: step 9610, loss = 23.18 (3.2 examples/sec; 4.990 sec/batch)
2018-05-02 03:03:03.292003: step 9620, loss = 22.94 (3.3 examples/sec; 4.889 sec/batch)
2018-05-02 03:03:53.153771: step 9630, loss = 20.64 (3.1 examples/sec; 5.115 sec/batch)
2018-05-02 03:04:42.805540: step 9640, loss = 25.46 (3.2 examples/sec; 5.021 sec/batch)
2018-05-02 03:05:32.693866: step 9650, loss = 22.80 (3.2 examples/sec; 4.962 sec/batch)
2018-05-02 03:06:22.143682: step 9660, loss = 23.06 (3.1 examples/sec; 5.153 sec/batch)
2018-05-02 03:07:12.048470: step 9670, loss = 23.19 (3.4 examples/sec; 4.735 sec/batch)
2018-05-02 03:08:01.915680: step 9680, loss = 22.65 (3.2 examples/sec; 4.961 sec/batch)
2018-05-02 03:08:48.903968: step 9690, loss = 24.19 (3.2 examples/sec; 5.041 sec/batch)
2018-05-02 03:09:38.425121: step 9700, loss = 21.43 (3.2 examples/sec; 4.931 sec/batch)
2018-05-02 03:10:31.258769: step 9710, loss = 21.92 (3.3 examples/sec; 4.783 sec/batch)
2018-05-02 03:11:20.990229: step 9720, loss = 21.89 (3.1 examples/sec; 5.089 sec/batch)
2018-05-02 03:12:10.959332: step 9730, loss = 21.55 (3.3 examples/sec; 4.880 sec/batch)
2018-05-02 03:13:00.544376: step 9740, loss = 23.07 (3.3 examples/sec; 4.905 sec/batch)
2018-05-02 03:13:50.387882: step 9750, loss = 26.29 (3.3 examples/sec; 4.825 sec/batch)
2018-05-02 03:14:40.317811: step 9760, loss = 22.34 (3.2 examples/sec; 5.071 sec/batch)
2018-05-02 03:15:30.367980: step 9770, loss = 23.80 (3.2 examples/sec; 4.989 sec/batch)
2018-05-02 03:16:19.356510: step 9780, loss = 23.36 (3.3 examples/sec; 4.874 sec/batch)
2018-05-02 03:17:08.434147: step 9790, loss = 23.15 (3.3 examples/sec; 4.914 sec/batch)
2018-05-02 03:17:57.538371: step 9800, loss = 22.23 (3.3 examples/sec; 4.908 sec/batch)
2018-05-02 03:18:48.219047: step 9810, loss = 20.33 (4.2 examples/sec; 3.819 sec/batch)
2018-05-02 03:19:36.956891: step 9820, loss = 22.52 (3.3 examples/sec; 4.906 sec/batch)
2018-05-02 03:20:26.390947: step 9830, loss = 22.44 (3.1 examples/sec; 5.131 sec/batch)
2018-05-02 03:21:15.912529: step 9840, loss = 22.46 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 03:22:05.066958: step 9850, loss = 23.25 (3.3 examples/sec; 4.857 sec/batch)
2018-05-02 03:22:54.698191: step 9860, loss = 23.53 (3.3 examples/sec; 4.877 sec/batch)
2018-05-02 03:23:44.150365: step 9870, loss = 24.93 (3.2 examples/sec; 5.035 sec/batch)
2018-05-02 03:24:34.078668: step 9880, loss = 22.10 (3.2 examples/sec; 4.940 sec/batch)
2018-05-02 03:25:23.380858: step 9890, loss = 23.76 (3.2 examples/sec; 4.932 sec/batch)
2018-05-02 03:26:13.185374: step 9900, loss = 21.87 (3.1 examples/sec; 5.187 sec/batch)
2018-05-02 03:27:06.175808: step 9910, loss = 21.31 (3.2 examples/sec; 5.054 sec/batch)
2018-05-02 03:27:56.036580: step 9920, loss = 24.80 (3.2 examples/sec; 5.072 sec/batch)
2018-05-02 03:28:46.359723: step 9930, loss = 24.22 (3.3 examples/sec; 4.908 sec/batch)
2018-05-02 03:29:33.302932: step 9940, loss = 24.28 (3.3 examples/sec; 4.890 sec/batch)
2018-05-02 03:30:22.748470: step 9950, loss = 23.03 (3.3 examples/sec; 4.922 sec/batch)
2018-05-02 03:31:11.928425: step 9960, loss = 21.19 (3.2 examples/sec; 5.011 sec/batch)
2018-05-02 03:32:01.273126: step 9970, loss = 25.71 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 03:32:50.508257: step 9980, loss = 22.79 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 03:33:39.897456: step 9990, loss = 22.67 (3.2 examples/sec; 5.058 sec/batch)
2018-05-02 03:34:29.268748: step 10000, loss = 24.19 (3.1 examples/sec; 5.197 sec/batch)
2018-05-02 03:35:23.321716: step 10010, loss = 20.92 (3.2 examples/sec; 5.043 sec/batch)
2018-05-02 03:36:12.996267: step 10020, loss = 24.98 (3.3 examples/sec; 4.870 sec/batch)
2018-05-02 03:37:02.326320: step 10030, loss = 20.70 (3.2 examples/sec; 4.943 sec/batch)
2018-05-02 03:37:51.951998: step 10040, loss = 24.92 (3.3 examples/sec; 4.919 sec/batch)
2018-05-02 03:38:41.353739: step 10050, loss = 22.02 (3.2 examples/sec; 4.977 sec/batch)
2018-05-02 03:39:27.925761: step 10060, loss = 22.15 (3.4 examples/sec; 4.674 sec/batch)
2018-05-02 03:40:17.461291: step 10070, loss = 22.11 (3.3 examples/sec; 4.915 sec/batch)
2018-05-02 03:41:07.329025: step 10080, loss = 21.86 (3.2 examples/sec; 5.040 sec/batch)
2018-05-02 03:41:57.099704: step 10090, loss = 22.03 (3.0 examples/sec; 5.255 sec/batch)
2018-05-02 03:42:46.540916: step 10100, loss = 24.92 (3.3 examples/sec; 4.872 sec/batch)
2018-05-02 03:43:39.324414: step 10110, loss = 22.75 (3.1 examples/sec; 5.095 sec/batch)
2018-05-02 03:44:29.069549: step 10120, loss = 22.14 (3.2 examples/sec; 5.050 sec/batch)
2018-05-02 03:45:17.660585: step 10130, loss = 23.42 (3.3 examples/sec; 4.886 sec/batch)
2018-05-02 03:46:06.575835: step 10140, loss = 22.35 (3.3 examples/sec; 4.884 sec/batch)
2018-05-02 03:46:55.909571: step 10150, loss = 23.09 (3.3 examples/sec; 4.824 sec/batch)
2018-05-02 03:47:46.036238: step 10160, loss = 23.04 (3.2 examples/sec; 4.981 sec/batch)
2018-05-02 03:48:35.454660: step 10170, loss = 23.19 (3.3 examples/sec; 4.858 sec/batch)
2018-05-02 03:49:24.747108: step 10180, loss = 24.54 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 03:50:11.185034: step 10190, loss = 22.67 (3.3 examples/sec; 4.917 sec/batch)
2018-05-02 03:51:00.970665: step 10200, loss = 24.68 (3.3 examples/sec; 4.912 sec/batch)
2018-05-02 03:51:53.894145: step 10210, loss = 21.94 (3.3 examples/sec; 4.794 sec/batch)
2018-05-02 03:52:43.438341: step 10220, loss = 22.47 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 03:53:32.870019: step 10230, loss = 23.41 (3.3 examples/sec; 4.842 sec/batch)
2018-05-02 03:54:22.272780: step 10240, loss = 24.90 (3.2 examples/sec; 5.039 sec/batch)
2018-05-02 03:55:12.233947: step 10250, loss = 23.31 (3.1 examples/sec; 5.163 sec/batch)
2018-05-02 03:56:01.548982: step 10260, loss = 22.21 (3.3 examples/sec; 4.853 sec/batch)
2018-05-02 03:56:51.120060: step 10270, loss = 22.21 (3.3 examples/sec; 4.857 sec/batch)
2018-05-02 03:57:40.528932: step 10280, loss = 23.02 (3.2 examples/sec; 5.049 sec/batch)
2018-05-02 03:58:29.797049: step 10290, loss = 24.32 (3.3 examples/sec; 4.883 sec/batch)
2018-05-02 03:59:19.260974: step 10300, loss = 25.29 (3.2 examples/sec; 4.928 sec/batch)
2018-05-02 04:00:09.948143: step 10310, loss = 21.67 (3.3 examples/sec; 4.862 sec/batch)
2018-05-02 04:00:59.583727: step 10320, loss = 26.01 (3.2 examples/sec; 5.048 sec/batch)
2018-05-02 04:01:48.814838: step 10330, loss = 23.41 (3.3 examples/sec; 4.797 sec/batch)
2018-05-02 04:02:38.068146: step 10340, loss = 22.50 (3.4 examples/sec; 4.731 sec/batch)
2018-05-02 04:03:27.030572: step 10350, loss = 23.44 (3.2 examples/sec; 5.010 sec/batch)
2018-05-02 04:04:15.875636: step 10360, loss = 21.04 (3.3 examples/sec; 4.914 sec/batch)
2018-05-02 04:05:05.446342: step 10370, loss = 21.21 (3.1 examples/sec; 5.238 sec/batch)
2018-05-02 04:05:55.536163: step 10380, loss = 25.34 (3.3 examples/sec; 4.857 sec/batch)
2018-05-02 04:06:44.326440: step 10390, loss = 22.13 (3.3 examples/sec; 4.815 sec/batch)
2018-05-02 04:07:33.880789: step 10400, loss = 24.27 (3.3 examples/sec; 4.916 sec/batch)
2018-05-02 04:08:27.149135: step 10410, loss = 22.13 (3.3 examples/sec; 4.838 sec/batch)
2018-05-02 04:09:16.740633: step 10420, loss = 22.81 (3.1 examples/sec; 5.106 sec/batch)
2018-05-02 04:10:06.042846: step 10430, loss = 23.77 (4.0 examples/sec; 4.003 sec/batch)
2018-05-02 04:10:54.048075: step 10440, loss = 23.43 (3.2 examples/sec; 4.971 sec/batch)
2018-05-02 04:11:43.977083: step 10450, loss = 22.92 (3.2 examples/sec; 4.965 sec/batch)
2018-05-02 04:12:33.470326: step 10460, loss = 22.49 (3.3 examples/sec; 4.889 sec/batch)
2018-05-02 04:13:22.503163: step 10470, loss = 23.33 (3.2 examples/sec; 5.007 sec/batch)
2018-05-02 04:14:12.336537: step 10480, loss = 23.56 (3.2 examples/sec; 5.064 sec/batch)
2018-05-02 04:15:01.883241: step 10490, loss = 21.59 (3.4 examples/sec; 4.772 sec/batch)
2018-05-02 04:15:51.435426: step 10500, loss = 20.90 (3.2 examples/sec; 4.981 sec/batch)
2018-05-02 04:16:43.945017: step 10510, loss = 22.57 (3.3 examples/sec; 4.887 sec/batch)
2018-05-02 04:17:32.801017: step 10520, loss = 21.00 (3.2 examples/sec; 4.957 sec/batch)
2018-05-02 04:18:22.132309: step 10530, loss = 22.33 (3.2 examples/sec; 4.948 sec/batch)
2018-05-02 04:19:11.948881: step 10540, loss = 24.35 (3.3 examples/sec; 4.846 sec/batch)
2018-05-02 04:20:01.799087: step 10550, loss = 25.49 (3.2 examples/sec; 4.949 sec/batch)
2018-05-02 04:20:47.978236: step 10560, loss = 24.05 (3.2 examples/sec; 4.977 sec/batch)
2018-05-02 04:21:37.643195: step 10570, loss = 23.96 (3.2 examples/sec; 4.974 sec/batch)
2018-05-02 04:22:27.284819: step 10580, loss = 20.73 (3.3 examples/sec; 4.830 sec/batch)
2018-05-02 04:23:16.514817: step 10590, loss = 21.98 (3.3 examples/sec; 4.873 sec/batch)
2018-05-02 04:24:05.613342: step 10600, loss = 26.15 (3.2 examples/sec; 4.979 sec/batch)
2018-05-02 04:24:59.731177: step 10610, loss = 24.91 (3.2 examples/sec; 5.076 sec/batch)
2018-05-02 04:25:49.096109: step 10620, loss = 22.22 (3.3 examples/sec; 4.884 sec/batch)
2018-05-02 04:26:38.973115: step 10630, loss = 21.32 (3.2 examples/sec; 5.071 sec/batch)
2018-05-02 04:27:28.648504: step 10640, loss = 20.75 (3.3 examples/sec; 4.884 sec/batch)
2018-05-02 04:28:18.148207: step 10650, loss = 22.09 (3.2 examples/sec; 5.013 sec/batch)
2018-05-02 04:29:07.967716: step 10660, loss = 23.81 (3.3 examples/sec; 4.858 sec/batch)
2018-05-02 04:29:57.479772: step 10670, loss = 21.22 (3.2 examples/sec; 5.011 sec/batch)
2018-05-02 04:30:44.201144: step 10680, loss = 21.55 (3.5 examples/sec; 4.543 sec/batch)
2018-05-02 04:31:34.555402: step 10690, loss = 21.31 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 04:32:24.315788: step 10700, loss = 22.11 (3.2 examples/sec; 5.071 sec/batch)
2018-05-02 04:33:17.429411: step 10710, loss = 22.67 (3.2 examples/sec; 5.021 sec/batch)
2018-05-02 04:34:07.369597: step 10720, loss = 24.37 (3.2 examples/sec; 4.924 sec/batch)
2018-05-02 04:34:57.110778: step 10730, loss = 22.83 (3.3 examples/sec; 4.907 sec/batch)
2018-05-02 04:35:46.559563: step 10740, loss = 21.80 (3.3 examples/sec; 4.906 sec/batch)
2018-05-02 04:36:35.766944: step 10750, loss = 22.38 (3.2 examples/sec; 5.052 sec/batch)
2018-05-02 04:37:25.690383: step 10760, loss = 23.65 (3.3 examples/sec; 4.781 sec/batch)
2018-05-02 04:38:15.731456: step 10770, loss = 22.58 (3.2 examples/sec; 4.953 sec/batch)
2018-05-02 04:39:05.932400: step 10780, loss = 20.42 (3.3 examples/sec; 4.840 sec/batch)
2018-05-02 04:39:55.057563: step 10790, loss = 21.24 (3.2 examples/sec; 4.927 sec/batch)
2018-05-02 04:40:44.181831: step 10800, loss = 21.65 (3.3 examples/sec; 4.856 sec/batch)
2018-05-02 04:41:34.639225: step 10810, loss = 23.73 (3.2 examples/sec; 5.030 sec/batch)
2018-05-02 04:42:25.063322: step 10820, loss = 23.16 (3.2 examples/sec; 5.046 sec/batch)
2018-05-02 04:43:14.130278: step 10830, loss = 22.15 (3.3 examples/sec; 4.916 sec/batch)
2018-05-02 04:44:03.870612: step 10840, loss = 23.80 (3.1 examples/sec; 5.162 sec/batch)
2018-05-02 04:44:53.173808: step 10850, loss = 22.76 (3.3 examples/sec; 4.871 sec/batch)
2018-05-02 04:45:42.858933: step 10860, loss = 22.00 (3.3 examples/sec; 4.907 sec/batch)
2018-05-02 04:46:32.580684: step 10870, loss = 22.17 (3.4 examples/sec; 4.744 sec/batch)
2018-05-02 04:47:22.111861: step 10880, loss = 23.13 (3.3 examples/sec; 4.804 sec/batch)
2018-05-02 04:48:12.036680: step 10890, loss = 21.61 (3.1 examples/sec; 5.119 sec/batch)
2018-05-02 04:49:02.240930: step 10900, loss = 22.90 (3.2 examples/sec; 4.996 sec/batch)
2018-05-02 04:49:54.917868: step 10910, loss = 21.73 (3.2 examples/sec; 4.984 sec/batch)
2018-05-02 04:50:44.103647: step 10920, loss = 20.44 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 04:51:30.654360: step 10930, loss = 22.66 (3.2 examples/sec; 5.026 sec/batch)
2018-05-02 04:52:20.670153: step 10940, loss = 21.85 (3.2 examples/sec; 5.019 sec/batch)
2018-05-02 04:53:10.656935: step 10950, loss = 22.56 (3.3 examples/sec; 4.863 sec/batch)
2018-05-02 04:54:00.093730: step 10960, loss = 22.49 (3.2 examples/sec; 5.011 sec/batch)
2018-05-02 04:54:50.278170: step 10970, loss = 23.45 (3.2 examples/sec; 5.068 sec/batch)
2018-05-02 04:55:39.803321: step 10980, loss = 22.52 (3.3 examples/sec; 4.877 sec/batch)
2018-05-02 04:56:29.330892: step 10990, loss = 23.76 (3.2 examples/sec; 5.007 sec/batch)
2018-05-02 04:57:18.553279: step 11000, loss = 22.36 (3.3 examples/sec; 4.835 sec/batch)
2018-05-02 04:58:12.152480: step 11010, loss = 23.06 (3.1 examples/sec; 5.095 sec/batch)
2018-05-02 04:59:01.753672: step 11020, loss = 21.78 (3.3 examples/sec; 4.911 sec/batch)
2018-05-02 04:59:51.566247: step 11030, loss = 23.37 (3.3 examples/sec; 4.883 sec/batch)
2018-05-02 05:00:41.221492: step 11040, loss = 23.76 (3.3 examples/sec; 4.887 sec/batch)
2018-05-02 05:01:28.222182: step 11050, loss = 22.62 (3.4 examples/sec; 4.649 sec/batch)
2018-05-02 05:02:17.444124: step 11060, loss = 25.81 (3.2 examples/sec; 4.975 sec/batch)
2018-05-02 05:03:07.258988: step 11070, loss = 23.98 (3.2 examples/sec; 4.978 sec/batch)
2018-05-02 05:03:56.680572: step 11080, loss = 22.69 (3.3 examples/sec; 4.789 sec/batch)
2018-05-02 05:04:46.738096: step 11090, loss = 22.69 (3.2 examples/sec; 5.050 sec/batch)
2018-05-02 05:05:36.143750: step 11100, loss = 23.46 (3.2 examples/sec; 4.987 sec/batch)
2018-05-02 05:06:29.151110: step 11110, loss = 22.59 (3.3 examples/sec; 4.848 sec/batch)
2018-05-02 05:07:18.593720: step 11120, loss = 23.34 (3.2 examples/sec; 5.052 sec/batch)
2018-05-02 05:08:08.136576: step 11130, loss = 22.20 (3.2 examples/sec; 4.933 sec/batch)
2018-05-02 05:08:57.908955: step 11140, loss = 20.52 (3.3 examples/sec; 4.904 sec/batch)
2018-05-02 05:09:47.314276: step 11150, loss = 21.25 (3.3 examples/sec; 4.904 sec/batch)
2018-05-02 05:10:37.047474: step 11160, loss = 22.85 (3.3 examples/sec; 4.880 sec/batch)
2018-05-02 05:11:26.630272: step 11170, loss = 19.89 (3.2 examples/sec; 4.949 sec/batch)
2018-05-02 05:12:14.130741: step 11180, loss = 23.74 (3.1 examples/sec; 5.130 sec/batch)
2018-05-02 05:13:03.657099: step 11190, loss = 21.49 (3.2 examples/sec; 5.041 sec/batch)
2018-05-02 05:13:53.633961: step 11200, loss = 21.37 (3.2 examples/sec; 5.001 sec/batch)
2018-05-02 05:14:47.314575: step 11210, loss = 23.57 (3.1 examples/sec; 5.086 sec/batch)
2018-05-02 05:15:36.907054: step 11220, loss = 22.17 (3.1 examples/sec; 5.089 sec/batch)
2018-05-02 05:16:26.773881: step 11230, loss = 24.61 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 05:17:16.481775: step 11240, loss = 21.13 (3.2 examples/sec; 5.038 sec/batch)
2018-05-02 05:18:05.698999: step 11250, loss = 20.68 (3.3 examples/sec; 4.882 sec/batch)
2018-05-02 05:18:55.590218: step 11260, loss = 22.13 (3.4 examples/sec; 4.745 sec/batch)
2018-05-02 05:19:45.410325: step 11270, loss = 21.81 (3.2 examples/sec; 4.979 sec/batch)
2018-05-02 05:20:35.250271: step 11280, loss = 22.40 (3.2 examples/sec; 4.929 sec/batch)
2018-05-02 05:21:25.333869: step 11290, loss = 22.62 (3.1 examples/sec; 5.091 sec/batch)
2018-05-02 05:22:12.173820: step 11300, loss = 22.15 (3.2 examples/sec; 5.041 sec/batch)
2018-05-02 05:23:05.120049: step 11310, loss = 22.47 (3.2 examples/sec; 4.978 sec/batch)
2018-05-02 05:23:54.717763: step 11320, loss = 22.43 (3.3 examples/sec; 4.912 sec/batch)
2018-05-02 05:24:44.566303: step 11330, loss = 24.19 (3.2 examples/sec; 5.009 sec/batch)
2018-05-02 05:25:33.441958: step 11340, loss = 22.74 (3.3 examples/sec; 4.829 sec/batch)
2018-05-02 05:26:22.522131: step 11350, loss = 21.93 (3.3 examples/sec; 4.813 sec/batch)
2018-05-02 05:27:12.856036: step 11360, loss = 21.27 (3.1 examples/sec; 5.114 sec/batch)
2018-05-02 05:28:02.326754: step 11370, loss = 22.18 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 05:28:51.941689: step 11380, loss = 24.73 (3.4 examples/sec; 4.737 sec/batch)
2018-05-02 05:29:41.137195: step 11390, loss = 21.51 (3.2 examples/sec; 4.926 sec/batch)
2018-05-02 05:30:31.238079: step 11400, loss = 21.80 (3.1 examples/sec; 5.121 sec/batch)
2018-05-02 05:31:24.657935: step 11410, loss = 22.34 (3.2 examples/sec; 4.949 sec/batch)
2018-05-02 05:32:11.143858: step 11420, loss = 22.44 (3.3 examples/sec; 4.817 sec/batch)
2018-05-02 05:33:00.970398: step 11430, loss = 22.51 (3.2 examples/sec; 5.029 sec/batch)
2018-05-02 05:33:50.177595: step 11440, loss = 21.53 (3.4 examples/sec; 4.690 sec/batch)
2018-05-02 05:34:39.930949: step 11450, loss = 23.06 (3.2 examples/sec; 5.044 sec/batch)
2018-05-02 05:35:29.915668: step 11460, loss = 21.91 (3.2 examples/sec; 4.939 sec/batch)
2018-05-02 05:36:19.477853: step 11470, loss = 20.93 (3.3 examples/sec; 4.907 sec/batch)
2018-05-02 05:37:09.091356: step 11480, loss = 22.42 (3.2 examples/sec; 4.934 sec/batch)
2018-05-02 05:37:58.164629: step 11490, loss = 26.02 (3.2 examples/sec; 4.939 sec/batch)
2018-05-02 05:38:48.689618: step 11500, loss = 24.17 (3.2 examples/sec; 5.031 sec/batch)
2018-05-02 05:39:42.246001: step 11510, loss = 22.13 (3.2 examples/sec; 4.951 sec/batch)
2018-05-02 05:40:31.535715: step 11520, loss = 21.38 (3.2 examples/sec; 5.021 sec/batch)
2018-05-02 05:41:21.886426: step 11530, loss = 22.55 (3.2 examples/sec; 5.002 sec/batch)
2018-05-02 05:42:10.010145: step 11540, loss = 22.19 (4.2 examples/sec; 3.830 sec/batch)
2018-05-02 05:42:57.862138: step 11550, loss = 20.85 (3.3 examples/sec; 4.907 sec/batch)
2018-05-02 05:43:47.318575: step 11560, loss = 23.59 (3.2 examples/sec; 4.929 sec/batch)
2018-05-02 05:44:36.948504: step 11570, loss = 21.13 (3.2 examples/sec; 4.933 sec/batch)
2018-05-02 05:45:26.253198: step 11580, loss = 22.20 (3.1 examples/sec; 5.178 sec/batch)
2018-05-02 05:46:15.134693: step 11590, loss = 23.72 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 05:47:05.272724: step 11600, loss = 22.32 (3.3 examples/sec; 4.915 sec/batch)
2018-05-02 05:47:57.728872: step 11610, loss = 21.89 (3.5 examples/sec; 4.582 sec/batch)
2018-05-02 05:48:47.747437: step 11620, loss = 21.71 (3.2 examples/sec; 4.975 sec/batch)
2018-05-02 05:49:37.321713: step 11630, loss = 20.61 (3.1 examples/sec; 5.118 sec/batch)
2018-05-02 05:50:26.847104: step 11640, loss = 21.74 (3.3 examples/sec; 4.850 sec/batch)
2018-05-02 05:51:16.514624: step 11650, loss = 25.63 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 05:52:06.012723: step 11660, loss = 22.83 (3.1 examples/sec; 5.103 sec/batch)
2018-05-02 05:52:52.753127: step 11670, loss = 21.10 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 05:53:42.445351: step 11680, loss = 22.03 (3.2 examples/sec; 5.052 sec/batch)
2018-05-02 05:54:32.149976: step 11690, loss = 21.55 (3.2 examples/sec; 4.977 sec/batch)
2018-05-02 05:55:22.162165: step 11700, loss = 23.92 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 05:56:15.770336: step 11710, loss = 20.79 (3.1 examples/sec; 5.222 sec/batch)
2018-05-02 05:57:05.922301: step 11720, loss = 21.55 (3.3 examples/sec; 4.914 sec/batch)
2018-05-02 05:57:55.904146: step 11730, loss = 20.98 (3.2 examples/sec; 5.007 sec/batch)
2018-05-02 05:58:45.395502: step 11740, loss = 20.99 (3.1 examples/sec; 5.084 sec/batch)
2018-05-02 05:59:35.112172: step 11750, loss = 21.69 (3.0 examples/sec; 5.272 sec/batch)
2018-05-02 06:00:24.895278: step 11760, loss = 21.24 (3.2 examples/sec; 5.034 sec/batch)
2018-05-02 06:01:14.315645: step 11770, loss = 21.06 (3.3 examples/sec; 4.894 sec/batch)
2018-05-02 06:02:04.412804: step 11780, loss = 22.33 (3.2 examples/sec; 4.950 sec/batch)
2018-05-02 06:02:51.589086: step 11790, loss = 22.03 (3.1 examples/sec; 5.184 sec/batch)
2018-05-02 06:03:40.979883: step 11800, loss = 21.37 (3.3 examples/sec; 4.914 sec/batch)
2018-05-02 06:04:34.057270: step 11810, loss = 23.22 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 06:05:23.928990: step 11820, loss = 21.78 (3.1 examples/sec; 5.194 sec/batch)
2018-05-02 06:06:16.035572: step 11830, loss = 20.73 (2.5 examples/sec; 6.292 sec/batch)
2018-05-02 06:07:06.907562: step 11840, loss = 21.70 (3.3 examples/sec; 4.902 sec/batch)
2018-05-02 06:07:56.382079: step 11850, loss = 21.95 (3.4 examples/sec; 4.742 sec/batch)
2018-05-02 06:08:45.548077: step 11860, loss = 23.12 (3.1 examples/sec; 5.127 sec/batch)
2018-05-02 06:09:34.451233: step 11870, loss = 20.60 (3.4 examples/sec; 4.710 sec/batch)
2018-05-02 06:10:24.036686: step 11880, loss = 21.47 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 06:11:13.565334: step 11890, loss = 22.10 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 06:12:03.323007: step 11900, loss = 21.91 (3.2 examples/sec; 4.977 sec/batch)
2018-05-02 06:12:54.325234: step 11910, loss = 20.83 (4.2 examples/sec; 3.791 sec/batch)
2018-05-02 06:13:42.263048: step 11920, loss = 21.22 (3.2 examples/sec; 4.963 sec/batch)
2018-05-02 06:14:31.455939: step 11930, loss = 20.60 (3.2 examples/sec; 5.024 sec/batch)
2018-05-02 06:15:21.000642: step 11940, loss = 21.60 (3.1 examples/sec; 5.105 sec/batch)
2018-05-02 06:16:10.990447: step 11950, loss = 22.68 (3.2 examples/sec; 4.963 sec/batch)
2018-05-02 06:17:00.520785: step 11960, loss = 19.80 (3.2 examples/sec; 5.059 sec/batch)
2018-05-02 06:17:50.066337: step 11970, loss = 23.09 (3.3 examples/sec; 4.810 sec/batch)
2018-05-02 06:18:39.625783: step 11980, loss = 22.16 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 06:19:29.189661: step 11990, loss = 21.28 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 06:20:19.317959: step 12000, loss = 22.50 (3.2 examples/sec; 4.979 sec/batch)
2018-05-02 06:21:12.862490: step 12010, loss = 21.81 (3.2 examples/sec; 4.950 sec/batch)
2018-05-02 06:22:01.730792: step 12020, loss = 21.89 (3.2 examples/sec; 5.002 sec/batch)
2018-05-02 06:22:51.693593: step 12030, loss = 21.68 (3.1 examples/sec; 5.131 sec/batch)
2018-05-02 06:23:38.941220: step 12040, loss = 22.94 (3.2 examples/sec; 4.998 sec/batch)
2018-05-02 06:24:28.500179: step 12050, loss = 22.18 (3.2 examples/sec; 5.017 sec/batch)
2018-05-02 06:25:18.440760: step 12060, loss = 21.64 (3.1 examples/sec; 5.148 sec/batch)
2018-05-02 06:26:08.202466: step 12070, loss = 22.60 (3.1 examples/sec; 5.082 sec/batch)
2018-05-02 06:26:57.764753: step 12080, loss = 21.64 (3.1 examples/sec; 5.093 sec/batch)
2018-05-02 06:27:47.488291: step 12090, loss = 21.79 (3.3 examples/sec; 4.891 sec/batch)
2018-05-02 06:28:36.558602: step 12100, loss = 21.53 (3.2 examples/sec; 4.949 sec/batch)
2018-05-02 06:29:29.495798: step 12110, loss = 21.86 (3.2 examples/sec; 4.934 sec/batch)
2018-05-02 06:30:18.843491: step 12120, loss = 22.21 (3.1 examples/sec; 5.087 sec/batch)
2018-05-02 06:31:08.317210: step 12130, loss = 22.05 (3.3 examples/sec; 4.886 sec/batch)
2018-05-02 06:31:58.208491: step 12140, loss = 20.72 (3.2 examples/sec; 4.999 sec/batch)
2018-05-02 06:32:47.873842: step 12150, loss = 23.14 (3.2 examples/sec; 5.015 sec/batch)
2018-05-02 06:33:34.625061: step 12160, loss = 21.81 (3.2 examples/sec; 4.982 sec/batch)
2018-05-02 06:34:23.724645: step 12170, loss = 22.07 (3.4 examples/sec; 4.681 sec/batch)
2018-05-02 06:35:13.375552: step 12180, loss = 20.34 (3.3 examples/sec; 4.824 sec/batch)
2018-05-02 06:36:02.964960: step 12190, loss = 22.67 (3.2 examples/sec; 4.985 sec/batch)
2018-05-02 06:36:53.141091: step 12200, loss = 24.15 (3.1 examples/sec; 5.110 sec/batch)
2018-05-02 06:37:46.022754: step 12210, loss = 22.54 (3.3 examples/sec; 4.907 sec/batch)
2018-05-02 06:38:35.297417: step 12220, loss = 21.43 (3.5 examples/sec; 4.592 sec/batch)
2018-05-02 06:39:24.826008: step 12230, loss = 22.31 (3.2 examples/sec; 4.951 sec/batch)
2018-05-02 06:40:14.754802: step 12240, loss = 20.80 (3.3 examples/sec; 4.886 sec/batch)
2018-05-02 06:41:04.686510: step 12250, loss = 21.78 (3.1 examples/sec; 5.200 sec/batch)
2018-05-02 06:41:56.982445: step 12260, loss = 21.85 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 06:42:46.153179: step 12270, loss = 20.99 (3.3 examples/sec; 4.898 sec/batch)
2018-05-02 06:43:35.040997: step 12280, loss = 24.14 (4.2 examples/sec; 3.847 sec/batch)
2018-05-02 06:44:23.248171: step 12290, loss = 21.46 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 06:45:12.587098: step 12300, loss = 21.31 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 06:46:06.362535: step 12310, loss = 22.60 (3.1 examples/sec; 5.088 sec/batch)
2018-05-02 06:46:55.929412: step 12320, loss = 23.10 (3.3 examples/sec; 4.783 sec/batch)
2018-05-02 06:47:45.354198: step 12330, loss = 21.15 (3.3 examples/sec; 4.879 sec/batch)
2018-05-02 06:48:34.679931: step 12340, loss = 21.67 (3.3 examples/sec; 4.902 sec/batch)
2018-05-02 06:49:24.263569: step 12350, loss = 19.51 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 06:50:13.280746: step 12360, loss = 22.97 (3.1 examples/sec; 5.093 sec/batch)
2018-05-02 06:51:03.449250: step 12370, loss = 23.54 (3.2 examples/sec; 5.012 sec/batch)
2018-05-02 06:51:54.056906: step 12380, loss = 21.92 (3.2 examples/sec; 5.033 sec/batch)
2018-05-02 06:52:43.916390: step 12390, loss = 22.32 (3.3 examples/sec; 4.820 sec/batch)
2018-05-02 06:53:33.787100: step 12400, loss = 21.39 (3.2 examples/sec; 5.077 sec/batch)
2018-05-02 06:54:24.655064: step 12410, loss = 21.48 (3.2 examples/sec; 5.037 sec/batch)
2018-05-02 06:55:14.121101: step 12420, loss = 22.32 (3.4 examples/sec; 4.766 sec/batch)
2018-05-02 06:56:04.576123: step 12430, loss = 21.74 (3.2 examples/sec; 5.066 sec/batch)
2018-05-02 06:56:53.808443: step 12440, loss = 22.22 (3.1 examples/sec; 5.108 sec/batch)
2018-05-02 06:57:43.110065: step 12450, loss = 21.82 (3.2 examples/sec; 5.059 sec/batch)
2018-05-02 06:58:32.391916: step 12460, loss = 20.56 (3.2 examples/sec; 5.056 sec/batch)
2018-05-02 06:59:22.168304: step 12470, loss = 22.06 (3.2 examples/sec; 4.994 sec/batch)
2018-05-02 07:00:11.615739: step 12480, loss = 20.71 (3.2 examples/sec; 5.015 sec/batch)
2018-05-02 07:01:01.253172: step 12490, loss = 22.55 (3.3 examples/sec; 4.916 sec/batch)
2018-05-02 07:01:51.107876: step 12500, loss = 22.64 (3.2 examples/sec; 4.978 sec/batch)
2018-05-02 07:02:44.479208: step 12510, loss = 21.03 (3.2 examples/sec; 5.079 sec/batch)
2018-05-02 07:03:33.997909: step 12520, loss = 21.49 (3.3 examples/sec; 4.870 sec/batch)
2018-05-02 07:04:21.289774: step 12530, loss = 22.85 (3.1 examples/sec; 5.093 sec/batch)
2018-05-02 07:05:10.937732: step 12540, loss = 23.84 (3.2 examples/sec; 4.925 sec/batch)
2018-05-02 07:06:01.014670: step 12550, loss = 22.40 (3.2 examples/sec; 5.012 sec/batch)
2018-05-02 07:06:50.442090: step 12560, loss = 20.63 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 07:07:40.539218: step 12570, loss = 22.15 (3.2 examples/sec; 4.952 sec/batch)
2018-05-02 07:08:29.690700: step 12580, loss = 22.03 (3.2 examples/sec; 4.953 sec/batch)
2018-05-02 07:09:19.595350: step 12590, loss = 21.12 (3.1 examples/sec; 5.175 sec/batch)
2018-05-02 07:10:09.775703: step 12600, loss = 22.70 (3.1 examples/sec; 5.080 sec/batch)
2018-05-02 07:11:02.853010: step 12610, loss = 22.04 (3.2 examples/sec; 5.001 sec/batch)
2018-05-02 07:11:52.468604: step 12620, loss = 20.13 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 07:12:42.008846: step 12630, loss = 22.14 (3.2 examples/sec; 4.937 sec/batch)
2018-05-02 07:13:31.600358: step 12640, loss = 22.82 (3.2 examples/sec; 4.978 sec/batch)
2018-05-02 07:14:18.433434: step 12650, loss = 22.68 (3.2 examples/sec; 5.068 sec/batch)
2018-05-02 07:15:08.914445: step 12660, loss = 21.79 (3.1 examples/sec; 5.093 sec/batch)
2018-05-02 07:15:59.064497: step 12670, loss = 23.27 (3.2 examples/sec; 4.998 sec/batch)
2018-05-02 07:16:48.809353: step 12680, loss = 22.51 (3.2 examples/sec; 5.003 sec/batch)
2018-05-02 07:17:37.755225: step 12690, loss = 21.36 (3.2 examples/sec; 4.963 sec/batch)
2018-05-02 07:18:27.075600: step 12700, loss = 22.59 (3.3 examples/sec; 4.797 sec/batch)
2018-05-02 07:19:20.634377: step 12710, loss = 20.81 (3.3 examples/sec; 4.877 sec/batch)
2018-05-02 07:20:10.517097: step 12720, loss = 20.68 (3.3 examples/sec; 4.918 sec/batch)
2018-05-02 07:21:00.137933: step 12730, loss = 22.62 (3.3 examples/sec; 4.880 sec/batch)
2018-05-02 07:21:49.752125: step 12740, loss = 20.92 (3.2 examples/sec; 5.070 sec/batch)
2018-05-02 07:22:39.365731: step 12750, loss = 21.71 (3.2 examples/sec; 4.952 sec/batch)
2018-05-02 07:23:28.464809: step 12760, loss = 24.17 (3.3 examples/sec; 4.911 sec/batch)
2018-05-02 07:24:16.960274: step 12770, loss = 23.01 (4.2 examples/sec; 3.816 sec/batch)
2018-05-02 07:25:05.460940: step 12780, loss = 23.17 (3.2 examples/sec; 4.971 sec/batch)
2018-05-02 07:25:54.598963: step 12790, loss = 22.22 (3.3 examples/sec; 4.812 sec/batch)
2018-05-02 07:26:43.351388: step 12800, loss = 20.94 (3.2 examples/sec; 4.944 sec/batch)
2018-05-02 07:27:35.990131: step 12810, loss = 23.11 (3.3 examples/sec; 4.923 sec/batch)
2018-05-02 07:28:24.569706: step 12820, loss = 20.07 (3.3 examples/sec; 4.835 sec/batch)
2018-05-02 07:29:14.326984: step 12830, loss = 21.53 (3.2 examples/sec; 5.027 sec/batch)
2018-05-02 07:30:04.578076: step 12840, loss = 20.69 (3.2 examples/sec; 5.028 sec/batch)
2018-05-02 07:30:53.809185: step 12850, loss = 22.03 (3.3 examples/sec; 4.816 sec/batch)
2018-05-02 07:31:43.811585: step 12860, loss = 22.03 (3.1 examples/sec; 5.134 sec/batch)
2018-05-02 07:32:33.557505: step 12870, loss = 22.43 (3.2 examples/sec; 5.019 sec/batch)
2018-05-02 07:33:23.185935: step 12880, loss = 21.00 (3.2 examples/sec; 4.980 sec/batch)
2018-05-02 07:34:12.594458: step 12890, loss = 22.16 (3.4 examples/sec; 4.748 sec/batch)
2018-05-02 07:34:59.177056: step 12900, loss = 22.42 (3.2 examples/sec; 4.951 sec/batch)
2018-05-02 07:35:52.366369: step 12910, loss = 20.51 (3.2 examples/sec; 4.934 sec/batch)
2018-05-02 07:36:41.696895: step 12920, loss = 22.26 (3.2 examples/sec; 5.043 sec/batch)
2018-05-02 07:37:31.467251: step 12930, loss = 21.43 (3.3 examples/sec; 4.880 sec/batch)
2018-05-02 07:38:19.906443: step 12940, loss = 21.46 (3.3 examples/sec; 4.777 sec/batch)
2018-05-02 07:39:10.142921: step 12950, loss = 20.09 (3.2 examples/sec; 5.025 sec/batch)
2018-05-02 07:39:59.127537: step 12960, loss = 21.39 (3.2 examples/sec; 4.955 sec/batch)
2018-05-02 07:40:48.536738: step 12970, loss = 20.86 (3.2 examples/sec; 4.927 sec/batch)
2018-05-02 07:41:36.948140: step 12980, loss = 22.08 (3.4 examples/sec; 4.739 sec/batch)
2018-05-02 07:42:26.232645: step 12990, loss = 22.77 (3.3 examples/sec; 4.850 sec/batch)
2018-05-02 07:43:16.970755: step 13000, loss = 22.54 (3.1 examples/sec; 5.163 sec/batch)
2018-05-02 07:44:09.446077: step 13010, loss = 21.15 (3.2 examples/sec; 4.952 sec/batch)
2018-05-02 07:44:55.981498: step 13020, loss = 23.15 (3.3 examples/sec; 4.907 sec/batch)
2018-05-02 07:45:45.451659: step 13030, loss = 22.94 (3.3 examples/sec; 4.916 sec/batch)
2018-05-02 07:46:34.874618: step 13040, loss = 20.89 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 07:47:23.742883: step 13050, loss = 20.56 (3.3 examples/sec; 4.896 sec/batch)
2018-05-02 07:48:12.838372: step 13060, loss = 21.44 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 07:49:02.695535: step 13070, loss = 22.34 (3.2 examples/sec; 4.998 sec/batch)
2018-05-02 07:49:52.201308: step 13080, loss = 22.29 (3.2 examples/sec; 4.959 sec/batch)
2018-05-02 07:50:41.811126: step 13090, loss = 20.24 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 07:51:30.914636: step 13100, loss = 22.15 (3.1 examples/sec; 5.081 sec/batch)
2018-05-02 07:52:23.820936: step 13110, loss = 21.55 (3.3 examples/sec; 4.916 sec/batch)
2018-05-02 07:53:13.062766: step 13120, loss = 19.38 (3.2 examples/sec; 5.018 sec/batch)
2018-05-02 07:54:02.539808: step 13130, loss = 22.46 (3.2 examples/sec; 4.968 sec/batch)
2018-05-02 07:54:52.377990: step 13140, loss = 21.53 (3.2 examples/sec; 4.994 sec/batch)
2018-05-02 07:55:39.802877: step 13150, loss = 20.32 (3.2 examples/sec; 4.994 sec/batch)
2018-05-02 07:56:29.101971: step 13160, loss = 22.94 (3.3 examples/sec; 4.866 sec/batch)
2018-05-02 07:57:18.771812: step 13170, loss = 21.09 (3.3 examples/sec; 4.886 sec/batch)
2018-05-02 07:58:08.526155: step 13180, loss = 20.72 (3.2 examples/sec; 5.057 sec/batch)
2018-05-02 07:58:58.708473: step 13190, loss = 22.09 (3.2 examples/sec; 5.014 sec/batch)
2018-05-02 07:59:48.602732: step 13200, loss = 21.09 (3.2 examples/sec; 4.928 sec/batch)
2018-05-02 08:00:41.692138: step 13210, loss = 20.57 (3.3 examples/sec; 4.783 sec/batch)
2018-05-02 08:01:31.019184: step 13220, loss = 21.58 (3.2 examples/sec; 4.965 sec/batch)
2018-05-02 08:02:19.798421: step 13230, loss = 21.25 (3.3 examples/sec; 4.798 sec/batch)
2018-05-02 08:03:09.060902: step 13240, loss = 20.25 (3.2 examples/sec; 5.034 sec/batch)
2018-05-02 08:03:58.301736: step 13250, loss = 21.39 (3.3 examples/sec; 4.880 sec/batch)
2018-05-02 08:04:48.717145: step 13260, loss = 21.05 (3.1 examples/sec; 5.104 sec/batch)
2018-05-02 08:05:35.831173: step 13270, loss = 21.54 (3.1 examples/sec; 5.205 sec/batch)
2018-05-02 08:06:25.337130: step 13280, loss = 23.89 (3.3 examples/sec; 4.819 sec/batch)
2018-05-02 08:07:15.011499: step 13290, loss = 22.32 (3.2 examples/sec; 5.079 sec/batch)
2018-05-02 08:08:05.192389: step 13300, loss = 21.77 (3.2 examples/sec; 5.026 sec/batch)
2018-05-02 08:08:58.753265: step 13310, loss = 21.83 (3.1 examples/sec; 5.187 sec/batch)
2018-05-02 08:09:48.517428: step 13320, loss = 22.53 (3.2 examples/sec; 4.935 sec/batch)
2018-05-02 08:10:37.906317: step 13330, loss = 23.57 (3.3 examples/sec; 4.779 sec/batch)
2018-05-02 08:11:28.119470: step 13340, loss = 20.90 (3.1 examples/sec; 5.106 sec/batch)
2018-05-02 08:12:18.188948: step 13350, loss = 20.40 (3.3 examples/sec; 4.911 sec/batch)
2018-05-02 08:13:08.179854: step 13360, loss = 22.03 (3.2 examples/sec; 5.044 sec/batch)
2018-05-02 08:13:58.203548: step 13370, loss = 22.89 (3.3 examples/sec; 4.871 sec/batch)
2018-05-02 08:14:48.436843: step 13380, loss = 22.28 (3.1 examples/sec; 5.207 sec/batch)
2018-05-02 08:15:35.375264: step 13390, loss = 20.63 (3.6 examples/sec; 4.415 sec/batch)
2018-05-02 08:16:24.985209: step 13400, loss = 22.31 (3.2 examples/sec; 4.943 sec/batch)
2018-05-02 08:17:17.744787: step 13410, loss = 21.56 (3.3 examples/sec; 4.873 sec/batch)
2018-05-02 08:18:07.708367: step 13420, loss = 21.51 (3.3 examples/sec; 4.872 sec/batch)
2018-05-02 08:18:57.229621: step 13430, loss = 21.38 (3.2 examples/sec; 4.988 sec/batch)
2018-05-02 08:19:46.074079: step 13440, loss = 21.38 (3.2 examples/sec; 5.038 sec/batch)
2018-05-02 08:20:36.121441: step 13450, loss = 21.61 (3.3 examples/sec; 4.794 sec/batch)
2018-05-02 08:21:25.868822: step 13460, loss = 20.20 (3.2 examples/sec; 4.926 sec/batch)
2018-05-02 08:22:15.940908: step 13470, loss = 22.29 (3.2 examples/sec; 5.030 sec/batch)
2018-05-02 08:23:05.667797: step 13480, loss = 21.80 (3.2 examples/sec; 4.950 sec/batch)
2018-05-02 08:23:55.339617: step 13490, loss = 21.45 (3.2 examples/sec; 5.029 sec/batch)
2018-05-02 08:24:45.251401: step 13500, loss = 22.60 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 08:25:37.463591: step 13510, loss = 22.46 (4.0 examples/sec; 4.004 sec/batch)
2018-05-02 08:26:25.255194: step 13520, loss = 21.36 (3.2 examples/sec; 4.961 sec/batch)
2018-05-02 08:27:14.789232: step 13530, loss = 21.75 (3.2 examples/sec; 4.975 sec/batch)
2018-05-02 08:28:03.793329: step 13540, loss = 19.89 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 08:28:53.685386: step 13550, loss = 20.67 (3.3 examples/sec; 4.902 sec/batch)
2018-05-02 08:29:43.826411: step 13560, loss = 21.41 (3.2 examples/sec; 4.962 sec/batch)
2018-05-02 08:30:33.032799: step 13570, loss = 20.40 (3.3 examples/sec; 4.851 sec/batch)
2018-05-02 08:31:21.929005: step 13580, loss = 21.05 (3.3 examples/sec; 4.896 sec/batch)
2018-05-02 08:32:11.358850: step 13590, loss = 21.58 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 08:33:00.594566: step 13600, loss = 24.16 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 08:33:53.610253: step 13610, loss = 22.11 (3.3 examples/sec; 4.865 sec/batch)
2018-05-02 08:34:42.334993: step 13620, loss = 19.57 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 08:35:31.665055: step 13630, loss = 22.37 (3.2 examples/sec; 4.924 sec/batch)
2018-05-02 08:36:18.659925: step 13640, loss = 21.37 (3.1 examples/sec; 5.124 sec/batch)
2018-05-02 08:37:08.433033: step 13650, loss = 21.93 (3.2 examples/sec; 5.001 sec/batch)
2018-05-02 08:37:58.428161: step 13660, loss = 21.84 (3.2 examples/sec; 5.023 sec/batch)
2018-05-02 08:38:47.882897: step 13670, loss = 20.37 (3.3 examples/sec; 4.887 sec/batch)
2018-05-02 08:39:37.180046: step 13680, loss = 21.09 (3.2 examples/sec; 5.007 sec/batch)
2018-05-02 08:40:26.589194: step 13690, loss = 20.16 (3.3 examples/sec; 4.885 sec/batch)
2018-05-02 08:41:15.692616: step 13700, loss = 20.15 (3.4 examples/sec; 4.743 sec/batch)
2018-05-02 08:42:08.815903: step 13710, loss = 21.08 (3.3 examples/sec; 4.861 sec/batch)
2018-05-02 08:42:58.243256: step 13720, loss = 22.52 (3.2 examples/sec; 4.952 sec/batch)
2018-05-02 08:43:48.249450: step 13730, loss = 22.40 (3.3 examples/sec; 4.919 sec/batch)
2018-05-02 08:44:38.876669: step 13740, loss = 20.65 (3.0 examples/sec; 5.276 sec/batch)
2018-05-02 08:45:28.851284: step 13750, loss = 21.05 (3.3 examples/sec; 4.839 sec/batch)
2018-05-02 08:46:16.143304: step 13760, loss = 21.06 (3.5 examples/sec; 4.567 sec/batch)
2018-05-02 08:47:05.904821: step 13770, loss = 20.21 (3.2 examples/sec; 4.993 sec/batch)
2018-05-02 08:47:55.000295: step 13780, loss = 20.13 (3.3 examples/sec; 4.906 sec/batch)
2018-05-02 08:48:44.319947: step 13790, loss = 20.86 (3.3 examples/sec; 4.780 sec/batch)
2018-05-02 08:49:34.239091: step 13800, loss = 20.93 (3.1 examples/sec; 5.122 sec/batch)
2018-05-02 08:50:26.601854: step 13810, loss = 20.83 (3.4 examples/sec; 4.767 sec/batch)
2018-05-02 08:51:16.252562: step 13820, loss = 21.31 (3.1 examples/sec; 5.149 sec/batch)
2018-05-02 08:52:06.308877: step 13830, loss = 22.39 (3.2 examples/sec; 5.002 sec/batch)
2018-05-02 08:52:55.505143: step 13840, loss = 24.51 (3.2 examples/sec; 5.043 sec/batch)
2018-05-02 08:53:44.908370: step 13850, loss = 20.68 (3.3 examples/sec; 4.822 sec/batch)
2018-05-02 08:54:34.835341: step 13860, loss = 23.23 (3.1 examples/sec; 5.210 sec/batch)
2018-05-02 08:55:25.148995: step 13870, loss = 20.74 (3.2 examples/sec; 4.934 sec/batch)
2018-05-02 08:56:14.538971: step 13880, loss = 22.30 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 08:57:01.457249: step 13890, loss = 22.09 (3.2 examples/sec; 5.008 sec/batch)
2018-05-02 08:57:51.231182: step 13900, loss = 20.72 (3.2 examples/sec; 4.960 sec/batch)
2018-05-02 08:58:45.291082: step 13910, loss = 21.23 (3.1 examples/sec; 5.108 sec/batch)
2018-05-02 08:59:35.175467: step 13920, loss = 20.09 (3.2 examples/sec; 5.055 sec/batch)
2018-05-02 09:00:24.701524: step 13930, loss = 20.64 (3.2 examples/sec; 4.945 sec/batch)
2018-05-02 09:01:14.860998: step 13940, loss = 20.70 (3.3 examples/sec; 4.868 sec/batch)
2018-05-02 09:02:04.409039: step 13950, loss = 20.92 (3.3 examples/sec; 4.865 sec/batch)
2018-05-02 09:02:54.355541: step 13960, loss = 20.50 (3.1 examples/sec; 5.209 sec/batch)
2018-05-02 09:03:43.391273: step 13970, loss = 20.26 (3.2 examples/sec; 4.962 sec/batch)
2018-05-02 09:04:32.922502: step 13980, loss = 20.98 (3.2 examples/sec; 4.976 sec/batch)
2018-05-02 09:05:22.222958: step 13990, loss = 22.63 (3.3 examples/sec; 4.850 sec/batch)
2018-05-02 09:06:11.421996: step 14000, loss = 22.51 (3.2 examples/sec; 5.001 sec/batch)
2018-05-02 09:07:01.959008: step 14010, loss = 22.43 (3.2 examples/sec; 4.961 sec/batch)
2018-05-02 09:07:51.206530: step 14020, loss = 21.28 (3.3 examples/sec; 4.812 sec/batch)
2018-05-02 09:08:40.878264: step 14030, loss = 22.07 (3.2 examples/sec; 5.037 sec/batch)
2018-05-02 09:09:30.300267: step 14040, loss = 20.51 (3.2 examples/sec; 4.966 sec/batch)
2018-05-02 09:10:19.538315: step 14050, loss = 21.12 (3.3 examples/sec; 4.871 sec/batch)
2018-05-02 09:11:09.288490: step 14060, loss = 22.14 (3.3 examples/sec; 4.857 sec/batch)
2018-05-02 09:11:59.013608: step 14070, loss = 20.81 (3.2 examples/sec; 4.936 sec/batch)
2018-05-02 09:12:48.042995: step 14080, loss = 22.73 (3.3 examples/sec; 4.888 sec/batch)
2018-05-02 09:13:37.272433: step 14090, loss = 20.81 (3.3 examples/sec; 4.831 sec/batch)
2018-05-02 09:14:27.358037: step 14100, loss = 21.44 (3.3 examples/sec; 4.818 sec/batch)
2018-05-02 09:15:19.991935: step 14110, loss = 20.82 (3.3 examples/sec; 4.805 sec/batch)
2018-05-02 09:16:09.087097: step 14120, loss = 20.23 (3.3 examples/sec; 4.897 sec/batch)
2018-05-02 09:16:55.934330: step 14130, loss = 21.90 (3.4 examples/sec; 4.661 sec/batch)
2018-05-02 09:17:44.990769: step 14140, loss = 20.47 (3.2 examples/sec; 4.929 sec/batch)
2018-05-02 09:18:35.183229: step 14150, loss = 21.22 (3.2 examples/sec; 4.953 sec/batch)
2018-05-02 09:19:25.239649: step 14160, loss = 21.97 (3.3 examples/sec; 4.878 sec/batch)
2018-05-02 09:20:15.074655: step 14170, loss = 20.71 (3.2 examples/sec; 5.059 sec/batch)
2018-05-02 09:21:04.401804: step 14180, loss = 21.28 (3.3 examples/sec; 4.841 sec/batch)
2018-05-02 09:21:54.121789: step 14190, loss = 21.77 (3.1 examples/sec; 5.110 sec/batch)
2018-05-02 09:22:43.635585: step 14200, loss = 19.94 (3.2 examples/sec; 4.936 sec/batch)
2018-05-02 09:23:36.742965: step 14210, loss = 20.28 (3.2 examples/sec; 4.944 sec/batch)
2018-05-02 09:24:26.326081: step 14220, loss = 20.78 (3.2 examples/sec; 5.075 sec/batch)
2018-05-02 09:25:16.676073: step 14230, loss = 22.68 (3.2 examples/sec; 5.074 sec/batch)
2018-05-02 09:26:06.186290: step 14240, loss = 21.30 (3.2 examples/sec; 5.060 sec/batch)
2018-05-02 09:26:54.514860: step 14250, loss = 21.51 (4.0 examples/sec; 4.005 sec/batch)
2018-05-02 09:27:42.337721: step 14260, loss = 22.55 (3.3 examples/sec; 4.899 sec/batch)
2018-05-02 09:28:31.820151: step 14270, loss = 21.84 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 09:29:21.524374: step 14280, loss = 19.85 (3.3 examples/sec; 4.897 sec/batch)
2018-05-02 09:30:10.530882: step 14290, loss = 21.49 (3.3 examples/sec; 4.892 sec/batch)
2018-05-02 09:30:59.682302: step 14300, loss = 22.36 (3.2 examples/sec; 4.924 sec/batch)
2018-05-02 09:31:53.812814: step 14310, loss = 20.44 (3.1 examples/sec; 5.108 sec/batch)
2018-05-02 09:32:43.248355: step 14320, loss = 20.43 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 09:33:33.366264: step 14330, loss = 21.23 (3.2 examples/sec; 4.979 sec/batch)
2018-05-02 09:34:22.892249: step 14340, loss = 21.63 (3.1 examples/sec; 5.107 sec/batch)
2018-05-02 09:35:12.256389: step 14350, loss = 21.11 (3.2 examples/sec; 4.952 sec/batch)
2018-05-02 09:36:01.724596: step 14360, loss = 21.54 (3.1 examples/sec; 5.115 sec/batch)
2018-05-02 09:36:51.428811: step 14370, loss = 20.03 (3.2 examples/sec; 4.973 sec/batch)
2018-05-02 09:37:38.184679: step 14380, loss = 22.62 (3.2 examples/sec; 4.980 sec/batch)
2018-05-02 09:38:28.761963: step 14390, loss = 20.52 (3.2 examples/sec; 5.054 sec/batch)
2018-05-02 09:39:18.543589: step 14400, loss = 19.79 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 09:40:11.351139: step 14410, loss = 21.06 (3.3 examples/sec; 4.835 sec/batch)
2018-05-02 09:41:00.654209: step 14420, loss = 21.67 (3.3 examples/sec; 4.880 sec/batch)
2018-05-02 09:41:50.150083: step 14430, loss = 20.43 (3.2 examples/sec; 5.039 sec/batch)
2018-05-02 09:42:39.375559: step 14440, loss = 22.34 (3.2 examples/sec; 4.972 sec/batch)
2018-05-02 09:43:29.165636: step 14450, loss = 21.86 (3.0 examples/sec; 5.258 sec/batch)
2018-05-02 09:44:18.747006: step 14460, loss = 20.34 (3.2 examples/sec; 5.019 sec/batch)
2018-05-02 09:45:08.259939: step 14470, loss = 21.32 (3.3 examples/sec; 4.889 sec/batch)
2018-05-02 09:45:58.213088: step 14480, loss = 19.98 (3.3 examples/sec; 4.802 sec/batch)
2018-05-02 09:46:48.311087: step 14490, loss = 20.47 (3.2 examples/sec; 4.966 sec/batch)
2018-05-02 09:47:34.975649: step 14500, loss = 19.74 (3.2 examples/sec; 5.070 sec/batch)
2018-05-02 09:48:28.266870: step 14510, loss = 21.14 (3.2 examples/sec; 5.037 sec/batch)
2018-05-02 09:49:17.692090: step 14520, loss = 20.08 (3.2 examples/sec; 4.994 sec/batch)
2018-05-02 09:50:07.065902: step 14530, loss = 20.19 (3.2 examples/sec; 5.060 sec/batch)
2018-05-02 09:50:56.609362: step 14540, loss = 19.64 (3.3 examples/sec; 4.843 sec/batch)
2018-05-02 09:51:46.480709: step 14550, loss = 20.50 (3.3 examples/sec; 4.912 sec/batch)
2018-05-02 09:52:36.434071: step 14560, loss = 20.28 (3.1 examples/sec; 5.104 sec/batch)
2018-05-02 09:53:25.972955: step 14570, loss = 20.13 (3.3 examples/sec; 4.858 sec/batch)
2018-05-02 09:54:15.445330: step 14580, loss = 21.91 (3.3 examples/sec; 4.895 sec/batch)
2018-05-02 09:55:04.908080: step 14590, loss = 23.05 (3.3 examples/sec; 4.846 sec/batch)
2018-05-02 09:55:55.431913: step 14600, loss = 19.97 (3.1 examples/sec; 5.098 sec/batch)
2018-05-02 09:56:48.313895: step 14610, loss = 21.19 (3.2 examples/sec; 4.974 sec/batch)
2018-05-02 09:57:35.437948: step 14620, loss = 18.73 (3.1 examples/sec; 5.107 sec/batch)
2018-05-02 09:58:24.526164: step 14630, loss = 21.52 (3.2 examples/sec; 4.934 sec/batch)
2018-05-02 09:59:13.947339: step 14640, loss = 20.20 (3.3 examples/sec; 4.911 sec/batch)
2018-05-02 10:00:03.006228: step 14650, loss = 20.84 (3.3 examples/sec; 4.914 sec/batch)
2018-05-02 10:00:53.244700: step 14660, loss = 21.07 (3.2 examples/sec; 4.977 sec/batch)
2018-05-02 10:01:42.132226: step 14670, loss = 21.13 (3.4 examples/sec; 4.732 sec/batch)
2018-05-02 10:02:31.826829: step 14680, loss = 23.38 (3.2 examples/sec; 4.928 sec/batch)
2018-05-02 10:03:22.603835: step 14690, loss = 22.64 (3.1 examples/sec; 5.169 sec/batch)
2018-05-02 10:04:11.760651: step 14700, loss = 20.91 (3.2 examples/sec; 4.972 sec/batch)
2018-05-02 10:05:04.456645: step 14710, loss = 21.23 (3.3 examples/sec; 4.888 sec/batch)
2018-05-02 10:05:53.533164: step 14720, loss = 20.94 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 10:06:42.511344: step 14730, loss = 22.27 (3.2 examples/sec; 4.932 sec/batch)
2018-05-02 10:07:31.360876: step 14740, loss = 22.64 (4.0 examples/sec; 4.032 sec/batch)
2018-05-02 10:08:18.397101: step 14750, loss = 21.73 (3.4 examples/sec; 4.700 sec/batch)
2018-05-02 10:09:08.043822: step 14760, loss = 21.51 (3.3 examples/sec; 4.809 sec/batch)
2018-05-02 10:09:56.914238: step 14770, loss = 21.38 (3.3 examples/sec; 4.854 sec/batch)
2018-05-02 10:10:45.994747: step 14780, loss = 24.97 (3.3 examples/sec; 4.796 sec/batch)
2018-05-02 10:11:35.406845: step 14790, loss = 21.98 (3.3 examples/sec; 4.903 sec/batch)
2018-05-02 10:12:24.975651: step 14800, loss = 20.36 (3.2 examples/sec; 4.924 sec/batch)
2018-05-02 10:13:17.870366: step 14810, loss = 22.11 (3.2 examples/sec; 4.945 sec/batch)
2018-05-02 10:14:07.592054: step 14820, loss = 21.40 (3.2 examples/sec; 5.003 sec/batch)
2018-05-02 10:14:56.583364: step 14830, loss = 21.32 (3.2 examples/sec; 5.010 sec/batch)
2018-05-02 10:15:46.072905: step 14840, loss = 21.62 (3.2 examples/sec; 4.958 sec/batch)
2018-05-02 10:16:35.234359: step 14850, loss = 22.10 (3.3 examples/sec; 4.830 sec/batch)
2018-05-02 10:17:24.335051: step 14860, loss = 20.28 (3.2 examples/sec; 4.941 sec/batch)
2018-05-02 10:18:11.018933: step 14870, loss = 19.67 (3.3 examples/sec; 4.892 sec/batch)
2018-05-02 10:19:00.485961: step 14880, loss = 20.66 (3.3 examples/sec; 4.793 sec/batch)
2018-05-02 10:19:49.575640: step 14890, loss = 20.23 (3.3 examples/sec; 4.902 sec/batch)
2018-05-02 10:20:39.330281: step 14900, loss = 19.82 (3.2 examples/sec; 4.933 sec/batch)
2018-05-02 10:21:32.800300: step 14910, loss = 19.44 (3.2 examples/sec; 4.960 sec/batch)
2018-05-02 10:22:22.030311: step 14920, loss = 21.18 (3.2 examples/sec; 4.927 sec/batch)
2018-05-02 10:23:11.961180: step 14930, loss = 20.07 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 10:24:01.134330: step 14940, loss = 22.73 (3.2 examples/sec; 4.968 sec/batch)
2018-05-02 10:24:49.977199: step 14950, loss = 20.68 (3.3 examples/sec; 4.888 sec/batch)
2018-05-02 10:25:39.530683: step 14960, loss = 19.22 (3.2 examples/sec; 4.975 sec/batch)
2018-05-02 10:26:28.397018: step 14970, loss = 20.08 (3.3 examples/sec; 4.852 sec/batch)
2018-05-02 10:27:17.103814: step 14980, loss = 22.37 (3.3 examples/sec; 4.919 sec/batch)
2018-05-02 10:28:03.909967: step 14990, loss = 20.85 (4.1 examples/sec; 3.883 sec/batch)
2018-05-02 10:28:53.489427: step 15000, loss = 20.18 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 10:29:46.784218: step 15010, loss = 20.57 (3.2 examples/sec; 5.015 sec/batch)
2018-05-02 10:30:36.249210: step 15020, loss = 19.32 (3.1 examples/sec; 5.132 sec/batch)
2018-05-02 10:31:25.644316: step 15030, loss = 21.42 (3.3 examples/sec; 4.922 sec/batch)
2018-05-02 10:32:15.627713: step 15040, loss = 21.90 (3.3 examples/sec; 4.866 sec/batch)
2018-05-02 10:33:04.648842: step 15050, loss = 20.81 (3.3 examples/sec; 4.873 sec/batch)
2018-05-02 10:33:53.747273: step 15060, loss = 19.97 (3.0 examples/sec; 5.263 sec/batch)
2018-05-02 10:34:43.823005: step 15070, loss = 21.68 (3.1 examples/sec; 5.235 sec/batch)
2018-05-02 10:35:33.486800: step 15080, loss = 21.41 (3.2 examples/sec; 4.996 sec/batch)
2018-05-02 10:36:23.542593: step 15090, loss = 20.58 (3.1 examples/sec; 5.119 sec/batch)
2018-05-02 10:37:13.075695: step 15100, loss = 21.66 (3.4 examples/sec; 4.697 sec/batch)
2018-05-02 10:38:05.888139: step 15110, loss = 21.16 (3.3 examples/sec; 4.834 sec/batch)
2018-05-02 10:38:52.494378: step 15120, loss = 20.22 (3.2 examples/sec; 5.048 sec/batch)
2018-05-02 10:39:41.769280: step 15130, loss = 20.54 (3.2 examples/sec; 4.943 sec/batch)
2018-05-02 10:40:31.960049: step 15140, loss = 19.56 (3.4 examples/sec; 4.770 sec/batch)
2018-05-02 10:41:21.516870: step 15150, loss = 22.93 (3.2 examples/sec; 4.961 sec/batch)
2018-05-02 10:42:10.902012: step 15160, loss = 22.12 (3.3 examples/sec; 4.904 sec/batch)
2018-05-02 10:43:00.751383: step 15170, loss = 20.13 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 10:43:50.742936: step 15180, loss = 20.69 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 10:44:40.214102: step 15190, loss = 22.71 (3.3 examples/sec; 4.885 sec/batch)
2018-05-02 10:45:29.517905: step 15200, loss = 20.66 (3.3 examples/sec; 4.911 sec/batch)
2018-05-02 10:46:22.760355: step 15210, loss = 21.42 (3.2 examples/sec; 4.936 sec/batch)
2018-05-02 10:47:11.793412: step 15220, loss = 22.72 (3.3 examples/sec; 4.917 sec/batch)
2018-05-02 10:48:01.999442: step 15230, loss = 21.33 (3.2 examples/sec; 4.971 sec/batch)
2018-05-02 10:48:48.316368: step 15240, loss = 21.52 (3.2 examples/sec; 4.983 sec/batch)
2018-05-02 10:49:37.807778: step 15250, loss = 20.35 (3.3 examples/sec; 4.892 sec/batch)
2018-05-02 10:50:27.182458: step 15260, loss = 20.60 (3.2 examples/sec; 4.946 sec/batch)
2018-05-02 10:51:16.148546: step 15270, loss = 20.55 (3.3 examples/sec; 4.876 sec/batch)
2018-05-02 10:52:05.331345: step 15280, loss = 21.44 (3.3 examples/sec; 4.909 sec/batch)
2018-05-02 10:52:55.027818: step 15290, loss = 21.31 (3.3 examples/sec; 4.903 sec/batch)
2018-05-02 10:53:44.445132: step 15300, loss = 22.31 (3.2 examples/sec; 4.949 sec/batch)
2018-05-02 10:54:37.976777: step 15310, loss = 21.33 (3.1 examples/sec; 5.083 sec/batch)
2018-05-02 10:55:27.324071: step 15320, loss = 20.29 (3.3 examples/sec; 4.826 sec/batch)
2018-05-02 10:56:17.233170: step 15330, loss = 20.81 (3.1 examples/sec; 5.092 sec/batch)
2018-05-02 10:57:06.469237: step 15340, loss = 20.95 (3.3 examples/sec; 4.863 sec/batch)
2018-05-02 10:57:55.771029: step 15350, loss = 20.56 (3.4 examples/sec; 4.730 sec/batch)
2018-05-02 10:58:42.699671: step 15360, loss = 20.74 (4.3 examples/sec; 3.695 sec/batch)
2018-05-02 10:59:32.507679: step 15370, loss = 20.05 (3.2 examples/sec; 4.939 sec/batch)
2018-05-02 11:00:21.708173: step 15380, loss = 22.77 (3.3 examples/sec; 4.889 sec/batch)
2018-05-02 11:01:11.330073: step 15390, loss = 21.66 (3.2 examples/sec; 5.054 sec/batch)
2018-05-02 11:02:00.851461: step 15400, loss = 21.04 (3.2 examples/sec; 5.032 sec/batch)
2018-05-02 11:02:53.522115: step 15410, loss = 20.36 (3.3 examples/sec; 4.854 sec/batch)
2018-05-02 11:03:43.306548: step 15420, loss = 20.46 (3.2 examples/sec; 4.974 sec/batch)
2018-05-02 11:04:32.135488: step 15430, loss = 22.19 (3.2 examples/sec; 5.018 sec/batch)
2018-05-02 11:05:21.882275: step 15440, loss = 21.95 (3.1 examples/sec; 5.145 sec/batch)
2018-05-02 11:06:11.799380: step 15450, loss = 21.49 (3.2 examples/sec; 4.990 sec/batch)
2018-05-02 11:07:01.575483: step 15460, loss = 20.81 (3.1 examples/sec; 5.129 sec/batch)
2018-05-02 11:07:50.511336: step 15470, loss = 21.73 (3.2 examples/sec; 4.937 sec/batch)
2018-05-02 11:08:39.699168: step 15480, loss = 20.77 (3.2 examples/sec; 4.974 sec/batch)
2018-05-02 11:09:26.306859: step 15490, loss = 20.84 (3.2 examples/sec; 4.948 sec/batch)
2018-05-02 11:10:15.951126: step 15500, loss = 20.36 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 11:11:09.144018: step 15510, loss = 20.08 (3.2 examples/sec; 5.004 sec/batch)
2018-05-02 11:11:58.976489: step 15520, loss = 20.07 (3.3 examples/sec; 4.857 sec/batch)
2018-05-02 11:12:48.434100: step 15530, loss = 22.68 (3.3 examples/sec; 4.913 sec/batch)
2018-05-02 11:13:37.887020: step 15540, loss = 22.07 (3.1 examples/sec; 5.080 sec/batch)
2018-05-02 11:14:26.968768: step 15550, loss = 20.16 (3.2 examples/sec; 5.060 sec/batch)
2018-05-02 11:15:16.513609: step 15560, loss = 21.47 (3.2 examples/sec; 5.044 sec/batch)
2018-05-02 11:16:06.156983: step 15570, loss = 22.00 (3.2 examples/sec; 4.975 sec/batch)
2018-05-02 11:16:56.108827: step 15580, loss = 19.93 (3.2 examples/sec; 5.033 sec/batch)
2018-05-02 11:17:45.312411: step 15590, loss = 21.44 (3.2 examples/sec; 4.995 sec/batch)
2018-05-02 11:18:35.129151: step 15600, loss = 21.29 (3.1 examples/sec; 5.101 sec/batch)
2018-05-02 11:19:25.461321: step 15610, loss = 20.83 (3.2 examples/sec; 5.050 sec/batch)
2018-05-02 11:20:15.456462: step 15620, loss = 20.71 (3.2 examples/sec; 4.990 sec/batch)
2018-05-02 11:21:04.307390: step 15630, loss = 21.00 (3.2 examples/sec; 4.937 sec/batch)
2018-05-02 11:21:53.716661: step 15640, loss = 20.33 (3.5 examples/sec; 4.631 sec/batch)
2018-05-02 11:22:42.647635: step 15650, loss = 21.92 (3.3 examples/sec; 4.838 sec/batch)
2018-05-02 11:23:32.288200: step 15660, loss = 20.47 (3.2 examples/sec; 4.970 sec/batch)
2018-05-02 11:24:21.984818: step 15670, loss = 20.02 (3.1 examples/sec; 5.095 sec/batch)
2018-05-02 11:25:11.161694: step 15680, loss = 20.81 (3.3 examples/sec; 4.799 sec/batch)
2018-05-02 11:26:00.751848: step 15690, loss = 19.77 (3.3 examples/sec; 4.910 sec/batch)
2018-05-02 11:26:50.123917: step 15700, loss = 19.73 (3.3 examples/sec; 4.884 sec/batch)
2018-05-02 11:27:43.966371: step 15710, loss = 21.06 (3.2 examples/sec; 5.057 sec/batch)
2018-05-02 11:28:33.887054: step 15720, loss = 19.84 (3.1 examples/sec; 5.104 sec/batch)
2018-05-02 11:29:21.725838: step 15730, loss = 21.33 (4.2 examples/sec; 3.785 sec/batch)
2018-05-02 11:30:10.144081: step 15740, loss = 21.19 (3.2 examples/sec; 4.983 sec/batch)
2018-05-02 11:30:59.577366: step 15750, loss = 21.69 (3.2 examples/sec; 4.975 sec/batch)
2018-05-02 11:31:48.696418: step 15760, loss = 20.36 (3.2 examples/sec; 4.970 sec/batch)
2018-05-02 11:32:37.392923: step 15770, loss = 20.75 (3.4 examples/sec; 4.722 sec/batch)
2018-05-02 11:33:26.968313: step 15780, loss = 20.23 (3.2 examples/sec; 5.038 sec/batch)
2018-05-02 11:34:16.299109: step 15790, loss = 20.14 (3.2 examples/sec; 4.958 sec/batch)
2018-05-02 11:35:05.460907: step 15800, loss = 20.23 (3.2 examples/sec; 4.962 sec/batch)
2018-05-02 11:35:57.891310: step 15810, loss = 19.84 (3.3 examples/sec; 4.831 sec/batch)
2018-05-02 11:36:47.306745: step 15820, loss = 22.50 (3.2 examples/sec; 5.072 sec/batch)
2018-05-02 11:37:36.847770: step 15830, loss = 19.97 (3.2 examples/sec; 5.001 sec/batch)
2018-05-02 11:38:26.050954: step 15840, loss = 21.65 (3.2 examples/sec; 4.992 sec/batch)
2018-05-02 11:39:15.369166: step 15850, loss = 20.93 (3.2 examples/sec; 4.999 sec/batch)
2018-05-02 11:40:02.101439: step 15860, loss = 21.60 (3.2 examples/sec; 5.049 sec/batch)
2018-05-02 11:40:51.428329: step 15870, loss = 19.68 (3.3 examples/sec; 4.838 sec/batch)
2018-05-02 11:41:40.659553: step 15880, loss = 21.99 (3.3 examples/sec; 4.864 sec/batch)
2018-05-02 11:42:29.406808: step 15890, loss = 20.78 (3.3 examples/sec; 4.873 sec/batch)
2018-05-02 11:43:18.684381: step 15900, loss = 20.86 (3.2 examples/sec; 5.021 sec/batch)
2018-05-02 11:44:11.679620: step 15910, loss = 22.13 (3.2 examples/sec; 5.001 sec/batch)
2018-05-02 11:45:00.608185: step 15920, loss = 22.53 (3.3 examples/sec; 4.854 sec/batch)
2018-05-02 11:45:49.903734: step 15930, loss = 19.69 (3.3 examples/sec; 4.803 sec/batch)
2018-05-02 11:46:39.182815: step 15940, loss = 21.24 (3.2 examples/sec; 5.039 sec/batch)
2018-05-02 11:47:28.726671: step 15950, loss = 20.22 (3.1 examples/sec; 5.091 sec/batch)
2018-05-02 11:48:17.698642: step 15960, loss = 20.59 (3.2 examples/sec; 4.975 sec/batch)
2018-05-02 11:49:07.257750: step 15970, loss = 20.05 (3.2 examples/sec; 4.929 sec/batch)
2018-05-02 11:49:54.986134: step 15980, loss = 19.25 (4.2 examples/sec; 3.821 sec/batch)
2018-05-02 11:50:43.698716: step 15990, loss = 21.37 (3.2 examples/sec; 5.016 sec/batch)
2018-05-02 11:51:33.065474: step 16000, loss = 20.18 (3.2 examples/sec; 4.959 sec/batch)
2018-05-02 11:52:26.177516: step 16010, loss = 20.35 (3.2 examples/sec; 4.995 sec/batch)
2018-05-02 11:53:15.480341: step 16020, loss = 20.80 (3.3 examples/sec; 4.776 sec/batch)
2018-05-02 11:54:05.163998: step 16030, loss = 20.52 (3.2 examples/sec; 5.005 sec/batch)
2018-05-02 11:54:54.308864: step 16040, loss = 21.24 (3.3 examples/sec; 4.799 sec/batch)
2018-05-02 11:55:44.335445: step 16050, loss = 20.42 (3.2 examples/sec; 4.977 sec/batch)
2018-05-02 11:56:33.564472: step 16060, loss = 21.07 (3.3 examples/sec; 4.887 sec/batch)
2018-05-02 11:57:22.447205: step 16070, loss = 20.39 (3.2 examples/sec; 4.961 sec/batch)
2018-05-02 11:58:12.103591: step 16080, loss = 19.41 (3.3 examples/sec; 4.809 sec/batch)
2018-05-02 11:59:00.910023: step 16090, loss = 21.68 (3.2 examples/sec; 4.928 sec/batch)
2018-05-02 11:59:49.972480: step 16100, loss = 21.51 (3.3 examples/sec; 4.842 sec/batch)
2018-05-02 12:00:39.612386: step 16110, loss = 22.20 (3.2 examples/sec; 5.033 sec/batch)
2018-05-02 12:01:29.166628: step 16120, loss = 20.63 (3.3 examples/sec; 4.896 sec/batch)
2018-05-02 12:02:18.509206: step 16130, loss = 21.19 (3.2 examples/sec; 5.078 sec/batch)
2018-05-02 12:03:08.019413: step 16140, loss = 21.17 (3.2 examples/sec; 4.972 sec/batch)
2018-05-02 12:03:57.473344: step 16150, loss = 21.73 (3.2 examples/sec; 4.985 sec/batch)
2018-05-02 12:04:47.060122: step 16160, loss = 20.57 (3.2 examples/sec; 5.022 sec/batch)
2018-05-02 12:05:36.284665: step 16170, loss = 19.84 (3.2 examples/sec; 5.010 sec/batch)
2018-05-02 12:06:25.374758: step 16180, loss = 20.70 (3.3 examples/sec; 4.847 sec/batch)
2018-05-02 12:07:15.054156: step 16190, loss = 19.93 (3.3 examples/sec; 4.893 sec/batch)
2018-05-02 12:08:04.009390: step 16200, loss = 19.37 (3.3 examples/sec; 4.870 sec/batch)
2018-05-02 12:08:56.971211: step 16210, loss = 20.42 (3.3 examples/sec; 4.890 sec/batch)
2018-05-02 12:09:46.784672: step 16220, loss = 23.08 (3.1 examples/sec; 5.184 sec/batch)
2018-05-02 12:10:33.538456: step 16230, loss = 21.24 (3.3 examples/sec; 4.794 sec/batch)
2018-05-02 12:11:22.820301: step 16240, loss = 20.22 (3.2 examples/sec; 4.973 sec/batch)
2018-05-02 12:12:12.779025: step 16250, loss = 20.71 (3.1 examples/sec; 5.124 sec/batch)
2018-05-02 12:13:01.962609: step 16260, loss = 20.14 (3.3 examples/sec; 4.870 sec/batch)
2018-05-02 12:13:52.112101: step 16270, loss = 19.86 (3.2 examples/sec; 4.963 sec/batch)
2018-05-02 12:14:41.514161: step 16280, loss = 19.94 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 12:15:30.912339: step 16290, loss = 21.06 (3.3 examples/sec; 4.789 sec/batch)
2018-05-02 12:16:19.550712: step 16300, loss = 19.91 (3.3 examples/sec; 4.913 sec/batch)
2018-05-02 12:17:12.127501: step 16310, loss = 20.58 (3.1 examples/sec; 5.093 sec/batch)
2018-05-02 12:18:02.121151: step 16320, loss = 19.79 (3.2 examples/sec; 5.007 sec/batch)
2018-05-02 12:18:52.621784: step 16330, loss = 20.69 (3.1 examples/sec; 5.081 sec/batch)
2018-05-02 12:19:42.082509: step 16340, loss = 21.57 (3.1 examples/sec; 5.139 sec/batch)
2018-05-02 12:20:31.336455: step 16350, loss = 21.39 (3.3 examples/sec; 4.914 sec/batch)
2018-05-02 12:21:18.057135: step 16360, loss = 20.19 (3.2 examples/sec; 5.001 sec/batch)
2018-05-02 12:22:07.376297: step 16370, loss = 20.02 (3.2 examples/sec; 4.957 sec/batch)
2018-05-02 12:22:56.096138: step 16380, loss = 20.19 (3.4 examples/sec; 4.700 sec/batch)
2018-05-02 12:23:45.419264: step 16390, loss = 19.14 (3.3 examples/sec; 4.899 sec/batch)
2018-05-02 12:24:34.572139: step 16400, loss = 21.89 (3.3 examples/sec; 4.906 sec/batch)
2018-05-02 12:25:28.184936: step 16410, loss = 21.22 (3.1 examples/sec; 5.093 sec/batch)
2018-05-02 12:26:17.951013: step 16420, loss = 22.50 (3.2 examples/sec; 5.019 sec/batch)
2018-05-02 12:27:07.069355: step 16430, loss = 21.90 (3.4 examples/sec; 4.771 sec/batch)
2018-05-02 12:27:56.545326: step 16440, loss = 20.83 (3.2 examples/sec; 5.021 sec/batch)
2018-05-02 12:28:45.985108: step 16450, loss = 20.73 (3.2 examples/sec; 5.032 sec/batch)
2018-05-02 12:29:35.123287: step 16460, loss = 20.41 (3.2 examples/sec; 4.978 sec/batch)
2018-05-02 12:30:24.429101: step 16470, loss = 20.25 (3.2 examples/sec; 5.034 sec/batch)
2018-05-02 12:31:10.717928: step 16480, loss = 20.64 (3.3 examples/sec; 4.837 sec/batch)
2018-05-02 12:31:59.945141: step 16490, loss = 20.99 (3.3 examples/sec; 4.782 sec/batch)
2018-05-02 12:32:49.611554: step 16500, loss = 20.83 (3.0 examples/sec; 5.247 sec/batch)
2018-05-02 12:33:42.935691: step 16510, loss = 21.96 (3.3 examples/sec; 4.904 sec/batch)
2018-05-02 12:34:32.469090: step 16520, loss = 20.44 (3.2 examples/sec; 4.934 sec/batch)
2018-05-02 12:35:21.477144: step 16530, loss = 20.35 (3.3 examples/sec; 4.888 sec/batch)
2018-05-02 12:36:10.693947: step 16540, loss = 20.82 (3.4 examples/sec; 4.768 sec/batch)
2018-05-02 12:37:00.630704: step 16550, loss = 19.59 (3.2 examples/sec; 4.994 sec/batch)
2018-05-02 12:37:49.288349: step 16560, loss = 21.36 (3.3 examples/sec; 4.888 sec/batch)
2018-05-02 12:38:38.166481: step 16570, loss = 21.42 (3.3 examples/sec; 4.897 sec/batch)
2018-05-02 12:39:27.594763: step 16580, loss = 20.39 (3.2 examples/sec; 4.999 sec/batch)
2018-05-02 12:40:16.696329: step 16590, loss = 20.77 (3.2 examples/sec; 4.936 sec/batch)
2018-05-02 12:41:06.243105: step 16600, loss = 21.55 (3.3 examples/sec; 4.916 sec/batch)
2018-05-02 12:41:56.384353: step 16610, loss = 19.89 (3.2 examples/sec; 4.974 sec/batch)
2018-05-02 12:42:45.656220: step 16620, loss = 20.29 (3.3 examples/sec; 4.879 sec/batch)
2018-05-02 12:43:35.515026: step 16630, loss = 19.39 (3.1 examples/sec; 5.162 sec/batch)
2018-05-02 12:44:24.396898: step 16640, loss = 20.92 (3.2 examples/sec; 4.984 sec/batch)
2018-05-02 12:45:14.684638: step 16650, loss = 20.82 (3.2 examples/sec; 4.953 sec/batch)
2018-05-02 12:46:03.905099: step 16660, loss = 20.46 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 12:46:53.517764: step 16670, loss = 20.46 (3.4 examples/sec; 4.776 sec/batch)
2018-05-02 12:47:42.311319: step 16680, loss = 19.38 (3.3 examples/sec; 4.901 sec/batch)
2018-05-02 12:48:31.014992: step 16690, loss = 21.59 (3.4 examples/sec; 4.761 sec/batch)
2018-05-02 12:49:20.712386: step 16700, loss = 21.45 (3.1 examples/sec; 5.087 sec/batch)
2018-05-02 12:50:13.791389: step 16710, loss = 20.89 (3.1 examples/sec; 5.239 sec/batch)
2018-05-02 12:51:04.287519: step 16720, loss = 19.91 (3.1 examples/sec; 5.179 sec/batch)
2018-05-02 12:51:51.200656: step 16730, loss = 21.57 (3.3 examples/sec; 4.860 sec/batch)
2018-05-02 12:52:40.406289: step 16740, loss = 18.85 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 12:53:29.846277: step 16750, loss = 22.80 (3.2 examples/sec; 4.988 sec/batch)
2018-05-02 12:54:19.256536: step 16760, loss = 21.64 (3.2 examples/sec; 4.936 sec/batch)
2018-05-02 12:55:08.939423: step 16770, loss = 21.36 (3.1 examples/sec; 5.082 sec/batch)
2018-05-02 12:55:58.941320: step 16780, loss = 20.13 (3.1 examples/sec; 5.085 sec/batch)
2018-05-02 12:56:48.130638: step 16790, loss = 21.20 (3.3 examples/sec; 4.836 sec/batch)
2018-05-02 12:57:37.638453: step 16800, loss = 21.09 (3.1 examples/sec; 5.083 sec/batch)
2018-05-02 12:58:31.451667: step 16810, loss = 21.00 (3.2 examples/sec; 4.945 sec/batch)
2018-05-02 12:59:21.012231: step 16820, loss = 20.94 (3.2 examples/sec; 4.939 sec/batch)
2018-05-02 13:00:09.968094: step 16830, loss = 20.24 (3.3 examples/sec; 4.863 sec/batch)
2018-05-02 13:00:59.603331: step 16840, loss = 19.90 (3.3 examples/sec; 4.883 sec/batch)
2018-05-02 13:01:46.457718: step 16850, loss = 21.67 (4.2 examples/sec; 3.808 sec/batch)
2018-05-02 13:02:35.829896: step 16860, loss = 19.16 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 13:03:25.615188: step 16870, loss = 20.10 (3.3 examples/sec; 4.800 sec/batch)
2018-05-02 13:04:14.732596: step 16880, loss = 19.77 (3.2 examples/sec; 5.006 sec/batch)
2018-05-02 13:05:04.541639: step 16890, loss = 20.66 (3.2 examples/sec; 4.939 sec/batch)
2018-05-02 13:05:54.272918: step 16900, loss = 19.61 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 13:06:47.066121: step 16910, loss = 21.25 (3.2 examples/sec; 4.924 sec/batch)
2018-05-02 13:07:35.827841: step 16920, loss = 20.52 (3.3 examples/sec; 4.880 sec/batch)
2018-05-02 13:08:25.062405: step 16930, loss = 19.78 (3.3 examples/sec; 4.863 sec/batch)
2018-05-02 13:09:14.870669: step 16940, loss = 21.01 (3.2 examples/sec; 4.954 sec/batch)
2018-05-02 13:10:04.622469: step 16950, loss = 20.10 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 13:10:54.164183: step 16960, loss = 19.87 (3.2 examples/sec; 5.011 sec/batch)
2018-05-02 13:11:43.359371: step 16970, loss = 20.91 (3.3 examples/sec; 4.822 sec/batch)
2018-05-02 13:12:30.187586: step 16980, loss = 19.96 (3.2 examples/sec; 4.985 sec/batch)
2018-05-02 13:13:19.623947: step 16990, loss = 21.11 (3.3 examples/sec; 4.862 sec/batch)
2018-05-02 13:14:08.741974: step 17000, loss = 22.32 (3.3 examples/sec; 4.849 sec/batch)
2018-05-02 13:15:01.369262: step 17010, loss = 19.29 (3.3 examples/sec; 4.828 sec/batch)
2018-05-02 13:15:50.191163: step 17020, loss = 21.53 (3.3 examples/sec; 4.919 sec/batch)
2018-05-02 13:16:39.542213: step 17030, loss = 20.82 (3.3 examples/sec; 4.920 sec/batch)
2018-05-02 13:17:28.321753: step 17040, loss = 21.18 (3.4 examples/sec; 4.736 sec/batch)
2018-05-02 13:18:17.721006: step 17050, loss = 20.31 (3.3 examples/sec; 4.877 sec/batch)
2018-05-02 13:19:06.815750: step 17060, loss = 20.12 (3.3 examples/sec; 4.890 sec/batch)
2018-05-02 13:19:56.062410: step 17070, loss = 21.40 (3.2 examples/sec; 5.023 sec/batch)
2018-05-02 13:20:45.307445: step 17080, loss = 21.89 (3.3 examples/sec; 4.838 sec/batch)
2018-05-02 13:21:34.843893: step 17090, loss = 20.17 (3.2 examples/sec; 4.968 sec/batch)
2018-05-02 13:22:21.868168: step 17100, loss = 22.18 (4.2 examples/sec; 3.791 sec/batch)
2018-05-02 13:23:15.244513: step 17110, loss = 19.91 (3.3 examples/sec; 4.898 sec/batch)
2018-05-02 13:24:04.797198: step 17120, loss = 22.71 (3.2 examples/sec; 5.032 sec/batch)
2018-05-02 13:24:54.438036: step 17130, loss = 19.24 (3.2 examples/sec; 5.023 sec/batch)
2018-05-02 13:25:43.717375: step 17140, loss = 21.24 (3.3 examples/sec; 4.896 sec/batch)
2018-05-02 13:26:33.758376: step 17150, loss = 19.81 (3.3 examples/sec; 4.910 sec/batch)
2018-05-02 13:27:23.138875: step 17160, loss = 20.19 (3.3 examples/sec; 4.815 sec/batch)
2018-05-02 13:28:12.522277: step 17170, loss = 20.42 (3.3 examples/sec; 4.803 sec/batch)
2018-05-02 13:29:02.097198: step 17180, loss = 20.66 (3.2 examples/sec; 4.968 sec/batch)
2018-05-02 13:29:52.316897: step 17190, loss = 20.80 (3.2 examples/sec; 5.009 sec/batch)
2018-05-02 13:30:42.227769: step 17200, loss = 21.01 (3.3 examples/sec; 4.804 sec/batch)
2018-05-02 13:31:34.949015: step 17210, loss = 20.53 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 13:32:24.461909: step 17220, loss = 20.20 (3.3 examples/sec; 4.884 sec/batch)
2018-05-02 13:33:10.724659: step 17230, loss = 21.34 (3.3 examples/sec; 4.784 sec/batch)
2018-05-02 13:33:59.888319: step 17240, loss = 18.51 (3.3 examples/sec; 4.790 sec/batch)
2018-05-02 13:34:49.270379: step 17250, loss = 20.47 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 13:35:38.817394: step 17260, loss = 22.40 (3.3 examples/sec; 4.881 sec/batch)
2018-05-02 13:36:28.080550: step 17270, loss = 21.03 (3.2 examples/sec; 5.008 sec/batch)
2018-05-02 13:37:17.285621: step 17280, loss = 21.89 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 13:38:06.335850: step 17290, loss = 19.37 (3.3 examples/sec; 4.905 sec/batch)
2018-05-02 13:38:55.758957: step 17300, loss = 20.72 (3.1 examples/sec; 5.145 sec/batch)
2018-05-02 13:39:48.251400: step 17310, loss = 20.12 (3.2 examples/sec; 4.996 sec/batch)
2018-05-02 13:40:37.909411: step 17320, loss = 20.34 (3.2 examples/sec; 4.943 sec/batch)
2018-05-02 13:41:27.904738: step 17330, loss = 19.67 (3.1 examples/sec; 5.099 sec/batch)
2018-05-02 13:42:16.922281: step 17340, loss = 22.01 (3.3 examples/sec; 4.905 sec/batch)
2018-05-02 13:43:03.674393: step 17350, loss = 20.56 (3.1 examples/sec; 5.081 sec/batch)
2018-05-02 13:43:52.774630: step 17360, loss = 19.43 (3.2 examples/sec; 5.055 sec/batch)
2018-05-02 13:44:42.193716: step 17370, loss = 21.22 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 13:45:31.989516: step 17380, loss = 20.17 (3.2 examples/sec; 5.038 sec/batch)
2018-05-02 13:46:21.836455: step 17390, loss = 18.47 (3.1 examples/sec; 5.164 sec/batch)
2018-05-02 13:47:11.579876: step 17400, loss = 20.47 (3.2 examples/sec; 4.977 sec/batch)
2018-05-02 13:48:05.105861: step 17410, loss = 19.58 (3.2 examples/sec; 5.018 sec/batch)
2018-05-02 13:48:54.258002: step 17420, loss = 20.18 (3.2 examples/sec; 4.977 sec/batch)
2018-05-02 13:49:43.918402: step 17430, loss = 20.98 (3.2 examples/sec; 4.959 sec/batch)
2018-05-02 13:50:33.578511: step 17440, loss = 20.44 (3.3 examples/sec; 4.881 sec/batch)
2018-05-02 13:51:22.904450: step 17450, loss = 19.18 (3.1 examples/sec; 5.105 sec/batch)
2018-05-02 13:52:12.968871: step 17460, loss = 21.27 (3.1 examples/sec; 5.098 sec/batch)
2018-05-02 13:53:01.817622: step 17470, loss = 20.33 (3.9 examples/sec; 4.132 sec/batch)
2018-05-02 13:53:49.034664: step 17480, loss = 19.64 (3.2 examples/sec; 5.068 sec/batch)
2018-05-02 13:54:39.250252: step 17490, loss = 20.09 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 13:55:28.536343: step 17500, loss = 20.15 (3.2 examples/sec; 4.971 sec/batch)
2018-05-02 13:56:21.313264: step 17510, loss = 20.30 (3.2 examples/sec; 5.016 sec/batch)
2018-05-02 13:57:10.767083: step 17520, loss = 20.50 (3.1 examples/sec; 5.130 sec/batch)
2018-05-02 13:58:01.109557: step 17530, loss = 20.09 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 13:58:51.194652: step 17540, loss = 20.77 (3.1 examples/sec; 5.152 sec/batch)
2018-05-02 13:59:41.114583: step 17550, loss = 20.57 (3.2 examples/sec; 5.054 sec/batch)
2018-05-02 14:00:30.330421: step 17560, loss = 20.39 (3.2 examples/sec; 4.992 sec/batch)
2018-05-02 14:01:19.216129: step 17570, loss = 21.92 (3.3 examples/sec; 4.837 sec/batch)
2018-05-02 14:02:08.527794: step 17580, loss = 20.08 (3.2 examples/sec; 5.059 sec/batch)
2018-05-02 14:02:58.554444: step 17590, loss = 21.14 (3.2 examples/sec; 5.075 sec/batch)
2018-05-02 14:03:45.419780: step 17600, loss = 19.44 (3.3 examples/sec; 4.912 sec/batch)
2018-05-02 14:04:38.691526: step 17610, loss = 21.03 (3.2 examples/sec; 5.048 sec/batch)
2018-05-02 14:05:28.166463: step 17620, loss = 22.05 (3.2 examples/sec; 5.003 sec/batch)
2018-05-02 14:06:17.655063: step 17630, loss = 20.48 (3.2 examples/sec; 4.923 sec/batch)
2018-05-02 14:07:07.112690: step 17640, loss = 19.40 (3.2 examples/sec; 5.051 sec/batch)
2018-05-02 14:07:57.046482: step 17650, loss = 19.81 (3.2 examples/sec; 4.992 sec/batch)
2018-05-02 14:08:46.871145: step 17660, loss = 20.21 (3.3 examples/sec; 4.834 sec/batch)
2018-05-02 14:09:36.723414: step 17670, loss = 19.02 (3.1 examples/sec; 5.110 sec/batch)
2018-05-02 14:10:25.756026: step 17680, loss = 21.14 (3.2 examples/sec; 4.951 sec/batch)
2018-05-02 14:11:15.503007: step 17690, loss = 21.83 (3.2 examples/sec; 5.005 sec/batch)
2018-05-02 14:12:05.103690: step 17700, loss = 19.51 (3.3 examples/sec; 4.913 sec/batch)
2018-05-02 14:12:58.890042: step 17710, loss = 19.89 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 14:13:46.248285: step 17720, loss = 19.98 (3.1 examples/sec; 5.169 sec/batch)
2018-05-02 14:14:35.829125: step 17730, loss = 22.18 (3.3 examples/sec; 4.809 sec/batch)
2018-05-02 14:15:25.994363: step 17740, loss = 21.51 (3.3 examples/sec; 4.836 sec/batch)
2018-05-02 14:16:16.149685: step 17750, loss = 20.44 (3.1 examples/sec; 5.168 sec/batch)
2018-05-02 14:17:06.163061: step 17760, loss = 20.55 (3.2 examples/sec; 5.063 sec/batch)
2018-05-02 14:17:56.191006: step 17770, loss = 20.72 (3.2 examples/sec; 4.983 sec/batch)
2018-05-02 14:18:45.390785: step 17780, loss = 20.85 (3.3 examples/sec; 4.843 sec/batch)
2018-05-02 14:19:34.372646: step 17790, loss = 21.32 (3.2 examples/sec; 5.016 sec/batch)
2018-05-02 14:20:24.178383: step 17800, loss = 21.02 (3.3 examples/sec; 4.888 sec/batch)
2018-05-02 14:21:16.514106: step 17810, loss = 21.39 (3.2 examples/sec; 5.062 sec/batch)
2018-05-02 14:22:06.222477: step 17820, loss = 19.42 (3.3 examples/sec; 4.894 sec/batch)
2018-05-02 14:22:56.375092: step 17830, loss = 21.52 (3.1 examples/sec; 5.113 sec/batch)
2018-05-02 14:23:42.787212: step 17840, loss = 20.62 (4.2 examples/sec; 3.851 sec/batch)
2018-05-02 14:24:32.293087: step 17850, loss = 20.68 (3.3 examples/sec; 4.851 sec/batch)
2018-05-02 14:25:21.874138: step 17860, loss = 20.40 (3.2 examples/sec; 5.048 sec/batch)
2018-05-02 14:26:10.856265: step 17870, loss = 19.89 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 14:27:00.606048: step 17880, loss = 20.99 (3.2 examples/sec; 5.003 sec/batch)
2018-05-02 14:27:50.361327: step 17890, loss = 20.50 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 14:28:40.009008: step 17900, loss = 19.27 (3.2 examples/sec; 4.978 sec/batch)
2018-05-02 14:29:33.592465: step 17910, loss = 21.43 (3.2 examples/sec; 4.945 sec/batch)
2018-05-02 14:30:23.137397: step 17920, loss = 20.10 (3.1 examples/sec; 5.095 sec/batch)
2018-05-02 14:31:12.974550: step 17930, loss = 19.64 (3.1 examples/sec; 5.118 sec/batch)
2018-05-02 14:32:02.023410: step 17940, loss = 19.74 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 14:32:51.789295: step 17950, loss = 19.93 (3.4 examples/sec; 4.709 sec/batch)
2018-05-02 14:33:42.145867: step 17960, loss = 21.37 (3.1 examples/sec; 5.228 sec/batch)
2018-05-02 14:34:28.692434: step 17970, loss = 19.69 (3.1 examples/sec; 5.107 sec/batch)
2018-05-02 14:35:18.280152: step 17980, loss = 20.55 (3.1 examples/sec; 5.217 sec/batch)
2018-05-02 14:36:07.959257: step 17990, loss = 19.85 (3.3 examples/sec; 4.873 sec/batch)
2018-05-02 14:36:57.601036: step 18000, loss = 21.82 (3.2 examples/sec; 5.038 sec/batch)
2018-05-02 14:37:50.293228: step 18010, loss = 20.08 (3.4 examples/sec; 4.743 sec/batch)
2018-05-02 14:38:40.117950: step 18020, loss = 19.82 (3.2 examples/sec; 5.014 sec/batch)
2018-05-02 14:39:29.608907: step 18030, loss = 19.10 (3.3 examples/sec; 4.914 sec/batch)
2018-05-02 14:40:19.230689: step 18040, loss = 19.87 (3.2 examples/sec; 4.988 sec/batch)
2018-05-02 14:41:09.651006: step 18050, loss = 19.69 (3.3 examples/sec; 4.902 sec/batch)
2018-05-02 14:41:59.540927: step 18060, loss = 19.83 (3.2 examples/sec; 4.980 sec/batch)
2018-05-02 14:42:48.572909: step 18070, loss = 19.38 (3.2 examples/sec; 4.955 sec/batch)
2018-05-02 14:43:38.556973: step 18080, loss = 19.02 (3.1 examples/sec; 5.202 sec/batch)
2018-05-02 14:44:25.255613: step 18090, loss = 20.95 (3.1 examples/sec; 5.157 sec/batch)
2018-05-02 14:45:15.326561: step 18100, loss = 20.59 (3.1 examples/sec; 5.122 sec/batch)
2018-05-02 14:46:08.200767: step 18110, loss = 21.12 (3.2 examples/sec; 5.026 sec/batch)
2018-05-02 14:46:57.478291: step 18120, loss = 20.29 (3.3 examples/sec; 4.905 sec/batch)
2018-05-02 14:47:48.234657: step 18130, loss = 20.86 (3.1 examples/sec; 5.092 sec/batch)
2018-05-02 14:48:37.850075: step 18140, loss = 19.60 (3.1 examples/sec; 5.136 sec/batch)
2018-05-02 14:49:27.510570: step 18150, loss = 19.96 (3.3 examples/sec; 4.869 sec/batch)
2018-05-02 14:50:17.737541: step 18160, loss = 19.12 (3.2 examples/sec; 5.070 sec/batch)
2018-05-02 14:51:07.557625: step 18170, loss = 20.05 (3.3 examples/sec; 4.801 sec/batch)
2018-05-02 14:51:57.232561: step 18180, loss = 20.16 (3.3 examples/sec; 4.847 sec/batch)
2018-05-02 14:52:46.901520: step 18190, loss = 20.49 (3.2 examples/sec; 4.952 sec/batch)
2018-05-02 14:53:36.460160: step 18200, loss = 20.33 (3.2 examples/sec; 5.027 sec/batch)
2018-05-02 14:54:26.714802: step 18210, loss = 19.75 (3.3 examples/sec; 4.903 sec/batch)
2018-05-02 14:55:15.896525: step 18220, loss = 19.92 (3.3 examples/sec; 4.827 sec/batch)
2018-05-02 14:56:06.039481: step 18230, loss = 21.57 (3.2 examples/sec; 5.016 sec/batch)
2018-05-02 14:56:55.316161: step 18240, loss = 19.22 (3.3 examples/sec; 4.846 sec/batch)
2018-05-02 14:57:44.581346: step 18250, loss = 19.78 (3.2 examples/sec; 5.006 sec/batch)
2018-05-02 14:58:34.090622: step 18260, loss = 21.29 (3.2 examples/sec; 4.999 sec/batch)
2018-05-02 14:59:24.242236: step 18270, loss = 20.03 (3.1 examples/sec; 5.123 sec/batch)
2018-05-02 15:00:13.992310: step 18280, loss = 20.34 (3.3 examples/sec; 4.813 sec/batch)
2018-05-02 15:01:03.368938: step 18290, loss = 20.68 (3.3 examples/sec; 4.904 sec/batch)
2018-05-02 15:01:52.984973: step 18300, loss = 20.81 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 15:02:46.644854: step 18310, loss = 19.71 (3.3 examples/sec; 4.906 sec/batch)
2018-05-02 15:03:36.392196: step 18320, loss = 21.81 (3.3 examples/sec; 4.821 sec/batch)
2018-05-02 15:04:26.087454: step 18330, loss = 21.14 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 15:05:12.999785: step 18340, loss = 21.31 (3.2 examples/sec; 4.936 sec/batch)
2018-05-02 15:06:02.350521: step 18350, loss = 20.57 (3.2 examples/sec; 4.940 sec/batch)
2018-05-02 15:06:52.280982: step 18360, loss = 21.13 (3.2 examples/sec; 4.935 sec/batch)
2018-05-02 15:07:41.938202: step 18370, loss = 19.25 (3.2 examples/sec; 4.938 sec/batch)
2018-05-02 15:08:31.605766: step 18380, loss = 20.57 (3.2 examples/sec; 4.938 sec/batch)
2018-05-02 15:09:21.402458: step 18390, loss = 20.39 (3.2 examples/sec; 4.950 sec/batch)
2018-05-02 15:10:10.712500: step 18400, loss = 20.33 (3.1 examples/sec; 5.092 sec/batch)
2018-05-02 15:11:03.408254: step 18410, loss = 20.65 (3.4 examples/sec; 4.722 sec/batch)
2018-05-02 15:11:53.397447: step 18420, loss = 19.05 (3.2 examples/sec; 4.939 sec/batch)
2018-05-02 15:12:43.027820: step 18430, loss = 21.59 (3.2 examples/sec; 5.042 sec/batch)
2018-05-02 15:13:33.209587: step 18440, loss = 20.52 (3.1 examples/sec; 5.112 sec/batch)
2018-05-02 15:14:22.606963: step 18450, loss = 19.45 (3.2 examples/sec; 5.032 sec/batch)
2018-05-02 15:15:08.805038: step 18460, loss = 20.05 (3.2 examples/sec; 4.940 sec/batch)
2018-05-02 15:15:58.487454: step 18470, loss = 20.11 (3.2 examples/sec; 4.944 sec/batch)
2018-05-02 15:16:48.092746: step 18480, loss = 19.93 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 15:17:37.066133: step 18490, loss = 22.09 (3.2 examples/sec; 4.988 sec/batch)
2018-05-02 15:18:27.023425: step 18500, loss = 18.71 (3.3 examples/sec; 4.874 sec/batch)
2018-05-02 15:19:19.834236: step 18510, loss = 20.49 (3.3 examples/sec; 4.855 sec/batch)
2018-05-02 15:20:08.673567: step 18520, loss = 19.46 (3.3 examples/sec; 4.802 sec/batch)
2018-05-02 15:20:58.577629: step 18530, loss = 19.44 (3.1 examples/sec; 5.105 sec/batch)
2018-05-02 15:21:48.119951: step 18540, loss = 20.64 (3.2 examples/sec; 4.966 sec/batch)
2018-05-02 15:22:37.172448: step 18550, loss = 20.49 (3.3 examples/sec; 4.858 sec/batch)
2018-05-02 15:23:27.479909: step 18560, loss = 20.39 (3.3 examples/sec; 4.817 sec/batch)
2018-05-02 15:24:16.784680: step 18570, loss = 21.38 (3.3 examples/sec; 4.823 sec/batch)
2018-05-02 15:25:04.109049: step 18580, loss = 20.80 (4.2 examples/sec; 3.806 sec/batch)
2018-05-02 15:25:53.642290: step 18590, loss = 20.63 (3.1 examples/sec; 5.112 sec/batch)
2018-05-02 15:26:43.420017: step 18600, loss = 21.07 (3.1 examples/sec; 5.203 sec/batch)
2018-05-02 15:27:36.533767: step 18610, loss = 20.68 (3.2 examples/sec; 5.068 sec/batch)
2018-05-02 15:28:25.688342: step 18620, loss = 19.73 (3.3 examples/sec; 4.866 sec/batch)
2018-05-02 15:29:15.208539: step 18630, loss = 20.24 (3.2 examples/sec; 4.933 sec/batch)
2018-05-02 15:30:04.839016: step 18640, loss = 19.92 (3.1 examples/sec; 5.101 sec/batch)
2018-05-02 15:30:55.118774: step 18650, loss = 19.94 (3.2 examples/sec; 5.074 sec/batch)
2018-05-02 15:31:44.194420: step 18660, loss = 22.45 (3.3 examples/sec; 4.818 sec/batch)
2018-05-02 15:32:33.730622: step 18670, loss = 20.23 (3.2 examples/sec; 5.023 sec/batch)
2018-05-02 15:33:23.264049: step 18680, loss = 19.48 (3.2 examples/sec; 5.017 sec/batch)
2018-05-02 15:34:12.608501: step 18690, loss = 20.54 (3.2 examples/sec; 5.076 sec/batch)
2018-05-02 15:35:01.797013: step 18700, loss = 20.01 (3.2 examples/sec; 5.034 sec/batch)
2018-05-02 15:35:52.869087: step 18710, loss = 19.67 (3.1 examples/sec; 5.120 sec/batch)
2018-05-02 15:36:42.267155: step 18720, loss = 20.84 (3.2 examples/sec; 4.948 sec/batch)
2018-05-02 15:37:31.846610: step 18730, loss = 20.06 (3.2 examples/sec; 5.039 sec/batch)
2018-05-02 15:38:21.839488: step 18740, loss = 21.06 (3.3 examples/sec; 4.876 sec/batch)
2018-05-02 15:39:11.221017: step 18750, loss = 20.34 (3.2 examples/sec; 5.007 sec/batch)
2018-05-02 15:40:00.321250: step 18760, loss = 20.83 (3.3 examples/sec; 4.894 sec/batch)
2018-05-02 15:40:49.628651: step 18770, loss = 20.83 (3.2 examples/sec; 5.009 sec/batch)
2018-05-02 15:41:38.588518: step 18780, loss = 20.88 (3.2 examples/sec; 4.932 sec/batch)
2018-05-02 15:42:28.106691: step 18790, loss = 19.12 (3.3 examples/sec; 4.881 sec/batch)
2018-05-02 15:43:17.902714: step 18800, loss = 20.27 (3.2 examples/sec; 4.952 sec/batch)
2018-05-02 15:44:10.492684: step 18810, loss = 19.50 (3.3 examples/sec; 4.841 sec/batch)
2018-05-02 15:44:59.479138: step 18820, loss = 19.76 (3.3 examples/sec; 4.833 sec/batch)
2018-05-02 15:45:46.301408: step 18830, loss = 19.15 (3.3 examples/sec; 4.875 sec/batch)
2018-05-02 15:46:35.729004: step 18840, loss = 21.22 (3.1 examples/sec; 5.167 sec/batch)
2018-05-02 15:47:25.617313: step 18850, loss = 20.34 (3.2 examples/sec; 5.045 sec/batch)
2018-05-02 15:48:14.734136: step 18860, loss = 19.51 (3.2 examples/sec; 5.032 sec/batch)
2018-05-02 15:49:03.622960: step 18870, loss = 20.23 (3.4 examples/sec; 4.749 sec/batch)
2018-05-02 15:49:53.560524: step 18880, loss = 20.37 (3.3 examples/sec; 4.920 sec/batch)
2018-05-02 15:50:43.445990: step 18890, loss = 21.30 (3.2 examples/sec; 4.980 sec/batch)
2018-05-02 15:51:33.417675: step 18900, loss = 20.02 (3.1 examples/sec; 5.156 sec/batch)
2018-05-02 15:52:27.122662: step 18910, loss = 20.59 (3.2 examples/sec; 5.030 sec/batch)
2018-05-02 15:53:16.648559: step 18920, loss = 20.02 (3.1 examples/sec; 5.080 sec/batch)
2018-05-02 15:54:05.299432: step 18930, loss = 21.14 (3.3 examples/sec; 4.901 sec/batch)
2018-05-02 15:54:55.295077: step 18940, loss = 20.04 (3.2 examples/sec; 4.977 sec/batch)
2018-05-02 15:55:42.717205: step 18950, loss = 19.32 (4.2 examples/sec; 3.849 sec/batch)
2018-05-02 15:56:32.089709: step 18960, loss = 20.31 (3.2 examples/sec; 5.012 sec/batch)
2018-05-02 15:57:21.551651: step 18970, loss = 20.25 (3.2 examples/sec; 5.033 sec/batch)
2018-05-02 15:58:10.981970: step 18980, loss = 19.70 (3.4 examples/sec; 4.691 sec/batch)
2018-05-02 15:59:00.018420: step 18990, loss = 20.63 (3.3 examples/sec; 4.836 sec/batch)
2018-05-02 15:59:49.233491: step 19000, loss = 21.09 (3.2 examples/sec; 4.957 sec/batch)
2018-05-02 16:00:42.133865: step 19010, loss = 19.13 (3.3 examples/sec; 4.842 sec/batch)
2018-05-02 16:01:32.005249: step 19020, loss = 21.33 (3.2 examples/sec; 5.068 sec/batch)
2018-05-02 16:02:22.286534: step 19030, loss = 21.71 (3.2 examples/sec; 5.010 sec/batch)
2018-05-02 16:03:12.099202: step 19040, loss = 20.33 (3.1 examples/sec; 5.081 sec/batch)
2018-05-02 16:04:02.069844: step 19050, loss = 20.11 (3.2 examples/sec; 5.070 sec/batch)
2018-05-02 16:04:51.599634: step 19060, loss = 19.27 (3.2 examples/sec; 5.038 sec/batch)
2018-05-02 16:05:41.463470: step 19070, loss = 19.79 (3.1 examples/sec; 5.100 sec/batch)
2018-05-02 16:06:28.602215: step 19080, loss = 21.25 (3.3 examples/sec; 4.787 sec/batch)
2018-05-02 16:07:18.270511: step 19090, loss = 20.91 (3.2 examples/sec; 4.976 sec/batch)
2018-05-02 16:08:08.086964: step 19100, loss = 19.84 (3.2 examples/sec; 4.987 sec/batch)
2018-05-02 16:09:01.058117: step 19110, loss = 20.55 (3.3 examples/sec; 4.820 sec/batch)
2018-05-02 16:09:50.610653: step 19120, loss = 19.80 (3.2 examples/sec; 5.017 sec/batch)
2018-05-02 16:10:40.092233: step 19130, loss = 20.18 (3.2 examples/sec; 5.065 sec/batch)
2018-05-02 16:11:28.754213: step 19140, loss = 21.28 (3.3 examples/sec; 4.840 sec/batch)
2018-05-02 16:12:17.950081: step 19150, loss = 20.94 (3.3 examples/sec; 4.885 sec/batch)
2018-05-02 16:13:07.073299: step 19160, loss = 19.92 (3.3 examples/sec; 4.880 sec/batch)
2018-05-02 16:13:55.870585: step 19170, loss = 19.25 (3.4 examples/sec; 4.762 sec/batch)
2018-05-02 16:14:44.980483: step 19180, loss = 20.54 (3.2 examples/sec; 4.961 sec/batch)
2018-05-02 16:15:34.093553: step 19190, loss = 19.31 (3.3 examples/sec; 4.901 sec/batch)
2018-05-02 16:16:20.526432: step 19200, loss = 21.73 (3.4 examples/sec; 4.647 sec/batch)
2018-05-02 16:17:13.570864: step 19210, loss = 19.57 (3.1 examples/sec; 5.122 sec/batch)
2018-05-02 16:18:02.620675: step 19220, loss = 18.97 (3.3 examples/sec; 4.818 sec/batch)
2018-05-02 16:18:52.312142: step 19230, loss = 18.64 (3.1 examples/sec; 5.081 sec/batch)
2018-05-02 16:19:42.495519: step 19240, loss = 20.33 (3.2 examples/sec; 5.071 sec/batch)
2018-05-02 16:20:32.082038: step 19250, loss = 22.13 (3.2 examples/sec; 4.928 sec/batch)
2018-05-02 16:21:21.333628: step 19260, loss = 19.86 (3.3 examples/sec; 4.815 sec/batch)
2018-05-02 16:22:10.394886: step 19270, loss = 19.61 (3.2 examples/sec; 5.039 sec/batch)
2018-05-02 16:22:59.389005: step 19280, loss = 20.23 (3.3 examples/sec; 4.849 sec/batch)
2018-05-02 16:23:49.097744: step 19290, loss = 20.42 (3.0 examples/sec; 5.318 sec/batch)
2018-05-02 16:24:38.801444: step 19300, loss = 20.19 (3.2 examples/sec; 4.968 sec/batch)
2018-05-02 16:25:31.383802: step 19310, loss = 19.04 (3.2 examples/sec; 4.952 sec/batch)
2018-05-02 16:26:20.537305: step 19320, loss = 20.39 (3.2 examples/sec; 4.989 sec/batch)
2018-05-02 16:27:07.181654: step 19330, loss = 21.62 (3.3 examples/sec; 4.844 sec/batch)
2018-05-02 16:27:57.165181: step 19340, loss = 21.78 (3.2 examples/sec; 5.060 sec/batch)
2018-05-02 16:28:46.540359: step 19350, loss = 21.40 (3.3 examples/sec; 4.827 sec/batch)
2018-05-02 16:29:36.269252: step 19360, loss = 20.72 (3.2 examples/sec; 4.984 sec/batch)
2018-05-02 16:30:26.388348: step 19370, loss = 20.58 (3.3 examples/sec; 4.920 sec/batch)
2018-05-02 16:31:16.315511: step 19380, loss = 18.94 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 16:32:06.103785: step 19390, loss = 19.86 (3.2 examples/sec; 4.944 sec/batch)
2018-05-02 16:32:55.338871: step 19400, loss = 20.84 (3.1 examples/sec; 5.088 sec/batch)
2018-05-02 16:33:47.866269: step 19410, loss = 19.60 (3.2 examples/sec; 4.975 sec/batch)
2018-05-02 16:34:37.188021: step 19420, loss = 20.49 (3.2 examples/sec; 5.040 sec/batch)
2018-05-02 16:35:26.982467: step 19430, loss = 20.26 (3.2 examples/sec; 4.995 sec/batch)
2018-05-02 16:36:16.578276: step 19440, loss = 20.61 (3.2 examples/sec; 5.077 sec/batch)
2018-05-02 16:37:03.750903: step 19450, loss = 21.95 (3.3 examples/sec; 4.899 sec/batch)
2018-05-02 16:37:53.269447: step 19460, loss = 21.19 (3.2 examples/sec; 4.950 sec/batch)
2018-05-02 16:38:43.262554: step 19470, loss = 21.43 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 16:39:33.132979: step 19480, loss = 20.46 (3.2 examples/sec; 4.971 sec/batch)
2018-05-02 16:40:22.644891: step 19490, loss = 19.77 (3.4 examples/sec; 4.764 sec/batch)
2018-05-02 16:41:12.383750: step 19500, loss = 20.74 (3.2 examples/sec; 4.928 sec/batch)
2018-05-02 16:42:05.587292: step 19510, loss = 20.21 (3.3 examples/sec; 4.876 sec/batch)
2018-05-02 16:42:55.518403: step 19520, loss = 20.82 (3.3 examples/sec; 4.877 sec/batch)
2018-05-02 16:43:44.719151: step 19530, loss = 20.09 (3.2 examples/sec; 5.057 sec/batch)
2018-05-02 16:44:33.519956: step 19540, loss = 20.51 (3.2 examples/sec; 4.931 sec/batch)
2018-05-02 16:45:23.056339: step 19550, loss = 19.88 (3.2 examples/sec; 4.954 sec/batch)
2018-05-02 16:46:12.793167: step 19560, loss = 19.72 (3.3 examples/sec; 4.914 sec/batch)
2018-05-02 16:46:59.129683: step 19570, loss = 20.56 (3.2 examples/sec; 4.940 sec/batch)
2018-05-02 16:47:48.931622: step 19580, loss = 20.35 (3.1 examples/sec; 5.112 sec/batch)
2018-05-02 16:48:38.571238: step 19590, loss = 19.45 (3.2 examples/sec; 4.955 sec/batch)
2018-05-02 16:49:27.538576: step 19600, loss = 19.64 (3.2 examples/sec; 4.944 sec/batch)
2018-05-02 16:50:20.289905: step 19610, loss = 20.96 (3.2 examples/sec; 4.982 sec/batch)
2018-05-02 16:51:10.104328: step 19620, loss = 20.61 (3.3 examples/sec; 4.889 sec/batch)
2018-05-02 16:51:59.351073: step 19630, loss = 20.62 (3.2 examples/sec; 5.040 sec/batch)
2018-05-02 16:52:48.534975: step 19640, loss = 19.98 (3.3 examples/sec; 4.870 sec/batch)
2018-05-02 16:53:38.195893: step 19650, loss = 20.12 (3.2 examples/sec; 4.980 sec/batch)
2018-05-02 16:54:27.328148: step 19660, loss = 20.10 (3.3 examples/sec; 4.901 sec/batch)
2018-05-02 16:55:16.420686: step 19670, loss = 19.34 (3.3 examples/sec; 4.867 sec/batch)
2018-05-02 16:56:06.355424: step 19680, loss = 20.80 (3.2 examples/sec; 5.061 sec/batch)
2018-05-02 16:56:55.502793: step 19690, loss = 20.66 (3.2 examples/sec; 5.020 sec/batch)
2018-05-02 16:57:41.860694: step 19700, loss = 22.56 (3.1 examples/sec; 5.098 sec/batch)
2018-05-02 16:58:34.553132: step 19710, loss = 19.44 (3.2 examples/sec; 4.936 sec/batch)
2018-05-02 16:59:23.930887: step 19720, loss = 19.12 (3.3 examples/sec; 4.829 sec/batch)
2018-05-02 17:00:13.393472: step 19730, loss = 19.38 (3.3 examples/sec; 4.865 sec/batch)
2018-05-02 17:01:02.672489: step 19740, loss = 19.11 (3.2 examples/sec; 4.998 sec/batch)
2018-05-02 17:01:51.937981: step 19750, loss = 19.05 (3.3 examples/sec; 4.788 sec/batch)
2018-05-02 17:02:41.343775: step 19760, loss = 21.70 (3.3 examples/sec; 4.882 sec/batch)
2018-05-02 17:03:30.763847: step 19770, loss = 19.77 (3.1 examples/sec; 5.157 sec/batch)
2018-05-02 17:04:20.418762: step 19780, loss = 20.57 (3.2 examples/sec; 5.042 sec/batch)
2018-05-02 17:05:09.899022: step 19790, loss = 19.01 (3.3 examples/sec; 4.915 sec/batch)
2018-05-02 17:05:59.133317: step 19800, loss = 20.42 (3.2 examples/sec; 4.951 sec/batch)
2018-05-02 17:06:52.104070: step 19810, loss = 21.33 (3.2 examples/sec; 4.947 sec/batch)
2018-05-02 17:07:38.776742: step 19820, loss = 19.84 (3.3 examples/sec; 4.814 sec/batch)
2018-05-02 17:08:28.048086: step 19830, loss = 19.46 (3.2 examples/sec; 5.040 sec/batch)
2018-05-02 17:09:17.032891: step 19840, loss = 19.80 (3.1 examples/sec; 5.144 sec/batch)
2018-05-02 17:10:06.426861: step 19850, loss = 18.68 (3.2 examples/sec; 4.923 sec/batch)
2018-05-02 17:10:55.991248: step 19860, loss = 21.37 (3.3 examples/sec; 4.852 sec/batch)
2018-05-02 17:11:45.751997: step 19870, loss = 20.34 (3.3 examples/sec; 4.861 sec/batch)
2018-05-02 17:12:35.377659: step 19880, loss = 18.80 (3.2 examples/sec; 5.072 sec/batch)
2018-05-02 17:13:24.882916: step 19890, loss = 19.24 (3.2 examples/sec; 5.049 sec/batch)
2018-05-02 17:14:14.586058: step 19900, loss = 20.11 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 17:15:07.384942: step 19910, loss = 20.47 (3.3 examples/sec; 4.831 sec/batch)
2018-05-02 17:15:57.210265: step 19920, loss = 19.53 (3.2 examples/sec; 5.015 sec/batch)
2018-05-02 17:16:46.446489: step 19930, loss = 20.11 (3.3 examples/sec; 4.923 sec/batch)
2018-05-02 17:17:34.257794: step 19940, loss = 19.73 (4.1 examples/sec; 3.873 sec/batch)
2018-05-02 17:18:22.361285: step 19950, loss = 20.95 (3.3 examples/sec; 4.913 sec/batch)
2018-05-02 17:19:11.820370: step 19960, loss = 19.88 (3.2 examples/sec; 4.934 sec/batch)
2018-05-02 17:20:01.277526: step 19970, loss = 20.11 (3.0 examples/sec; 5.289 sec/batch)
2018-05-02 17:20:51.137238: step 19980, loss = 18.80 (3.3 examples/sec; 4.923 sec/batch)
2018-05-02 17:21:40.813735: step 19990, loss = 21.63 (3.2 examples/sec; 5.054 sec/batch)
2018-05-02 17:22:30.567158: step 20000, loss = 19.92 (3.3 examples/sec; 4.838 sec/batch)
2018-05-02 17:23:23.963544: step 20010, loss = 21.43 (3.2 examples/sec; 4.984 sec/batch)
2018-05-02 17:24:13.308530: step 20020, loss = 19.91 (3.2 examples/sec; 4.923 sec/batch)
2018-05-02 17:25:02.397919: step 20030, loss = 19.74 (3.2 examples/sec; 5.049 sec/batch)
2018-05-02 17:25:52.588960: step 20040, loss = 20.79 (3.3 examples/sec; 4.883 sec/batch)
2018-05-02 17:26:41.689824: step 20050, loss = 19.29 (3.2 examples/sec; 5.018 sec/batch)
2018-05-02 17:27:31.148096: step 20060, loss = 20.07 (3.1 examples/sec; 5.094 sec/batch)
2018-05-02 17:28:17.629978: step 20070, loss = 20.26 (3.3 examples/sec; 4.793 sec/batch)
2018-05-02 17:29:07.301080: step 20080, loss = 19.48 (3.2 examples/sec; 5.025 sec/batch)
2018-05-02 17:29:56.941479: step 20090, loss = 20.12 (3.2 examples/sec; 4.982 sec/batch)
2018-05-02 17:30:46.205637: step 20100, loss = 21.25 (3.3 examples/sec; 4.918 sec/batch)
2018-05-02 17:31:38.870018: step 20110, loss = 21.38 (3.3 examples/sec; 4.917 sec/batch)
2018-05-02 17:32:28.910041: step 20120, loss = 20.88 (3.3 examples/sec; 4.870 sec/batch)
2018-05-02 17:33:18.140333: step 20130, loss = 20.15 (3.2 examples/sec; 4.972 sec/batch)
2018-05-02 17:34:07.481284: step 20140, loss = 21.19 (3.2 examples/sec; 4.992 sec/batch)
2018-05-02 17:34:56.902092: step 20150, loss = 20.92 (3.2 examples/sec; 4.960 sec/batch)
2018-05-02 17:35:46.547695: step 20160, loss = 19.44 (3.3 examples/sec; 4.885 sec/batch)
2018-05-02 17:36:35.815368: step 20170, loss = 20.14 (3.3 examples/sec; 4.918 sec/batch)
2018-05-02 17:37:25.455914: step 20180, loss = 20.61 (3.3 examples/sec; 4.896 sec/batch)
2018-05-02 17:38:12.552080: step 20190, loss = 18.76 (3.1 examples/sec; 5.129 sec/batch)
2018-05-02 17:39:02.163922: step 20200, loss = 20.26 (3.3 examples/sec; 4.883 sec/batch)
2018-05-02 17:39:55.437945: step 20210, loss = 19.35 (3.2 examples/sec; 5.011 sec/batch)
2018-05-02 17:40:44.969985: step 20220, loss = 21.39 (3.2 examples/sec; 4.952 sec/batch)
2018-05-02 17:41:34.672122: step 20230, loss = 19.43 (3.3 examples/sec; 4.920 sec/batch)
2018-05-02 17:42:24.485276: step 20240, loss = 21.56 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 17:43:13.905888: step 20250, loss = 19.76 (3.3 examples/sec; 4.907 sec/batch)
2018-05-02 17:44:03.226642: step 20260, loss = 19.96 (3.2 examples/sec; 4.937 sec/batch)
2018-05-02 17:44:51.957848: step 20270, loss = 19.77 (3.4 examples/sec; 4.775 sec/batch)
2018-05-02 17:45:41.045485: step 20280, loss = 20.07 (3.2 examples/sec; 4.992 sec/batch)
2018-05-02 17:46:30.353800: step 20290, loss = 20.00 (3.2 examples/sec; 5.057 sec/batch)
2018-05-02 17:47:20.372870: step 20300, loss = 22.88 (3.2 examples/sec; 4.953 sec/batch)
2018-05-02 17:48:12.275382: step 20310, loss = 21.03 (4.1 examples/sec; 3.898 sec/batch)
2018-05-02 17:48:59.885108: step 20320, loss = 19.51 (3.3 examples/sec; 4.840 sec/batch)
2018-05-02 17:49:49.252990: step 20330, loss = 21.92 (3.4 examples/sec; 4.731 sec/batch)
2018-05-02 17:50:38.657951: step 20340, loss = 18.86 (3.2 examples/sec; 4.971 sec/batch)
2018-05-02 17:51:27.753130: step 20350, loss = 19.29 (3.3 examples/sec; 4.822 sec/batch)
2018-05-02 17:52:16.850167: step 20360, loss = 21.23 (3.2 examples/sec; 4.933 sec/batch)
2018-05-02 17:53:06.521206: step 20370, loss = 19.15 (3.3 examples/sec; 4.838 sec/batch)
2018-05-02 17:53:56.322435: step 20380, loss = 21.97 (3.2 examples/sec; 4.927 sec/batch)
2018-05-02 17:54:45.870595: step 20390, loss = 20.14 (3.2 examples/sec; 5.052 sec/batch)
2018-05-02 17:55:34.953002: step 20400, loss = 18.83 (3.3 examples/sec; 4.832 sec/batch)
2018-05-02 17:56:28.108309: step 20410, loss = 19.69 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 17:57:17.109835: step 20420, loss = 19.94 (3.3 examples/sec; 4.865 sec/batch)
2018-05-02 17:58:06.201016: step 20430, loss = 21.08 (3.2 examples/sec; 4.941 sec/batch)
2018-05-02 17:58:52.620805: step 20440, loss = 19.85 (3.3 examples/sec; 4.878 sec/batch)
2018-05-02 17:59:42.354372: step 20450, loss = 19.99 (3.1 examples/sec; 5.102 sec/batch)
2018-05-02 18:00:31.887091: step 20460, loss = 19.62 (3.1 examples/sec; 5.084 sec/batch)
2018-05-02 18:01:21.119638: step 20470, loss = 19.43 (3.2 examples/sec; 4.934 sec/batch)
2018-05-02 18:02:10.314429: step 20480, loss = 21.04 (3.2 examples/sec; 4.987 sec/batch)
2018-05-02 18:02:59.531584: step 20490, loss = 19.82 (3.2 examples/sec; 5.067 sec/batch)
2018-05-02 18:03:48.754817: step 20500, loss = 20.24 (3.3 examples/sec; 4.845 sec/batch)
2018-05-02 18:04:41.634725: step 20510, loss = 19.96 (3.3 examples/sec; 4.838 sec/batch)
2018-05-02 18:05:30.709004: step 20520, loss = 20.41 (3.3 examples/sec; 4.900 sec/batch)
2018-05-02 18:06:19.927679: step 20530, loss = 19.87 (3.3 examples/sec; 4.886 sec/batch)
2018-05-02 18:07:10.046227: step 20540, loss = 19.07 (3.3 examples/sec; 4.852 sec/batch)
2018-05-02 18:07:59.721054: step 20550, loss = 18.78 (3.3 examples/sec; 4.846 sec/batch)
2018-05-02 18:08:47.763495: step 20560, loss = 20.71 (4.2 examples/sec; 3.847 sec/batch)
2018-05-02 18:09:35.515852: step 20570, loss = 20.27 (3.3 examples/sec; 4.882 sec/batch)
2018-05-02 18:10:25.389650: step 20580, loss = 18.76 (3.3 examples/sec; 4.892 sec/batch)
2018-05-02 18:11:14.633934: step 20590, loss = 20.88 (3.2 examples/sec; 4.949 sec/batch)
2018-05-02 18:12:03.949752: step 20600, loss = 19.03 (3.3 examples/sec; 4.873 sec/batch)
2018-05-02 18:12:57.639406: step 20610, loss = 19.79 (3.2 examples/sec; 5.006 sec/batch)
2018-05-02 18:13:47.190680: step 20620, loss = 20.08 (3.3 examples/sec; 4.897 sec/batch)
2018-05-02 18:14:36.478027: step 20630, loss = 20.09 (3.2 examples/sec; 4.976 sec/batch)
2018-05-02 18:15:26.467470: step 20640, loss = 20.97 (3.3 examples/sec; 4.859 sec/batch)
2018-05-02 18:16:15.862702: step 20650, loss = 19.89 (3.2 examples/sec; 4.982 sec/batch)
2018-05-02 18:17:04.875912: step 20660, loss = 19.58 (3.3 examples/sec; 4.825 sec/batch)
2018-05-02 18:17:53.974861: step 20670, loss = 20.80 (3.2 examples/sec; 5.038 sec/batch)
2018-05-02 18:18:43.334291: step 20680, loss = 20.09 (3.2 examples/sec; 5.006 sec/batch)
2018-05-02 18:19:30.559660: step 20690, loss = 19.66 (3.2 examples/sec; 5.055 sec/batch)
2018-05-02 18:20:20.053471: step 20700, loss = 20.57 (3.2 examples/sec; 4.949 sec/batch)
2018-05-02 18:21:13.135649: step 20710, loss = 19.77 (3.2 examples/sec; 5.027 sec/batch)
2018-05-02 18:22:02.704028: step 20720, loss = 19.32 (3.2 examples/sec; 4.955 sec/batch)
2018-05-02 18:22:52.059232: step 20730, loss = 19.83 (3.4 examples/sec; 4.703 sec/batch)
2018-05-02 18:23:41.020420: step 20740, loss = 19.55 (3.3 examples/sec; 4.923 sec/batch)
2018-05-02 18:24:30.423617: step 20750, loss = 19.64 (3.3 examples/sec; 4.911 sec/batch)
2018-05-02 18:25:20.165131: step 20760, loss = 19.63 (3.4 examples/sec; 4.771 sec/batch)
2018-05-02 18:26:10.590703: step 20770, loss = 19.76 (3.3 examples/sec; 4.864 sec/batch)
2018-05-02 18:27:00.458333: step 20780, loss = 19.93 (3.2 examples/sec; 5.040 sec/batch)
2018-05-02 18:27:49.986671: step 20790, loss = 20.51 (3.2 examples/sec; 5.063 sec/batch)
2018-05-02 18:28:40.120413: step 20800, loss = 20.02 (3.1 examples/sec; 5.087 sec/batch)
2018-05-02 18:29:30.510672: step 20810, loss = 20.73 (3.2 examples/sec; 4.959 sec/batch)
2018-05-02 18:30:20.397579: step 20820, loss = 20.48 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 18:31:09.586794: step 20830, loss = 20.08 (3.2 examples/sec; 5.002 sec/batch)
2018-05-02 18:31:58.413099: step 20840, loss = 20.30 (3.3 examples/sec; 4.902 sec/batch)
2018-05-02 18:32:48.300842: step 20850, loss = 20.01 (3.2 examples/sec; 4.984 sec/batch)
2018-05-02 18:33:37.145575: step 20860, loss = 20.61 (3.3 examples/sec; 4.808 sec/batch)
2018-05-02 18:34:26.330335: step 20870, loss = 19.06 (3.3 examples/sec; 4.819 sec/batch)
2018-05-02 18:35:15.227841: step 20880, loss = 19.92 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 18:36:04.939507: step 20890, loss = 20.01 (3.1 examples/sec; 5.156 sec/batch)
2018-05-02 18:36:53.901283: step 20900, loss = 20.30 (3.1 examples/sec; 5.164 sec/batch)
2018-05-02 18:37:46.885430: step 20910, loss = 19.15 (3.2 examples/sec; 4.925 sec/batch)
2018-05-02 18:38:36.759249: step 20920, loss = 20.49 (3.2 examples/sec; 5.054 sec/batch)
2018-05-02 18:39:26.384442: step 20930, loss = 20.19 (3.3 examples/sec; 4.831 sec/batch)
2018-05-02 18:40:13.079304: step 20940, loss = 20.47 (3.2 examples/sec; 4.968 sec/batch)
2018-05-02 18:41:02.502155: step 20950, loss = 23.27 (3.2 examples/sec; 4.963 sec/batch)
2018-05-02 18:41:52.027998: step 20960, loss = 20.28 (3.1 examples/sec; 5.087 sec/batch)
2018-05-02 18:42:41.609370: step 20970, loss = 19.86 (3.1 examples/sec; 5.088 sec/batch)
2018-05-02 18:43:30.682263: step 20980, loss = 20.45 (3.2 examples/sec; 5.055 sec/batch)
2018-05-02 18:44:19.529202: step 20990, loss = 21.11 (3.2 examples/sec; 4.949 sec/batch)
2018-05-02 18:45:08.501736: step 21000, loss = 19.86 (3.2 examples/sec; 4.960 sec/batch)
2018-05-02 18:46:01.457371: step 21010, loss = 20.65 (3.2 examples/sec; 4.987 sec/batch)
2018-05-02 18:46:50.813330: step 21020, loss = 19.33 (3.4 examples/sec; 4.738 sec/batch)
2018-05-02 18:47:40.511386: step 21030, loss = 20.44 (3.1 examples/sec; 5.178 sec/batch)
2018-05-02 18:48:30.771696: step 21040, loss = 20.24 (3.1 examples/sec; 5.177 sec/batch)
2018-05-02 18:49:20.775375: step 21050, loss = 19.12 (3.2 examples/sec; 5.005 sec/batch)
2018-05-02 18:50:07.590973: step 21060, loss = 20.19 (3.3 examples/sec; 4.871 sec/batch)
2018-05-02 18:50:57.280631: step 21070, loss = 20.30 (3.2 examples/sec; 4.948 sec/batch)
2018-05-02 18:51:46.408826: step 21080, loss = 20.29 (3.4 examples/sec; 4.693 sec/batch)
2018-05-02 18:52:35.978502: step 21090, loss = 20.16 (3.2 examples/sec; 4.931 sec/batch)
2018-05-02 18:53:25.396823: step 21100, loss = 19.11 (3.2 examples/sec; 4.996 sec/batch)
2018-05-02 18:54:18.092599: step 21110, loss = 20.38 (3.4 examples/sec; 4.738 sec/batch)
2018-05-02 18:55:08.084893: step 21120, loss = 20.22 (3.1 examples/sec; 5.115 sec/batch)
2018-05-02 18:55:57.206160: step 21130, loss = 18.99 (3.3 examples/sec; 4.873 sec/batch)
2018-05-02 18:56:46.534352: step 21140, loss = 19.02 (3.1 examples/sec; 5.108 sec/batch)
2018-05-02 18:57:35.987012: step 21150, loss = 20.19 (3.2 examples/sec; 4.962 sec/batch)
2018-05-02 18:58:25.608739: step 21160, loss = 19.39 (3.2 examples/sec; 4.994 sec/batch)
2018-05-02 18:59:15.503825: step 21170, loss = 20.49 (3.3 examples/sec; 4.886 sec/batch)
2018-05-02 19:00:04.723707: step 21180, loss = 20.92 (3.3 examples/sec; 4.841 sec/batch)
2018-05-02 19:00:52.400125: step 21190, loss = 20.13 (3.1 examples/sec; 5.126 sec/batch)
2018-05-02 19:01:42.007609: step 21200, loss = 21.07 (3.3 examples/sec; 4.918 sec/batch)
2018-05-02 19:02:34.740976: step 21210, loss = 20.08 (3.2 examples/sec; 5.015 sec/batch)
2018-05-02 19:03:24.133934: step 21220, loss = 20.03 (3.3 examples/sec; 4.887 sec/batch)
2018-05-02 19:04:14.379137: step 21230, loss = 20.17 (3.1 examples/sec; 5.124 sec/batch)
2018-05-02 19:05:03.807484: step 21240, loss = 19.69 (3.2 examples/sec; 4.980 sec/batch)
2018-05-02 19:05:54.065832: step 21250, loss = 20.69 (3.1 examples/sec; 5.231 sec/batch)
2018-05-02 19:06:43.324234: step 21260, loss = 21.45 (3.2 examples/sec; 5.016 sec/batch)
2018-05-02 19:07:32.539856: step 21270, loss = 19.31 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 19:08:21.624405: step 21280, loss = 19.20 (3.2 examples/sec; 4.948 sec/batch)
2018-05-02 19:09:11.820015: step 21290, loss = 18.92 (3.3 examples/sec; 4.894 sec/batch)
2018-05-02 19:10:01.562709: step 21300, loss = 19.18 (3.2 examples/sec; 4.938 sec/batch)
2018-05-02 19:10:51.849689: step 21310, loss = 20.88 (3.2 examples/sec; 5.046 sec/batch)
2018-05-02 19:11:41.552200: step 21320, loss = 20.75 (3.3 examples/sec; 4.829 sec/batch)
2018-05-02 19:12:30.349389: step 21330, loss = 20.46 (3.1 examples/sec; 5.127 sec/batch)
2018-05-02 19:13:20.237581: step 21340, loss = 19.26 (3.3 examples/sec; 4.867 sec/batch)
2018-05-02 19:14:09.834870: step 21350, loss = 19.82 (3.2 examples/sec; 4.961 sec/batch)
2018-05-02 19:14:59.188908: step 21360, loss = 19.68 (3.3 examples/sec; 4.875 sec/batch)
2018-05-02 19:15:47.935191: step 21370, loss = 20.04 (3.2 examples/sec; 4.998 sec/batch)
2018-05-02 19:16:37.580607: step 21380, loss = 19.53 (3.2 examples/sec; 5.075 sec/batch)
2018-05-02 19:17:26.603694: step 21390, loss = 19.72 (3.2 examples/sec; 4.941 sec/batch)
2018-05-02 19:18:15.737810: step 21400, loss = 19.50 (3.2 examples/sec; 5.070 sec/batch)
2018-05-02 19:19:08.237251: step 21410, loss = 22.09 (3.4 examples/sec; 4.771 sec/batch)
2018-05-02 19:19:58.402331: step 21420, loss = 21.44 (3.3 examples/sec; 4.841 sec/batch)
2018-05-02 19:20:44.934277: step 21430, loss = 20.56 (3.1 examples/sec; 5.088 sec/batch)
2018-05-02 19:21:34.704996: step 21440, loss = 20.92 (3.1 examples/sec; 5.147 sec/batch)
2018-05-02 19:22:23.915739: step 21450, loss = 20.62 (3.2 examples/sec; 4.963 sec/batch)
2018-05-02 19:23:13.442892: step 21460, loss = 19.59 (3.2 examples/sec; 4.999 sec/batch)
2018-05-02 19:24:02.893100: step 21470, loss = 19.39 (3.3 examples/sec; 4.917 sec/batch)
2018-05-02 19:24:51.969830: step 21480, loss = 22.42 (3.3 examples/sec; 4.849 sec/batch)
2018-05-02 19:25:41.355850: step 21490, loss = 19.25 (3.2 examples/sec; 4.986 sec/batch)
2018-05-02 19:26:31.344296: step 21500, loss = 22.09 (3.2 examples/sec; 5.042 sec/batch)
2018-05-02 19:27:24.367843: step 21510, loss = 19.44 (3.2 examples/sec; 5.020 sec/batch)
2018-05-02 19:28:13.197584: step 21520, loss = 18.48 (3.3 examples/sec; 4.893 sec/batch)
2018-05-02 19:29:03.745664: step 21530, loss = 19.68 (3.2 examples/sec; 5.033 sec/batch)
2018-05-02 19:29:53.575798: step 21540, loss = 20.86 (3.3 examples/sec; 4.907 sec/batch)
2018-05-02 19:30:42.675284: step 21550, loss = 20.20 (3.4 examples/sec; 4.701 sec/batch)
2018-05-02 19:31:29.971428: step 21560, loss = 19.75 (3.1 examples/sec; 5.087 sec/batch)
2018-05-02 19:32:19.333051: step 21570, loss = 20.36 (3.3 examples/sec; 4.861 sec/batch)
2018-05-02 19:33:09.747773: step 21580, loss = 20.48 (3.3 examples/sec; 4.870 sec/batch)
2018-05-02 19:33:59.762906: step 21590, loss = 20.52 (3.2 examples/sec; 5.079 sec/batch)
2018-05-02 19:34:49.198505: step 21600, loss = 19.62 (3.3 examples/sec; 4.882 sec/batch)
2018-05-02 19:35:42.300491: step 21610, loss = 19.99 (3.0 examples/sec; 5.258 sec/batch)
2018-05-02 19:36:32.270914: step 21620, loss = 18.48 (3.2 examples/sec; 4.940 sec/batch)
2018-05-02 19:37:21.888751: step 21630, loss = 20.27 (3.3 examples/sec; 4.829 sec/batch)
2018-05-02 19:38:10.633884: step 21640, loss = 20.15 (3.4 examples/sec; 4.741 sec/batch)
2018-05-02 19:38:59.526359: step 21650, loss = 19.61 (3.3 examples/sec; 4.922 sec/batch)
2018-05-02 19:39:48.753062: step 21660, loss = 18.88 (3.3 examples/sec; 4.904 sec/batch)
2018-05-02 19:40:37.952965: step 21670, loss = 19.30 (3.3 examples/sec; 4.853 sec/batch)
2018-05-02 19:41:24.400687: step 21680, loss = 21.49 (3.2 examples/sec; 5.058 sec/batch)
2018-05-02 19:42:14.086644: step 21690, loss = 20.84 (3.3 examples/sec; 4.796 sec/batch)
2018-05-02 19:43:03.725300: step 21700, loss = 19.85 (3.2 examples/sec; 4.934 sec/batch)
2018-05-02 19:43:57.601591: step 21710, loss = 20.23 (3.2 examples/sec; 5.040 sec/batch)
2018-05-02 19:44:46.930073: step 21720, loss = 21.06 (3.2 examples/sec; 5.014 sec/batch)
2018-05-02 19:45:37.032330: step 21730, loss = 19.52 (3.2 examples/sec; 5.035 sec/batch)
2018-05-02 19:46:26.477031: step 21740, loss = 20.90 (3.2 examples/sec; 5.079 sec/batch)
2018-05-02 19:47:15.899201: step 21750, loss = 19.21 (3.3 examples/sec; 4.833 sec/batch)
2018-05-02 19:48:05.683317: step 21760, loss = 19.89 (3.2 examples/sec; 4.927 sec/batch)
2018-05-02 19:48:55.782638: step 21770, loss = 18.83 (3.2 examples/sec; 4.991 sec/batch)
2018-05-02 19:49:45.308153: step 21780, loss = 21.02 (3.3 examples/sec; 4.848 sec/batch)
2018-05-02 19:50:34.942313: step 21790, loss = 19.38 (3.2 examples/sec; 5.076 sec/batch)
2018-05-02 19:51:22.049886: step 21800, loss = 19.29 (3.3 examples/sec; 4.895 sec/batch)
2018-05-02 19:52:15.976348: step 21810, loss = 20.67 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 19:53:05.785937: step 21820, loss = 20.83 (3.1 examples/sec; 5.089 sec/batch)
2018-05-02 19:53:56.516624: step 21830, loss = 19.10 (3.1 examples/sec; 5.142 sec/batch)
2018-05-02 19:54:45.606755: step 21840, loss = 18.75 (3.2 examples/sec; 4.978 sec/batch)
2018-05-02 19:55:35.585995: step 21850, loss = 20.15 (3.2 examples/sec; 4.988 sec/batch)
2018-05-02 19:56:24.440882: step 21860, loss = 19.67 (3.3 examples/sec; 4.816 sec/batch)
2018-05-02 19:57:14.103607: step 21870, loss = 19.72 (3.2 examples/sec; 4.932 sec/batch)
2018-05-02 19:58:03.962114: step 21880, loss = 20.75 (3.2 examples/sec; 4.974 sec/batch)
2018-05-02 19:58:53.476772: step 21890, loss = 20.75 (3.3 examples/sec; 4.873 sec/batch)
2018-05-02 19:59:42.343553: step 21900, loss = 19.96 (3.1 examples/sec; 5.091 sec/batch)
2018-05-02 20:00:35.541941: step 21910, loss = 20.87 (3.2 examples/sec; 5.009 sec/batch)
2018-05-02 20:01:23.425143: step 21920, loss = 19.20 (4.3 examples/sec; 3.735 sec/batch)
2018-05-02 20:02:11.717100: step 21930, loss = 19.64 (3.3 examples/sec; 4.838 sec/batch)
2018-05-02 20:03:01.397590: step 21940, loss = 19.34 (3.1 examples/sec; 5.103 sec/batch)
2018-05-02 20:03:50.346972: step 21950, loss = 21.34 (3.3 examples/sec; 4.807 sec/batch)
2018-05-02 20:04:39.357233: step 21960, loss = 19.96 (3.2 examples/sec; 4.958 sec/batch)
2018-05-02 20:05:28.514170: step 21970, loss = 18.91 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 20:06:17.933770: step 21980, loss = 19.88 (3.2 examples/sec; 5.011 sec/batch)
2018-05-02 20:07:06.894688: step 21990, loss = 19.26 (3.3 examples/sec; 4.887 sec/batch)
2018-05-02 20:07:56.469891: step 22000, loss = 19.05 (3.3 examples/sec; 4.872 sec/batch)
2018-05-02 20:08:50.024800: step 22010, loss = 20.23 (3.2 examples/sec; 4.981 sec/batch)
2018-05-02 20:09:39.202532: step 22020, loss = 19.33 (3.2 examples/sec; 5.028 sec/batch)
2018-05-02 20:10:28.350367: step 22030, loss = 19.87 (3.3 examples/sec; 4.841 sec/batch)
2018-05-02 20:11:17.533564: step 22040, loss = 20.89 (3.2 examples/sec; 4.935 sec/batch)
2018-05-02 20:12:03.990346: step 22050, loss = 19.90 (3.2 examples/sec; 5.020 sec/batch)
2018-05-02 20:12:53.632883: step 22060, loss = 19.55 (3.1 examples/sec; 5.104 sec/batch)
2018-05-02 20:13:43.112450: step 22070, loss = 19.49 (3.4 examples/sec; 4.765 sec/batch)
2018-05-02 20:14:32.152251: step 22080, loss = 19.57 (3.2 examples/sec; 4.976 sec/batch)
2018-05-02 20:15:22.527859: step 22090, loss = 19.14 (3.3 examples/sec; 4.902 sec/batch)
2018-05-02 20:16:12.387557: step 22100, loss = 20.64 (3.2 examples/sec; 5.016 sec/batch)
2018-05-02 20:17:04.734820: step 22110, loss = 20.31 (3.3 examples/sec; 4.826 sec/batch)
2018-05-02 20:17:53.921704: step 22120, loss = 20.85 (3.2 examples/sec; 5.000 sec/batch)
2018-05-02 20:18:42.854843: step 22130, loss = 21.81 (3.2 examples/sec; 5.023 sec/batch)
2018-05-02 20:19:32.843651: step 22140, loss = 21.78 (3.2 examples/sec; 5.042 sec/batch)
2018-05-02 20:20:22.648777: step 22150, loss = 18.79 (3.3 examples/sec; 4.849 sec/batch)
2018-05-02 20:21:12.203971: step 22160, loss = 19.49 (3.3 examples/sec; 4.873 sec/batch)
2018-05-02 20:21:59.043133: step 22170, loss = 19.02 (4.1 examples/sec; 3.873 sec/batch)
2018-05-02 20:22:48.098182: step 22180, loss = 20.18 (3.2 examples/sec; 4.976 sec/batch)
2018-05-02 20:23:37.301884: step 22190, loss = 19.91 (3.3 examples/sec; 4.827 sec/batch)
2018-05-02 20:24:26.105328: step 22200, loss = 20.36 (3.3 examples/sec; 4.870 sec/batch)
2018-05-02 20:25:18.975855: step 22210, loss = 18.90 (3.2 examples/sec; 4.958 sec/batch)
2018-05-02 20:26:08.973591: step 22220, loss = 19.13 (3.2 examples/sec; 4.962 sec/batch)
2018-05-02 20:26:58.912077: step 22230, loss = 19.66 (3.2 examples/sec; 5.069 sec/batch)
2018-05-02 20:27:48.242045: step 22240, loss = 20.14 (3.2 examples/sec; 5.068 sec/batch)
2018-05-02 20:28:37.503132: step 22250, loss = 18.74 (3.2 examples/sec; 4.977 sec/batch)
2018-05-02 20:29:26.752626: step 22260, loss = 19.13 (3.2 examples/sec; 4.992 sec/batch)
2018-05-02 20:30:16.004132: step 22270, loss = 20.34 (3.2 examples/sec; 4.961 sec/batch)
2018-05-02 20:31:05.541959: step 22280, loss = 20.67 (3.2 examples/sec; 5.029 sec/batch)
2018-05-02 20:31:55.620453: step 22290, loss = 19.48 (3.2 examples/sec; 5.067 sec/batch)
2018-05-02 20:32:42.230365: step 22300, loss = 19.47 (3.2 examples/sec; 4.950 sec/batch)
2018-05-02 20:33:35.570182: step 22310, loss = 19.42 (3.3 examples/sec; 4.877 sec/batch)
2018-05-02 20:34:25.720341: step 22320, loss = 19.70 (3.3 examples/sec; 4.858 sec/batch)
2018-05-02 20:35:14.930455: step 22330, loss = 20.70 (3.2 examples/sec; 4.942 sec/batch)
2018-05-02 20:36:04.804487: step 22340, loss = 19.43 (3.3 examples/sec; 4.886 sec/batch)
2018-05-02 20:36:54.920405: step 22350, loss = 19.30 (3.2 examples/sec; 5.036 sec/batch)
2018-05-02 20:37:44.371934: step 22360, loss = 19.71 (3.2 examples/sec; 4.938 sec/batch)
2018-05-02 20:38:33.897725: step 22370, loss = 20.06 (3.2 examples/sec; 5.044 sec/batch)
2018-05-02 20:39:24.359425: step 22380, loss = 19.46 (3.2 examples/sec; 5.049 sec/batch)
2018-05-02 20:40:13.787283: step 22390, loss = 21.28 (3.2 examples/sec; 4.978 sec/batch)
2018-05-02 20:41:03.201938: step 22400, loss = 20.58 (3.3 examples/sec; 4.916 sec/batch)
2018-05-02 20:41:55.922795: step 22410, loss = 18.98 (3.2 examples/sec; 5.003 sec/batch)
2018-05-02 20:42:43.041728: step 22420, loss = 20.07 (3.3 examples/sec; 4.915 sec/batch)
2018-05-02 20:43:32.952705: step 22430, loss = 20.20 (3.3 examples/sec; 4.890 sec/batch)
2018-05-02 20:44:22.347856: step 22440, loss = 18.93 (3.2 examples/sec; 4.972 sec/batch)
2018-05-02 20:45:12.080170: step 22450, loss = 21.09 (3.2 examples/sec; 5.049 sec/batch)
2018-05-02 20:46:01.325056: step 22460, loss = 19.81 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 20:46:51.296545: step 22470, loss = 19.80 (3.3 examples/sec; 4.878 sec/batch)
2018-05-02 20:47:40.840903: step 22480, loss = 23.18 (3.4 examples/sec; 4.763 sec/batch)
2018-05-02 20:48:30.271664: step 22490, loss = 19.59 (3.3 examples/sec; 4.796 sec/batch)
2018-05-02 20:49:19.736147: step 22500, loss = 19.61 (3.4 examples/sec; 4.675 sec/batch)
2018-05-02 20:50:12.986300: step 22510, loss = 20.17 (3.2 examples/sec; 5.072 sec/batch)
2018-05-02 20:51:03.049086: step 22520, loss = 19.33 (3.2 examples/sec; 4.932 sec/batch)
2018-05-02 20:51:52.480677: step 22530, loss = 20.16 (3.3 examples/sec; 4.890 sec/batch)
2018-05-02 20:52:39.235343: step 22540, loss = 18.69 (4.2 examples/sec; 3.802 sec/batch)
2018-05-02 20:53:28.043258: step 22550, loss = 19.66 (3.3 examples/sec; 4.892 sec/batch)
2018-05-02 20:54:17.470029: step 22560, loss = 18.71 (3.2 examples/sec; 4.980 sec/batch)
2018-05-02 20:55:07.006268: step 22570, loss = 19.91 (3.2 examples/sec; 4.997 sec/batch)
2018-05-02 20:55:56.791108: step 22580, loss = 19.76 (3.2 examples/sec; 5.009 sec/batch)
2018-05-02 20:56:46.558125: step 22590, loss = 18.76 (3.2 examples/sec; 4.992 sec/batch)
2018-05-02 20:57:36.183101: step 22600, loss = 20.58 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 20:58:28.580017: step 22610, loss = 20.84 (3.3 examples/sec; 4.910 sec/batch)
2018-05-02 20:59:18.040177: step 22620, loss = 20.17 (3.2 examples/sec; 5.036 sec/batch)
2018-05-02 21:00:07.657555: step 22630, loss = 19.26 (3.3 examples/sec; 4.861 sec/batch)
2018-05-02 21:00:57.594909: step 22640, loss = 19.77 (3.3 examples/sec; 4.909 sec/batch)
2018-05-02 21:01:47.509754: step 22650, loss = 19.78 (3.1 examples/sec; 5.167 sec/batch)
2018-05-02 21:02:37.477946: step 22660, loss = 20.66 (3.2 examples/sec; 5.047 sec/batch)
2018-05-02 21:03:23.438079: step 22670, loss = 21.36 (3.2 examples/sec; 4.951 sec/batch)
2018-05-02 21:04:12.819777: step 22680, loss = 19.72 (3.3 examples/sec; 4.896 sec/batch)
2018-05-02 21:05:02.099847: step 22690, loss = 19.65 (3.3 examples/sec; 4.859 sec/batch)
2018-05-02 21:05:51.284516: step 22700, loss = 19.10 (3.5 examples/sec; 4.626 sec/batch)
2018-05-02 21:06:44.063289: step 22710, loss = 21.06 (3.3 examples/sec; 4.914 sec/batch)
2018-05-02 21:07:33.654389: step 22720, loss = 19.85 (3.3 examples/sec; 4.809 sec/batch)
2018-05-02 21:08:23.517875: step 22730, loss = 19.61 (3.2 examples/sec; 4.934 sec/batch)
2018-05-02 21:09:12.489219: step 22740, loss = 20.19 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 21:10:02.030156: step 22750, loss = 21.20 (3.2 examples/sec; 4.993 sec/batch)
2018-05-02 21:10:51.621011: step 22760, loss = 19.99 (3.3 examples/sec; 4.855 sec/batch)
2018-05-02 21:11:40.783941: step 22770, loss = 20.14 (3.4 examples/sec; 4.763 sec/batch)
2018-05-02 21:12:30.167612: step 22780, loss = 20.99 (3.2 examples/sec; 4.946 sec/batch)
2018-05-02 21:13:16.965756: step 22790, loss = 19.85 (3.3 examples/sec; 4.856 sec/batch)
2018-05-02 21:14:06.176285: step 22800, loss = 18.86 (3.3 examples/sec; 4.805 sec/batch)
2018-05-02 21:14:58.979310: step 22810, loss = 20.27 (3.2 examples/sec; 5.012 sec/batch)
2018-05-02 21:15:48.259737: step 22820, loss = 20.88 (3.3 examples/sec; 4.856 sec/batch)
2018-05-02 21:16:37.161933: step 22830, loss = 19.60 (3.4 examples/sec; 4.770 sec/batch)
2018-05-02 21:17:26.226223: step 22840, loss = 20.35 (3.3 examples/sec; 4.829 sec/batch)
2018-05-02 21:18:14.839870: step 22850, loss = 18.98 (3.3 examples/sec; 4.831 sec/batch)
2018-05-02 21:19:03.927936: step 22860, loss = 19.98 (3.2 examples/sec; 4.938 sec/batch)
2018-05-02 21:19:53.003728: step 22870, loss = 19.69 (3.3 examples/sec; 4.879 sec/batch)
2018-05-02 21:20:41.854282: step 22880, loss = 18.65 (3.4 examples/sec; 4.770 sec/batch)
2018-05-02 21:21:30.491404: step 22890, loss = 19.12 (3.3 examples/sec; 4.881 sec/batch)
2018-05-02 21:22:19.704078: step 22900, loss = 19.55 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 21:23:12.805065: step 22910, loss = 20.20 (3.2 examples/sec; 4.953 sec/batch)
2018-05-02 21:23:59.604327: step 22920, loss = 18.91 (3.3 examples/sec; 4.904 sec/batch)
2018-05-02 21:24:49.291260: step 22930, loss = 19.09 (3.3 examples/sec; 4.872 sec/batch)
2018-05-02 21:25:38.698494: step 22940, loss = 18.82 (3.3 examples/sec; 4.868 sec/batch)
2018-05-02 21:26:28.226764: step 22950, loss = 19.64 (3.2 examples/sec; 5.062 sec/batch)
2018-05-02 21:27:17.976149: step 22960, loss = 20.10 (3.3 examples/sec; 4.877 sec/batch)
2018-05-02 21:28:07.308348: step 22970, loss = 19.18 (3.2 examples/sec; 4.957 sec/batch)
2018-05-02 21:28:56.983944: step 22980, loss = 19.23 (3.1 examples/sec; 5.083 sec/batch)
2018-05-02 21:29:45.749191: step 22990, loss = 19.57 (3.2 examples/sec; 5.038 sec/batch)
2018-05-02 21:30:35.336097: step 23000, loss = 19.01 (3.1 examples/sec; 5.203 sec/batch)
2018-05-02 21:31:28.584295: step 23010, loss = 18.90 (3.3 examples/sec; 4.888 sec/batch)
2018-05-02 21:32:18.305072: step 23020, loss = 20.48 (3.2 examples/sec; 5.047 sec/batch)
2018-05-02 21:33:07.149496: step 23030, loss = 21.00 (3.1 examples/sec; 5.084 sec/batch)
2018-05-02 21:33:53.640858: step 23040, loss = 19.97 (3.2 examples/sec; 4.995 sec/batch)
2018-05-02 21:34:42.949418: step 23050, loss = 19.71 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 21:35:32.766126: step 23060, loss = 20.08 (3.3 examples/sec; 4.834 sec/batch)
2018-05-02 21:36:22.180279: step 23070, loss = 19.60 (3.2 examples/sec; 4.979 sec/batch)
2018-05-02 21:37:12.023638: step 23080, loss = 20.02 (3.3 examples/sec; 4.895 sec/batch)
2018-05-02 21:38:01.360988: step 23090, loss = 19.47 (3.3 examples/sec; 4.905 sec/batch)
2018-05-02 21:38:51.043530: step 23100, loss = 19.23 (3.1 examples/sec; 5.140 sec/batch)
2018-05-02 21:39:43.981568: step 23110, loss = 19.23 (3.2 examples/sec; 5.001 sec/batch)
2018-05-02 21:40:32.926975: step 23120, loss = 19.15 (3.3 examples/sec; 4.865 sec/batch)
2018-05-02 21:41:22.039929: step 23130, loss = 18.73 (3.4 examples/sec; 4.770 sec/batch)
2018-05-02 21:42:10.964780: step 23140, loss = 19.45 (3.2 examples/sec; 4.984 sec/batch)
2018-05-02 21:43:00.783224: step 23150, loss = 20.14 (3.2 examples/sec; 4.991 sec/batch)
2018-05-02 21:43:50.366857: step 23160, loss = 19.02 (3.3 examples/sec; 4.882 sec/batch)
2018-05-02 21:44:36.230748: step 23170, loss = 18.50 (3.3 examples/sec; 4.795 sec/batch)
2018-05-02 21:45:25.533778: step 23180, loss = 19.25 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 21:46:14.895120: step 23190, loss = 20.79 (3.3 examples/sec; 4.826 sec/batch)
2018-05-02 21:47:04.830140: step 23200, loss = 19.16 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 21:47:58.195900: step 23210, loss = 19.53 (3.2 examples/sec; 4.983 sec/batch)
2018-05-02 21:48:47.550271: step 23220, loss = 19.26 (3.2 examples/sec; 4.932 sec/batch)
2018-05-02 21:49:36.847445: step 23230, loss = 20.01 (3.3 examples/sec; 4.918 sec/batch)
2018-05-02 21:50:25.877549: step 23240, loss = 19.29 (3.3 examples/sec; 4.877 sec/batch)
2018-05-02 21:51:15.485469: step 23250, loss = 19.47 (3.2 examples/sec; 4.974 sec/batch)
2018-05-02 21:52:04.696091: step 23260, loss = 20.33 (3.2 examples/sec; 5.003 sec/batch)
2018-05-02 21:52:54.260682: step 23270, loss = 19.35 (3.2 examples/sec; 4.945 sec/batch)
2018-05-02 21:53:43.108416: step 23280, loss = 19.14 (3.3 examples/sec; 4.849 sec/batch)
2018-05-02 21:54:29.599998: step 23290, loss = 20.37 (3.2 examples/sec; 5.029 sec/batch)
2018-05-02 21:55:19.367543: step 23300, loss = 20.04 (3.2 examples/sec; 4.940 sec/batch)
2018-05-02 21:56:12.261690: step 23310, loss = 19.30 (3.2 examples/sec; 4.956 sec/batch)
2018-05-02 21:57:01.524551: step 23320, loss = 19.44 (3.3 examples/sec; 4.918 sec/batch)
2018-05-02 21:57:51.322147: step 23330, loss = 19.61 (3.3 examples/sec; 4.865 sec/batch)
2018-05-02 21:58:40.653778: step 23340, loss = 20.98 (3.2 examples/sec; 4.990 sec/batch)
2018-05-02 21:59:29.859825: step 23350, loss = 19.40 (3.4 examples/sec; 4.763 sec/batch)
2018-05-02 22:00:19.457145: step 23360, loss = 20.37 (3.2 examples/sec; 5.017 sec/batch)
2018-05-02 22:01:08.906952: step 23370, loss = 19.92 (3.3 examples/sec; 4.841 sec/batch)
2018-05-02 22:01:58.441690: step 23380, loss = 20.05 (3.2 examples/sec; 5.045 sec/batch)
2018-05-02 22:02:47.648687: step 23390, loss = 19.53 (3.2 examples/sec; 4.989 sec/batch)
2018-05-02 22:03:36.784744: step 23400, loss = 18.67 (3.2 examples/sec; 4.923 sec/batch)
2018-05-02 22:04:28.208071: step 23410, loss = 19.66 (4.3 examples/sec; 3.679 sec/batch)
2018-05-02 22:05:16.449897: step 23420, loss = 19.76 (3.2 examples/sec; 5.060 sec/batch)
2018-05-02 22:06:06.018247: step 23430, loss = 19.63 (3.2 examples/sec; 5.005 sec/batch)
2018-05-02 22:06:55.046135: step 23440, loss = 19.08 (3.3 examples/sec; 4.824 sec/batch)
2018-05-02 22:07:44.055269: step 23450, loss = 20.01 (3.3 examples/sec; 4.897 sec/batch)
2018-05-02 22:08:33.232603: step 23460, loss = 18.99 (3.2 examples/sec; 4.981 sec/batch)
2018-05-02 22:09:22.419587: step 23470, loss = 19.58 (3.4 examples/sec; 4.706 sec/batch)
2018-05-02 22:10:11.660811: step 23480, loss = 19.20 (3.3 examples/sec; 4.870 sec/batch)
2018-05-02 22:11:01.532040: step 23490, loss = 19.97 (3.1 examples/sec; 5.182 sec/batch)
2018-05-02 22:11:50.851963: step 23500, loss = 19.13 (3.3 examples/sec; 4.871 sec/batch)
2018-05-02 22:12:43.515745: step 23510, loss = 19.82 (3.1 examples/sec; 5.093 sec/batch)
2018-05-02 22:13:32.766549: step 23520, loss = 20.05 (3.3 examples/sec; 4.834 sec/batch)
2018-05-02 22:14:22.154246: step 23530, loss = 20.28 (3.1 examples/sec; 5.174 sec/batch)
2018-05-02 22:15:08.882599: step 23540, loss = 20.88 (3.2 examples/sec; 5.060 sec/batch)
2018-05-02 22:15:59.082287: step 23550, loss = 19.34 (3.2 examples/sec; 4.991 sec/batch)
2018-05-02 22:16:48.484175: step 23560, loss = 20.34 (3.2 examples/sec; 5.010 sec/batch)
2018-05-02 22:17:38.037724: step 23570, loss = 19.23 (3.5 examples/sec; 4.610 sec/batch)
2018-05-02 22:18:27.736749: step 23580, loss = 18.88 (3.3 examples/sec; 4.844 sec/batch)
2018-05-02 22:19:17.188329: step 23590, loss = 19.95 (3.3 examples/sec; 4.833 sec/batch)
2018-05-02 22:20:06.486311: step 23600, loss = 20.89 (3.2 examples/sec; 4.975 sec/batch)
2018-05-02 22:20:59.690688: step 23610, loss = 19.08 (3.3 examples/sec; 4.920 sec/batch)
2018-05-02 22:21:49.261695: step 23620, loss = 19.25 (3.2 examples/sec; 5.023 sec/batch)
2018-05-02 22:22:39.218576: step 23630, loss = 20.14 (3.2 examples/sec; 4.974 sec/batch)
2018-05-02 22:23:28.131328: step 23640, loss = 20.14 (3.4 examples/sec; 4.696 sec/batch)
2018-05-02 22:24:18.088789: step 23650, loss = 20.19 (3.2 examples/sec; 4.978 sec/batch)
2018-05-02 22:25:04.674111: step 23660, loss = 18.96 (3.3 examples/sec; 4.885 sec/batch)
2018-05-02 22:25:53.873196: step 23670, loss = 19.53 (3.2 examples/sec; 4.969 sec/batch)
2018-05-02 22:26:43.012693: step 23680, loss = 19.75 (3.2 examples/sec; 5.021 sec/batch)
2018-05-02 22:27:32.130770: step 23690, loss = 18.94 (3.4 examples/sec; 4.762 sec/batch)
2018-05-02 22:28:20.952644: step 23700, loss = 18.95 (3.3 examples/sec; 4.790 sec/batch)
2018-05-02 22:29:14.040995: step 23710, loss = 18.84 (3.2 examples/sec; 4.950 sec/batch)
2018-05-02 22:30:02.534779: step 23720, loss = 19.21 (3.3 examples/sec; 4.915 sec/batch)
2018-05-02 22:30:51.766983: step 23730, loss = 20.09 (3.2 examples/sec; 4.974 sec/batch)
2018-05-02 22:31:41.082448: step 23740, loss = 20.80 (3.2 examples/sec; 5.035 sec/batch)
2018-05-02 22:32:30.544989: step 23750, loss = 18.93 (3.3 examples/sec; 4.796 sec/batch)
2018-05-02 22:33:20.877962: step 23760, loss = 19.52 (3.2 examples/sec; 4.950 sec/batch)
2018-05-02 22:34:10.540624: step 23770, loss = 19.05 (3.2 examples/sec; 4.948 sec/batch)
2018-05-02 22:34:59.729130: step 23780, loss = 19.57 (3.3 examples/sec; 4.906 sec/batch)
2018-05-02 22:35:46.618855: step 23790, loss = 20.25 (3.2 examples/sec; 5.009 sec/batch)
2018-05-02 22:36:35.448516: step 23800, loss = 19.59 (3.2 examples/sec; 5.055 sec/batch)
2018-05-02 22:37:28.668528: step 23810, loss = 19.05 (3.2 examples/sec; 5.011 sec/batch)
2018-05-02 22:38:18.529983: step 23820, loss = 18.98 (3.2 examples/sec; 5.061 sec/batch)
2018-05-02 22:39:07.611526: step 23830, loss = 19.88 (3.3 examples/sec; 4.824 sec/batch)
2018-05-02 22:39:57.581172: step 23840, loss = 19.42 (3.3 examples/sec; 4.892 sec/batch)
2018-05-02 22:40:46.821576: step 23850, loss = 19.25 (3.2 examples/sec; 5.036 sec/batch)
2018-05-02 22:41:36.456634: step 23860, loss = 19.43 (3.2 examples/sec; 5.058 sec/batch)
2018-05-02 22:42:25.758669: step 23870, loss = 19.51 (3.3 examples/sec; 4.895 sec/batch)
2018-05-02 22:43:14.626452: step 23880, loss = 20.09 (3.3 examples/sec; 4.823 sec/batch)
2018-05-02 22:44:03.365930: step 23890, loss = 19.98 (3.2 examples/sec; 4.993 sec/batch)
2018-05-02 22:44:52.334141: step 23900, loss = 19.32 (3.2 examples/sec; 4.953 sec/batch)
2018-05-02 22:45:41.966079: step 23910, loss = 18.80 (3.2 examples/sec; 4.931 sec/batch)
2018-05-02 22:46:31.227429: step 23920, loss = 20.01 (3.2 examples/sec; 5.001 sec/batch)
2018-05-02 22:47:20.847011: step 23930, loss = 20.03 (3.2 examples/sec; 5.047 sec/batch)
2018-05-02 22:48:09.903960: step 23940, loss = 19.52 (3.3 examples/sec; 4.901 sec/batch)
2018-05-02 22:48:59.613569: step 23950, loss = 21.02 (3.3 examples/sec; 4.908 sec/batch)
2018-05-02 22:49:49.874851: step 23960, loss = 20.23 (3.2 examples/sec; 5.042 sec/batch)
2018-05-02 22:50:39.698469: step 23970, loss = 19.74 (3.3 examples/sec; 4.884 sec/batch)
2018-05-02 22:51:28.813006: step 23980, loss = 19.18 (3.4 examples/sec; 4.758 sec/batch)
2018-05-02 22:52:18.269271: step 23990, loss = 19.35 (3.3 examples/sec; 4.807 sec/batch)
2018-05-02 22:53:08.125883: step 24000, loss = 20.14 (3.3 examples/sec; 4.880 sec/batch)
2018-05-02 22:54:00.760868: step 24010, loss = 19.53 (3.2 examples/sec; 5.043 sec/batch)
2018-05-02 22:54:50.381728: step 24020, loss = 19.69 (3.1 examples/sec; 5.096 sec/batch)
2018-05-02 22:55:39.646539: step 24030, loss = 18.77 (3.3 examples/sec; 4.861 sec/batch)
2018-05-02 22:56:26.820534: step 24040, loss = 19.06 (3.2 examples/sec; 5.008 sec/batch)
2018-05-02 22:57:16.626954: step 24050, loss = 19.44 (3.2 examples/sec; 4.995 sec/batch)
2018-05-02 22:58:05.746100: step 24060, loss = 20.02 (3.3 examples/sec; 4.819 sec/batch)
2018-05-02 22:58:55.360859: step 24070, loss = 20.11 (3.3 examples/sec; 4.897 sec/batch)
2018-05-02 22:59:44.642647: step 24080, loss = 19.46 (3.3 examples/sec; 4.922 sec/batch)
2018-05-02 23:00:33.609578: step 24090, loss = 20.37 (3.3 examples/sec; 4.829 sec/batch)
2018-05-02 23:01:23.316241: step 24100, loss = 19.84 (3.2 examples/sec; 5.018 sec/batch)
2018-05-02 23:02:15.924742: step 24110, loss = 18.91 (3.2 examples/sec; 5.015 sec/batch)
2018-05-02 23:03:05.383518: step 24120, loss = 20.73 (3.3 examples/sec; 4.809 sec/batch)
2018-05-02 23:03:54.398417: step 24130, loss = 19.08 (3.2 examples/sec; 4.962 sec/batch)
2018-05-02 23:04:43.862677: step 24140, loss = 19.83 (3.3 examples/sec; 4.817 sec/batch)
2018-05-02 23:05:33.067112: step 24150, loss = 18.67 (3.2 examples/sec; 4.943 sec/batch)
2018-05-02 23:06:19.473102: step 24160, loss = 19.27 (3.2 examples/sec; 4.964 sec/batch)
2018-05-02 23:07:08.980538: step 24170, loss = 20.74 (3.2 examples/sec; 5.022 sec/batch)
2018-05-02 23:07:58.721734: step 24180, loss = 18.88 (3.3 examples/sec; 4.878 sec/batch)
2018-05-02 23:08:48.268155: step 24190, loss = 18.94 (3.2 examples/sec; 5.067 sec/batch)
2018-05-02 23:09:37.481566: step 24200, loss = 19.48 (3.2 examples/sec; 5.013 sec/batch)
2018-05-02 23:10:30.574399: step 24210, loss = 20.65 (3.2 examples/sec; 4.929 sec/batch)
2018-05-02 23:11:20.085113: step 24220, loss = 19.44 (3.3 examples/sec; 4.903 sec/batch)
2018-05-02 23:12:08.989088: step 24230, loss = 19.43 (3.2 examples/sec; 4.992 sec/batch)
2018-05-02 23:12:58.726397: step 24240, loss = 19.00 (3.3 examples/sec; 4.918 sec/batch)
2018-05-02 23:13:48.134599: step 24250, loss = 20.47 (3.2 examples/sec; 5.041 sec/batch)
2018-05-02 23:14:37.834377: step 24260, loss = 19.45 (3.3 examples/sec; 4.918 sec/batch)
2018-05-02 23:15:27.132818: step 24270, loss = 19.39 (3.3 examples/sec; 4.896 sec/batch)
2018-05-02 23:16:17.166256: step 24280, loss = 19.60 (3.2 examples/sec; 5.055 sec/batch)
2018-05-02 23:17:03.656256: step 24290, loss = 20.11 (3.2 examples/sec; 4.963 sec/batch)
2018-05-02 23:17:53.030604: step 24300, loss = 20.54 (3.1 examples/sec; 5.197 sec/batch)
2018-05-02 23:18:46.676506: step 24310, loss = 20.55 (3.1 examples/sec; 5.088 sec/batch)
2018-05-02 23:19:36.406637: step 24320, loss = 19.54 (3.3 examples/sec; 4.895 sec/batch)
2018-05-02 23:20:25.381204: step 24330, loss = 20.08 (3.3 examples/sec; 4.855 sec/batch)
2018-05-02 23:21:14.732319: step 24340, loss = 19.47 (3.2 examples/sec; 4.973 sec/batch)
2018-05-02 23:22:04.249377: step 24350, loss = 19.13 (3.2 examples/sec; 4.955 sec/batch)
2018-05-02 23:22:54.327089: step 24360, loss = 19.52 (3.2 examples/sec; 4.946 sec/batch)
2018-05-02 23:23:43.885484: step 24370, loss = 19.60 (3.2 examples/sec; 4.958 sec/batch)
2018-05-02 23:24:33.489158: step 24380, loss = 18.79 (3.2 examples/sec; 4.950 sec/batch)
2018-05-02 23:25:22.229868: step 24390, loss = 20.07 (3.3 examples/sec; 4.923 sec/batch)
2018-05-02 23:26:12.029056: step 24400, loss = 18.48 (3.2 examples/sec; 5.054 sec/batch)
2018-05-02 23:27:02.167858: step 24410, loss = 20.20 (3.3 examples/sec; 4.921 sec/batch)
2018-05-02 23:27:51.745181: step 24420, loss = 19.74 (3.2 examples/sec; 5.065 sec/batch)
2018-05-02 23:28:41.500938: step 24430, loss = 19.47 (3.1 examples/sec; 5.088 sec/batch)
2018-05-02 23:29:30.772042: step 24440, loss = 19.24 (3.3 examples/sec; 4.889 sec/batch)
2018-05-02 23:30:20.398828: step 24450, loss = 20.32 (3.3 examples/sec; 4.861 sec/batch)
2018-05-02 23:31:09.303795: step 24460, loss = 19.43 (3.3 examples/sec; 4.843 sec/batch)
2018-05-02 23:31:59.570758: step 24470, loss = 18.94 (3.1 examples/sec; 5.155 sec/batch)
2018-05-02 23:32:49.235156: step 24480, loss = 19.00 (3.2 examples/sec; 5.069 sec/batch)
2018-05-02 23:33:38.679055: step 24490, loss = 20.28 (3.2 examples/sec; 5.046 sec/batch)
2018-05-02 23:34:28.487233: step 24500, loss = 19.84 (3.2 examples/sec; 5.064 sec/batch)
2018-05-02 23:35:21.271408: step 24510, loss = 20.01 (3.2 examples/sec; 4.967 sec/batch)
2018-05-02 23:36:10.426849: step 24520, loss = 19.37 (3.1 examples/sec; 5.221 sec/batch)
2018-05-02 23:36:58.161850: step 24530, loss = 19.24 (4.2 examples/sec; 3.831 sec/batch)
2018-05-02 23:37:46.477041: step 24540, loss = 19.60 (3.3 examples/sec; 4.886 sec/batch)
2018-05-02 23:38:35.892210: step 24550, loss = 19.35 (3.2 examples/sec; 5.050 sec/batch)
2018-05-02 23:39:25.600793: step 24560, loss = 20.64 (3.3 examples/sec; 4.908 sec/batch)
2018-05-02 23:40:14.792628: step 24570, loss = 19.62 (3.2 examples/sec; 4.993 sec/batch)
2018-05-02 23:41:03.891313: step 24580, loss = 19.97 (3.2 examples/sec; 4.939 sec/batch)
2018-05-02 23:41:53.590541: step 24590, loss = 19.01 (3.3 examples/sec; 4.921 sec/batch)
2018-05-02 23:42:43.042457: step 24600, loss = 20.47 (3.3 examples/sec; 4.821 sec/batch)
2018-05-02 23:43:35.781097: step 24610, loss = 19.53 (3.2 examples/sec; 4.930 sec/batch)
2018-05-02 23:44:25.475371: step 24620, loss = 18.88 (3.2 examples/sec; 5.020 sec/batch)
2018-05-02 23:45:15.450432: step 24630, loss = 21.45 (3.2 examples/sec; 4.939 sec/batch)
2018-05-02 23:46:04.502638: step 24640, loss = 20.01 (3.3 examples/sec; 4.780 sec/batch)
2018-05-02 23:46:54.832386: step 24650, loss = 20.29 (3.1 examples/sec; 5.121 sec/batch)
2018-05-02 23:47:41.098767: step 24660, loss = 19.26 (3.2 examples/sec; 4.996 sec/batch)
2018-05-02 23:48:30.676624: step 24670, loss = 19.01 (3.1 examples/sec; 5.109 sec/batch)
2018-05-02 23:49:20.770127: step 24680, loss = 18.92 (3.2 examples/sec; 5.043 sec/batch)
2018-05-02 23:50:10.242574: step 24690, loss = 18.97 (3.2 examples/sec; 4.981 sec/batch)
2018-05-02 23:50:59.590032: step 24700, loss = 19.05 (3.2 examples/sec; 4.973 sec/batch)
2018-05-02 23:51:53.065617: step 24710, loss = 19.12 (3.3 examples/sec; 4.872 sec/batch)
2018-05-02 23:52:43.295306: step 24720, loss = 19.12 (3.2 examples/sec; 4.936 sec/batch)
2018-05-02 23:53:32.242547: step 24730, loss = 19.28 (3.3 examples/sec; 4.857 sec/batch)
2018-05-02 23:54:21.994181: step 24740, loss = 20.37 (3.2 examples/sec; 5.015 sec/batch)
2018-05-02 23:55:10.968891: step 24750, loss = 20.30 (3.4 examples/sec; 4.728 sec/batch)
2018-05-02 23:56:00.313975: step 24760, loss = 19.78 (3.2 examples/sec; 4.986 sec/batch)
2018-05-02 23:56:49.639308: step 24770, loss = 19.73 (3.3 examples/sec; 4.905 sec/batch)
2018-05-02 23:57:37.432236: step 24780, loss = 19.33 (4.3 examples/sec; 3.720 sec/batch)
2018-05-02 23:58:26.061218: step 24790, loss = 19.98 (3.1 examples/sec; 5.164 sec/batch)
2018-05-02 23:59:15.498099: step 24800, loss = 20.13 (3.3 examples/sec; 4.905 sec/batch)
2018-05-03 00:00:08.488769: step 24810, loss = 19.23 (3.2 examples/sec; 4.937 sec/batch)
2018-05-03 00:00:58.191303: step 24820, loss = 20.36 (3.2 examples/sec; 5.027 sec/batch)
2018-05-03 00:01:47.899919: step 24830, loss = 19.01 (3.2 examples/sec; 5.015 sec/batch)
2018-05-03 00:02:37.740296: step 24840, loss = 19.69 (3.2 examples/sec; 5.051 sec/batch)
2018-05-03 00:03:27.232870: step 24850, loss = 19.21 (3.3 examples/sec; 4.869 sec/batch)
2018-05-03 00:04:16.821464: step 24860, loss = 19.33 (3.2 examples/sec; 5.073 sec/batch)
2018-05-03 00:05:06.741009: step 24870, loss = 19.39 (3.2 examples/sec; 4.968 sec/batch)
2018-05-03 00:05:56.416927: step 24880, loss = 19.04 (3.3 examples/sec; 4.840 sec/batch)
2018-05-03 00:06:46.024724: step 24890, loss = 19.68 (3.4 examples/sec; 4.755 sec/batch)
2018-05-03 00:07:35.980698: step 24900, loss = 19.13 (3.2 examples/sec; 4.930 sec/batch)
2018-05-03 00:08:26.538296: step 24910, loss = 18.67 (3.2 examples/sec; 4.963 sec/batch)
2018-05-03 00:09:16.579858: step 24920, loss = 20.00 (3.3 examples/sec; 4.920 sec/batch)
2018-05-03 00:10:05.754412: step 24930, loss = 19.86 (3.1 examples/sec; 5.142 sec/batch)
2018-05-03 00:10:55.794713: step 24940, loss = 19.85 (3.3 examples/sec; 4.903 sec/batch)
2018-05-03 00:11:45.664612: step 24950, loss = 19.70 (3.1 examples/sec; 5.170 sec/batch)
2018-05-03 00:12:34.419344: step 24960, loss = 19.75 (3.2 examples/sec; 4.957 sec/batch)
2018-05-03 00:13:24.923891: step 24970, loss = 19.62 (3.2 examples/sec; 4.926 sec/batch)
2018-05-03 00:14:14.878603: step 24980, loss = 20.13 (3.3 examples/sec; 4.902 sec/batch)
2018-05-03 00:15:04.191214: step 24990, loss = 20.63 (3.3 examples/sec; 4.890 sec/batch)
2018-05-03 00:15:53.419319: step 25000, loss = 19.71 (3.3 examples/sec; 4.920 sec/batch)
2018-05-03 00:16:46.317450: step 25010, loss = 18.96 (3.2 examples/sec; 5.049 sec/batch)
2018-05-03 00:17:35.118969: step 25020, loss = 19.80 (3.3 examples/sec; 4.803 sec/batch)
2018-05-03 00:18:21.703572: step 25030, loss = 18.89 (3.1 examples/sec; 5.118 sec/batch)
2018-05-03 00:19:10.870936: step 25040, loss = 18.98 (3.3 examples/sec; 4.783 sec/batch)
2018-05-03 00:20:00.530980: step 25050, loss = 18.81 (3.1 examples/sec; 5.120 sec/batch)
2018-05-03 00:20:50.202088: step 25060, loss = 19.55 (3.2 examples/sec; 5.010 sec/batch)
2018-05-03 00:21:39.289988: step 25070, loss = 18.87 (3.3 examples/sec; 4.890 sec/batch)
2018-05-03 00:22:28.978665: step 25080, loss = 19.54 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 00:23:18.585209: step 25090, loss = 19.41 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 00:24:08.187882: step 25100, loss = 19.07 (3.4 examples/sec; 4.757 sec/batch)
2018-05-03 00:25:01.880983: step 25110, loss = 19.07 (3.1 examples/sec; 5.148 sec/batch)
2018-05-03 00:25:51.684592: step 25120, loss = 19.10 (3.3 examples/sec; 4.849 sec/batch)
2018-05-03 00:26:40.836333: step 25130, loss = 20.24 (3.2 examples/sec; 4.932 sec/batch)
2018-05-03 00:27:29.738045: step 25140, loss = 19.15 (3.3 examples/sec; 4.887 sec/batch)
2018-05-03 00:28:17.948518: step 25150, loss = 19.09 (4.2 examples/sec; 3.770 sec/batch)
2018-05-03 00:29:06.343063: step 25160, loss = 20.16 (3.2 examples/sec; 4.934 sec/batch)
2018-05-03 00:29:55.709300: step 25170, loss = 19.36 (3.3 examples/sec; 4.900 sec/batch)
2018-05-03 00:30:44.717006: step 25180, loss = 18.75 (3.3 examples/sec; 4.822 sec/batch)
2018-05-03 00:31:33.958527: step 25190, loss = 20.03 (3.4 examples/sec; 4.683 sec/batch)
2018-05-03 00:32:23.629492: step 25200, loss = 19.61 (3.1 examples/sec; 5.184 sec/batch)
2018-05-03 00:33:17.158350: step 25210, loss = 20.44 (3.3 examples/sec; 4.831 sec/batch)
2018-05-03 00:34:06.456944: step 25220, loss = 19.55 (3.3 examples/sec; 4.892 sec/batch)
2018-05-03 00:34:56.005568: step 25230, loss = 19.56 (3.3 examples/sec; 4.823 sec/batch)
2018-05-03 00:35:46.446152: step 25240, loss = 20.41 (3.1 examples/sec; 5.083 sec/batch)
2018-05-03 00:36:35.885133: step 25250, loss = 18.40 (3.2 examples/sec; 5.001 sec/batch)
2018-05-03 00:37:25.596088: step 25260, loss = 19.68 (3.3 examples/sec; 4.871 sec/batch)
2018-05-03 00:38:15.214587: step 25270, loss = 19.42 (3.2 examples/sec; 4.928 sec/batch)
2018-05-03 00:39:01.038805: step 25280, loss = 19.99 (3.3 examples/sec; 4.829 sec/batch)
2018-05-03 00:39:50.434298: step 25290, loss = 19.21 (3.1 examples/sec; 5.113 sec/batch)
2018-05-03 00:40:40.606670: step 25300, loss = 19.46 (3.3 examples/sec; 4.916 sec/batch)
2018-05-03 00:41:34.241536: step 25310, loss = 19.65 (3.3 examples/sec; 4.910 sec/batch)
2018-05-03 00:42:23.993889: step 25320, loss = 18.65 (3.3 examples/sec; 4.823 sec/batch)
2018-05-03 00:43:13.718494: step 25330, loss = 20.18 (3.3 examples/sec; 4.871 sec/batch)
2018-05-03 00:44:03.554825: step 25340, loss = 19.77 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 00:44:53.755072: step 25350, loss = 18.31 (3.2 examples/sec; 4.938 sec/batch)
2018-05-03 00:45:43.229060: step 25360, loss = 20.97 (3.2 examples/sec; 5.007 sec/batch)
2018-05-03 00:46:33.016283: step 25370, loss = 19.08 (3.2 examples/sec; 5.077 sec/batch)
2018-05-03 00:47:22.197073: step 25380, loss = 20.05 (3.1 examples/sec; 5.133 sec/batch)
2018-05-03 00:48:12.008505: step 25390, loss = 19.98 (3.2 examples/sec; 5.012 sec/batch)
2018-05-03 00:48:59.206613: step 25400, loss = 18.76 (3.1 examples/sec; 5.226 sec/batch)
2018-05-03 00:49:51.648070: step 25410, loss = 18.66 (3.3 examples/sec; 4.808 sec/batch)
2018-05-03 00:50:41.806913: step 25420, loss = 18.71 (3.2 examples/sec; 4.968 sec/batch)
2018-05-03 00:51:31.505279: step 25430, loss = 19.39 (3.2 examples/sec; 5.026 sec/batch)
2018-05-03 00:52:20.540709: step 25440, loss = 18.98 (3.3 examples/sec; 4.916 sec/batch)
2018-05-03 00:53:10.108153: step 25450, loss = 20.52 (3.1 examples/sec; 5.086 sec/batch)
2018-05-03 00:53:59.814722: step 25460, loss = 20.02 (3.2 examples/sec; 5.059 sec/batch)
2018-05-03 00:54:49.383410: step 25470, loss = 19.84 (3.4 examples/sec; 4.751 sec/batch)
2018-05-03 00:55:38.870708: step 25480, loss = 19.61 (3.2 examples/sec; 4.966 sec/batch)
2018-05-03 00:56:27.535626: step 25490, loss = 18.95 (3.3 examples/sec; 4.869 sec/batch)
2018-05-03 00:57:16.353021: step 25500, loss = 18.96 (3.3 examples/sec; 4.786 sec/batch)
2018-05-03 00:58:09.319009: step 25510, loss = 19.18 (3.2 examples/sec; 5.034 sec/batch)
2018-05-03 00:58:57.420818: step 25520, loss = 18.84 (4.2 examples/sec; 3.775 sec/batch)
2018-05-03 00:59:45.971633: step 25530, loss = 19.00 (3.1 examples/sec; 5.157 sec/batch)
2018-05-03 01:00:34.940542: step 25540, loss = 19.80 (3.3 examples/sec; 4.817 sec/batch)
2018-05-03 01:01:24.018258: step 25550, loss = 19.05 (3.1 examples/sec; 5.116 sec/batch)
2018-05-03 01:02:12.971404: step 25560, loss = 19.03 (3.2 examples/sec; 4.926 sec/batch)
2018-05-03 01:03:02.277575: step 25570, loss = 18.58 (3.3 examples/sec; 4.804 sec/batch)
2018-05-03 01:03:51.579227: step 25580, loss = 19.59 (3.4 examples/sec; 4.771 sec/batch)
2018-05-03 01:04:40.230136: step 25590, loss = 19.57 (3.3 examples/sec; 4.789 sec/batch)
2018-05-03 01:05:29.614092: step 25600, loss = 21.32 (3.3 examples/sec; 4.834 sec/batch)
2018-05-03 01:06:22.783164: step 25610, loss = 19.75 (3.3 examples/sec; 4.908 sec/batch)
2018-05-03 01:07:11.800657: step 25620, loss = 19.10 (3.2 examples/sec; 4.932 sec/batch)
2018-05-03 01:08:00.694395: step 25630, loss = 18.64 (3.3 examples/sec; 4.839 sec/batch)
2018-05-03 01:08:49.883216: step 25640, loss = 19.06 (3.3 examples/sec; 4.900 sec/batch)
2018-05-03 01:09:36.580716: step 25650, loss = 19.35 (3.3 examples/sec; 4.886 sec/batch)
2018-05-03 01:10:26.026959: step 25660, loss = 18.78 (3.2 examples/sec; 5.007 sec/batch)
2018-05-03 01:11:15.586280: step 25670, loss = 19.22 (3.3 examples/sec; 4.879 sec/batch)
2018-05-03 01:12:04.776183: step 25680, loss = 19.28 (3.2 examples/sec; 5.072 sec/batch)
2018-05-03 01:12:54.506545: step 25690, loss = 19.06 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 01:13:43.953870: step 25700, loss = 18.41 (3.2 examples/sec; 5.009 sec/batch)
2018-05-03 01:14:36.878022: step 25710, loss = 19.10 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 01:15:26.392445: step 25720, loss = 19.16 (3.2 examples/sec; 5.034 sec/batch)
2018-05-03 01:16:15.484057: step 25730, loss = 19.12 (3.3 examples/sec; 4.831 sec/batch)
2018-05-03 01:17:04.573509: step 25740, loss = 20.17 (3.2 examples/sec; 4.981 sec/batch)
2018-05-03 01:17:53.491912: step 25750, loss = 19.14 (3.3 examples/sec; 4.841 sec/batch)
2018-05-03 01:18:42.372521: step 25760, loss = 18.92 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 01:19:30.627611: step 25770, loss = 20.05 (4.2 examples/sec; 3.803 sec/batch)
2018-05-03 01:20:18.757866: step 25780, loss = 18.67 (3.3 examples/sec; 4.903 sec/batch)
2018-05-03 01:21:08.694046: step 25790, loss = 19.26 (3.2 examples/sec; 4.982 sec/batch)
2018-05-03 01:21:58.168780: step 25800, loss = 20.13 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 01:22:51.545654: step 25810, loss = 19.07 (3.2 examples/sec; 5.077 sec/batch)
2018-05-03 01:23:41.882406: step 25820, loss = 19.11 (3.2 examples/sec; 4.930 sec/batch)
2018-05-03 01:24:31.331483: step 25830, loss = 20.09 (3.3 examples/sec; 4.859 sec/batch)
2018-05-03 01:25:21.611254: step 25840, loss = 19.61 (3.0 examples/sec; 5.253 sec/batch)
2018-05-03 01:26:10.862123: step 25850, loss = 18.83 (3.3 examples/sec; 4.819 sec/batch)
2018-05-03 01:27:00.122091: step 25860, loss = 19.14 (3.3 examples/sec; 4.814 sec/batch)
2018-05-03 01:27:49.371269: step 25870, loss = 19.34 (3.2 examples/sec; 4.940 sec/batch)
2018-05-03 01:28:39.108482: step 25880, loss = 19.54 (3.3 examples/sec; 4.788 sec/batch)
2018-05-03 01:29:28.503949: step 25890, loss = 19.30 (3.2 examples/sec; 4.981 sec/batch)
2018-05-03 01:30:13.908580: step 25900, loss = 19.15 (3.4 examples/sec; 4.658 sec/batch)
2018-05-03 01:31:06.784264: step 25910, loss = 19.08 (3.4 examples/sec; 4.749 sec/batch)
2018-05-03 01:31:56.217592: step 25920, loss = 18.81 (3.3 examples/sec; 4.888 sec/batch)
2018-05-03 01:32:46.503913: step 25930, loss = 20.17 (3.2 examples/sec; 4.970 sec/batch)
2018-05-03 01:33:36.005565: step 25940, loss = 20.04 (3.2 examples/sec; 4.999 sec/batch)
2018-05-03 01:34:25.631890: step 25950, loss = 19.15 (3.3 examples/sec; 4.839 sec/batch)
2018-05-03 01:35:14.814361: step 25960, loss = 20.42 (3.3 examples/sec; 4.882 sec/batch)
2018-05-03 01:36:04.029018: step 25970, loss = 20.15 (3.3 examples/sec; 4.903 sec/batch)
2018-05-03 01:36:53.092312: step 25980, loss = 20.46 (3.2 examples/sec; 4.992 sec/batch)
2018-05-03 01:37:42.715785: step 25990, loss = 19.00 (3.3 examples/sec; 4.814 sec/batch)
2018-05-03 01:38:32.503521: step 26000, loss = 19.04 (3.1 examples/sec; 5.096 sec/batch)
2018-05-03 01:39:25.579478: step 26010, loss = 19.44 (3.2 examples/sec; 4.978 sec/batch)
2018-05-03 01:40:12.048166: step 26020, loss = 20.70 (3.2 examples/sec; 4.956 sec/batch)
2018-05-03 01:41:02.224464: step 26030, loss = 19.33 (3.1 examples/sec; 5.104 sec/batch)
2018-05-03 01:41:51.139526: step 26040, loss = 19.60 (3.3 examples/sec; 4.807 sec/batch)
2018-05-03 01:42:40.014478: step 26050, loss = 19.72 (3.3 examples/sec; 4.856 sec/batch)
2018-05-03 01:43:29.053491: step 26060, loss = 19.09 (3.3 examples/sec; 4.820 sec/batch)
2018-05-03 01:44:18.354387: step 26070, loss = 18.77 (3.1 examples/sec; 5.094 sec/batch)
2018-05-03 01:45:07.697671: step 26080, loss = 19.25 (3.2 examples/sec; 4.993 sec/batch)
2018-05-03 01:45:57.035529: step 26090, loss = 20.03 (3.4 examples/sec; 4.775 sec/batch)
2018-05-03 01:46:46.155398: step 26100, loss = 19.77 (3.3 examples/sec; 4.910 sec/batch)
2018-05-03 01:47:39.609360: step 26110, loss = 18.72 (3.1 examples/sec; 5.081 sec/batch)
2018-05-03 01:48:29.251806: step 26120, loss = 19.13 (3.3 examples/sec; 4.850 sec/batch)
2018-05-03 01:49:18.775379: step 26130, loss = 18.55 (3.2 examples/sec; 4.993 sec/batch)
2018-05-03 01:50:08.273859: step 26140, loss = 19.35 (3.3 examples/sec; 4.899 sec/batch)
2018-05-03 01:50:55.279385: step 26150, loss = 19.61 (3.2 examples/sec; 4.986 sec/batch)
2018-05-03 01:51:44.960341: step 26160, loss = 19.32 (3.3 examples/sec; 4.841 sec/batch)
2018-05-03 01:52:34.242751: step 26170, loss = 18.69 (3.3 examples/sec; 4.885 sec/batch)
2018-05-03 01:53:23.246027: step 26180, loss = 19.72 (3.3 examples/sec; 4.812 sec/batch)
2018-05-03 01:54:12.344708: step 26190, loss = 19.54 (3.1 examples/sec; 5.120 sec/batch)
2018-05-03 01:55:01.708787: step 26200, loss = 19.66 (3.2 examples/sec; 4.938 sec/batch)
2018-05-03 01:55:54.024715: step 26210, loss = 19.83 (3.2 examples/sec; 5.059 sec/batch)
2018-05-03 01:56:42.974033: step 26220, loss = 18.36 (3.3 examples/sec; 4.839 sec/batch)
2018-05-03 01:57:32.284386: step 26230, loss = 18.65 (3.3 examples/sec; 4.875 sec/batch)
2018-05-03 01:58:21.674645: step 26240, loss = 19.26 (3.2 examples/sec; 4.927 sec/batch)
2018-05-03 01:59:11.322976: step 26250, loss = 18.57 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 02:00:00.680628: step 26260, loss = 18.42 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 02:00:46.876177: step 26270, loss = 19.42 (3.2 examples/sec; 4.928 sec/batch)
2018-05-03 02:01:36.186508: step 26280, loss = 19.53 (3.2 examples/sec; 4.977 sec/batch)
2018-05-03 02:02:25.001362: step 26290, loss = 19.31 (3.2 examples/sec; 5.030 sec/batch)
2018-05-03 02:03:14.189428: step 26300, loss = 19.64 (3.3 examples/sec; 4.841 sec/batch)
2018-05-03 02:04:07.283927: step 26310, loss = 18.65 (3.2 examples/sec; 4.945 sec/batch)
2018-05-03 02:04:57.317626: step 26320, loss = 18.59 (3.2 examples/sec; 4.981 sec/batch)
2018-05-03 02:05:46.370360: step 26330, loss = 19.35 (3.3 examples/sec; 4.829 sec/batch)
2018-05-03 02:06:36.182266: step 26340, loss = 19.91 (3.2 examples/sec; 4.934 sec/batch)
2018-05-03 02:07:25.315968: step 26350, loss = 19.15 (3.3 examples/sec; 4.900 sec/batch)
2018-05-03 02:08:14.313689: step 26360, loss = 19.71 (3.3 examples/sec; 4.856 sec/batch)
2018-05-03 02:09:03.772702: step 26370, loss = 19.29 (3.3 examples/sec; 4.878 sec/batch)
2018-05-03 02:09:53.076953: step 26380, loss = 19.16 (3.3 examples/sec; 4.779 sec/batch)
2018-05-03 02:10:41.679607: step 26390, loss = 19.71 (3.3 examples/sec; 4.883 sec/batch)
2018-05-03 02:11:27.736249: step 26400, loss = 18.69 (3.2 examples/sec; 5.019 sec/batch)
2018-05-03 02:12:19.955792: step 26410, loss = 19.96 (3.4 examples/sec; 4.739 sec/batch)
2018-05-03 02:13:09.685298: step 26420, loss = 18.72 (3.2 examples/sec; 5.026 sec/batch)
2018-05-03 02:13:58.926248: step 26430, loss = 19.54 (3.2 examples/sec; 4.965 sec/batch)
2018-05-03 02:14:47.949421: step 26440, loss = 19.13 (3.2 examples/sec; 4.972 sec/batch)
2018-05-03 02:15:37.183377: step 26450, loss = 18.94 (3.3 examples/sec; 4.843 sec/batch)
2018-05-03 02:16:26.906780: step 26460, loss = 19.39 (3.1 examples/sec; 5.134 sec/batch)
2018-05-03 02:17:15.591238: step 26470, loss = 19.16 (3.3 examples/sec; 4.793 sec/batch)
2018-05-03 02:18:04.913007: step 26480, loss = 18.68 (3.3 examples/sec; 4.870 sec/batch)
2018-05-03 02:18:54.618731: step 26490, loss = 18.47 (3.3 examples/sec; 4.857 sec/batch)
2018-05-03 02:19:43.709544: step 26500, loss = 19.68 (3.2 examples/sec; 4.930 sec/batch)
2018-05-03 02:20:36.551266: step 26510, loss = 18.53 (3.3 examples/sec; 4.828 sec/batch)
2018-05-03 02:21:23.147538: step 26520, loss = 18.72 (3.2 examples/sec; 5.036 sec/batch)
2018-05-03 02:22:12.363083: step 26530, loss = 20.23 (3.2 examples/sec; 5.024 sec/batch)
2018-05-03 02:23:01.705080: step 26540, loss = 18.84 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 02:23:51.016086: step 26550, loss = 18.80 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 02:24:40.188209: step 26560, loss = 18.97 (3.3 examples/sec; 4.820 sec/batch)
2018-05-03 02:25:29.322152: step 26570, loss = 18.23 (3.2 examples/sec; 4.985 sec/batch)
2018-05-03 02:26:18.237615: step 26580, loss = 18.71 (3.2 examples/sec; 4.956 sec/batch)
2018-05-03 02:27:07.691605: step 26590, loss = 19.80 (3.1 examples/sec; 5.135 sec/batch)
2018-05-03 02:27:56.568594: step 26600, loss = 19.05 (3.3 examples/sec; 4.890 sec/batch)
2018-05-03 02:28:49.428816: step 26610, loss = 19.30 (3.2 examples/sec; 4.959 sec/batch)
2018-05-03 02:29:39.409824: step 26620, loss = 19.04 (3.2 examples/sec; 4.937 sec/batch)
2018-05-03 02:30:29.786372: step 26630, loss = 19.08 (3.2 examples/sec; 4.999 sec/batch)
2018-05-03 02:31:19.146367: step 26640, loss = 19.34 (3.2 examples/sec; 4.949 sec/batch)
2018-05-03 02:32:06.094234: step 26650, loss = 18.95 (3.3 examples/sec; 4.871 sec/batch)
2018-05-03 02:32:55.745915: step 26660, loss = 19.45 (3.1 examples/sec; 5.101 sec/batch)
2018-05-03 02:33:44.862500: step 26670, loss = 19.23 (3.2 examples/sec; 4.959 sec/batch)
2018-05-03 02:34:33.962950: step 26680, loss = 19.09 (3.4 examples/sec; 4.740 sec/batch)
2018-05-03 02:35:23.382904: step 26690, loss = 20.01 (3.2 examples/sec; 4.979 sec/batch)
2018-05-03 02:36:13.558112: step 26700, loss = 18.47 (3.2 examples/sec; 4.924 sec/batch)
2018-05-03 02:37:07.016629: step 26710, loss = 19.62 (3.3 examples/sec; 4.811 sec/batch)
2018-05-03 02:37:56.573074: step 26720, loss = 18.65 (3.3 examples/sec; 4.911 sec/batch)
2018-05-03 02:38:45.608054: step 26730, loss = 19.34 (3.2 examples/sec; 5.038 sec/batch)
2018-05-03 02:39:35.424156: step 26740, loss = 19.62 (3.2 examples/sec; 4.974 sec/batch)
2018-05-03 02:40:24.581606: step 26750, loss = 18.49 (3.2 examples/sec; 4.958 sec/batch)
2018-05-03 02:41:13.390042: step 26760, loss = 19.24 (3.3 examples/sec; 4.792 sec/batch)
2018-05-03 02:42:00.293176: step 26770, loss = 18.67 (3.2 examples/sec; 5.053 sec/batch)
2018-05-03 02:42:49.764979: step 26780, loss = 19.88 (3.1 examples/sec; 5.108 sec/batch)
2018-05-03 02:43:39.581222: step 26790, loss = 19.54 (3.3 examples/sec; 4.907 sec/batch)
2018-05-03 02:44:29.858173: step 26800, loss = 18.96 (3.2 examples/sec; 4.996 sec/batch)
2018-05-03 02:45:22.611430: step 26810, loss = 19.39 (3.2 examples/sec; 4.934 sec/batch)
2018-05-03 02:46:12.008156: step 26820, loss = 19.64 (3.3 examples/sec; 4.874 sec/batch)
2018-05-03 02:47:00.867785: step 26830, loss = 18.99 (3.4 examples/sec; 4.687 sec/batch)
2018-05-03 02:47:50.003528: step 26840, loss = 20.77 (3.2 examples/sec; 4.984 sec/batch)
2018-05-03 02:48:38.812868: step 26850, loss = 19.60 (3.3 examples/sec; 4.858 sec/batch)
2018-05-03 02:49:27.850751: step 26860, loss = 19.08 (3.2 examples/sec; 4.958 sec/batch)
2018-05-03 02:50:16.683085: step 26870, loss = 19.27 (3.3 examples/sec; 4.860 sec/batch)
2018-05-03 02:51:06.441916: step 26880, loss = 19.43 (3.3 examples/sec; 4.814 sec/batch)
2018-05-03 02:51:55.371204: step 26890, loss = 18.96 (3.4 examples/sec; 4.743 sec/batch)
2018-05-03 02:52:42.601986: step 26900, loss = 19.23 (3.2 examples/sec; 5.032 sec/batch)
2018-05-03 02:53:35.007841: step 26910, loss = 20.41 (3.4 examples/sec; 4.773 sec/batch)
2018-05-03 02:54:24.016342: step 26920, loss = 19.44 (3.3 examples/sec; 4.811 sec/batch)
2018-05-03 02:55:13.357013: step 26930, loss = 19.26 (3.3 examples/sec; 4.904 sec/batch)
2018-05-03 02:56:02.668979: step 26940, loss = 18.69 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 02:56:51.978274: step 26950, loss = 18.64 (3.2 examples/sec; 5.050 sec/batch)
2018-05-03 02:57:41.661265: step 26960, loss = 19.17 (3.1 examples/sec; 5.092 sec/batch)
2018-05-03 02:58:31.820654: step 26970, loss = 18.88 (3.2 examples/sec; 5.069 sec/batch)
2018-05-03 02:59:20.945320: step 26980, loss = 19.49 (3.3 examples/sec; 4.896 sec/batch)
2018-05-03 03:00:09.904108: step 26990, loss = 18.93 (3.3 examples/sec; 4.844 sec/batch)
2018-05-03 03:00:58.961612: step 27000, loss = 19.18 (3.3 examples/sec; 4.857 sec/batch)
2018-05-03 03:01:51.238865: step 27010, loss = 19.27 (3.3 examples/sec; 4.800 sec/batch)
2018-05-03 03:02:37.932662: step 27020, loss = 19.39 (3.2 examples/sec; 5.056 sec/batch)
2018-05-03 03:03:27.419558: step 27030, loss = 18.77 (3.3 examples/sec; 4.871 sec/batch)
2018-05-03 03:04:16.503869: step 27040, loss = 20.45 (3.2 examples/sec; 5.053 sec/batch)
2018-05-03 03:05:05.949241: step 27050, loss = 19.67 (3.3 examples/sec; 4.874 sec/batch)
2018-05-03 03:05:55.321095: step 27060, loss = 20.00 (3.2 examples/sec; 5.024 sec/batch)
2018-05-03 03:06:44.605612: step 27070, loss = 19.22 (3.2 examples/sec; 4.980 sec/batch)
2018-05-03 03:07:33.788969: step 27080, loss = 18.82 (3.3 examples/sec; 4.862 sec/batch)
2018-05-03 03:08:23.036039: step 27090, loss = 19.06 (3.4 examples/sec; 4.746 sec/batch)
2018-05-03 03:09:12.846886: step 27100, loss = 19.12 (3.2 examples/sec; 4.929 sec/batch)
2018-05-03 03:10:05.466764: step 27110, loss = 19.35 (3.3 examples/sec; 4.856 sec/batch)
2018-05-03 03:10:54.650248: step 27120, loss = 19.25 (3.2 examples/sec; 5.039 sec/batch)
2018-05-03 03:11:44.430665: step 27130, loss = 19.30 (3.1 examples/sec; 5.118 sec/batch)
2018-05-03 03:12:31.601629: step 27140, loss = 18.50 (4.3 examples/sec; 3.686 sec/batch)
2018-05-03 03:13:19.909636: step 27150, loss = 19.54 (3.3 examples/sec; 4.907 sec/batch)
2018-05-03 03:14:09.816896: step 27160, loss = 19.75 (3.1 examples/sec; 5.093 sec/batch)
2018-05-03 03:14:58.957329: step 27170, loss = 18.73 (3.2 examples/sec; 4.925 sec/batch)
2018-05-03 03:15:47.884133: step 27180, loss = 18.32 (3.3 examples/sec; 4.899 sec/batch)
2018-05-03 03:16:38.019540: step 27190, loss = 19.14 (3.1 examples/sec; 5.238 sec/batch)
2018-05-03 03:17:27.459899: step 27200, loss = 18.75 (3.2 examples/sec; 5.053 sec/batch)
2018-05-03 03:18:20.387875: step 27210, loss = 19.21 (3.2 examples/sec; 4.929 sec/batch)
2018-05-03 03:19:09.904861: step 27220, loss = 19.02 (3.6 examples/sec; 4.450 sec/batch)
2018-05-03 03:20:00.118545: step 27230, loss = 19.96 (3.3 examples/sec; 4.904 sec/batch)
2018-05-03 03:20:49.594068: step 27240, loss = 19.77 (3.2 examples/sec; 5.035 sec/batch)
2018-05-03 03:21:38.901599: step 27250, loss = 19.20 (3.3 examples/sec; 4.857 sec/batch)
2018-05-03 03:22:27.371783: step 27260, loss = 20.39 (3.2 examples/sec; 5.036 sec/batch)
2018-05-03 03:23:14.021096: step 27270, loss = 19.41 (3.2 examples/sec; 5.001 sec/batch)
2018-05-03 03:24:03.608861: step 27280, loss = 19.19 (3.2 examples/sec; 4.995 sec/batch)
2018-05-03 03:24:52.723091: step 27290, loss = 19.19 (3.2 examples/sec; 5.009 sec/batch)
2018-05-03 03:25:41.675483: step 27300, loss = 19.01 (3.3 examples/sec; 4.817 sec/batch)
2018-05-03 03:26:34.814935: step 27310, loss = 20.08 (3.2 examples/sec; 4.939 sec/batch)
2018-05-03 03:27:23.832994: step 27320, loss = 18.97 (3.3 examples/sec; 4.871 sec/batch)
2018-05-03 03:28:12.974355: step 27330, loss = 19.36 (3.3 examples/sec; 4.859 sec/batch)
2018-05-03 03:29:01.847854: step 27340, loss = 19.08 (3.2 examples/sec; 4.963 sec/batch)
2018-05-03 03:29:50.940589: step 27350, loss = 18.35 (3.3 examples/sec; 4.864 sec/batch)
2018-05-03 03:30:40.037544: step 27360, loss = 18.82 (3.2 examples/sec; 5.025 sec/batch)
2018-05-03 03:31:28.709882: step 27370, loss = 18.20 (3.3 examples/sec; 4.801 sec/batch)
2018-05-03 03:32:18.084411: step 27380, loss = 19.22 (3.3 examples/sec; 4.845 sec/batch)
2018-05-03 03:33:04.528334: step 27390, loss = 18.98 (4.2 examples/sec; 3.837 sec/batch)
2018-05-03 03:33:53.400336: step 27400, loss = 19.48 (3.3 examples/sec; 4.882 sec/batch)
2018-05-03 03:34:46.451739: step 27410, loss = 19.69 (3.2 examples/sec; 4.999 sec/batch)
2018-05-03 03:35:35.860433: step 27420, loss = 18.61 (3.2 examples/sec; 4.958 sec/batch)
2018-05-03 03:36:25.260999: step 27430, loss = 18.64 (3.3 examples/sec; 4.897 sec/batch)
2018-05-03 03:37:14.410305: step 27440, loss = 19.93 (3.1 examples/sec; 5.128 sec/batch)
2018-05-03 03:38:03.146835: step 27450, loss = 19.64 (3.2 examples/sec; 4.957 sec/batch)
2018-05-03 03:38:52.303104: step 27460, loss = 18.72 (3.3 examples/sec; 4.846 sec/batch)
2018-05-03 03:39:42.038787: step 27470, loss = 19.46 (3.2 examples/sec; 4.935 sec/batch)
2018-05-03 03:40:31.463116: step 27480, loss = 19.82 (3.3 examples/sec; 4.850 sec/batch)
2018-05-03 03:41:19.897743: step 27490, loss = 18.68 (3.2 examples/sec; 4.925 sec/batch)
2018-05-03 03:42:08.904127: step 27500, loss = 18.93 (3.2 examples/sec; 5.026 sec/batch)
2018-05-03 03:43:01.262392: step 27510, loss = 18.77 (3.3 examples/sec; 4.815 sec/batch)
2018-05-03 03:43:47.313641: step 27520, loss = 19.39 (3.2 examples/sec; 5.065 sec/batch)
2018-05-03 03:44:36.482304: step 27530, loss = 19.42 (3.3 examples/sec; 4.861 sec/batch)
2018-05-03 03:45:25.615665: step 27540, loss = 18.97 (3.2 examples/sec; 4.951 sec/batch)
2018-05-03 03:46:14.923518: step 27550, loss = 19.14 (3.2 examples/sec; 5.000 sec/batch)
2018-05-03 03:47:04.314447: step 27560, loss = 19.01 (3.2 examples/sec; 4.945 sec/batch)
2018-05-03 03:47:53.772889: step 27570, loss = 19.42 (3.2 examples/sec; 4.936 sec/batch)
2018-05-03 03:48:42.848140: step 27580, loss = 19.34 (3.2 examples/sec; 4.949 sec/batch)
2018-05-03 03:49:31.750086: step 27590, loss = 20.03 (3.2 examples/sec; 4.944 sec/batch)
2018-05-03 03:50:21.247636: step 27600, loss = 19.41 (3.2 examples/sec; 5.020 sec/batch)
2018-05-03 03:51:14.189517: step 27610, loss = 19.85 (3.2 examples/sec; 4.943 sec/batch)
2018-05-03 03:52:03.723845: step 27620, loss = 19.25 (3.2 examples/sec; 4.960 sec/batch)
2018-05-03 03:52:52.402582: step 27630, loss = 18.70 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 03:53:39.265325: step 27640, loss = 18.97 (3.6 examples/sec; 4.440 sec/batch)
2018-05-03 03:54:28.773477: step 27650, loss = 18.69 (3.4 examples/sec; 4.763 sec/batch)
2018-05-03 03:55:17.694568: step 27660, loss = 19.50 (3.3 examples/sec; 4.811 sec/batch)
2018-05-03 03:56:07.147118: step 27670, loss = 18.51 (3.2 examples/sec; 5.034 sec/batch)
2018-05-03 03:56:55.748701: step 27680, loss = 19.01 (3.3 examples/sec; 4.871 sec/batch)
2018-05-03 03:57:44.504300: step 27690, loss = 18.95 (3.2 examples/sec; 4.963 sec/batch)
2018-05-03 03:58:33.850945: step 27700, loss = 18.79 (3.2 examples/sec; 4.984 sec/batch)
2018-05-03 03:59:26.644877: step 27710, loss = 19.38 (3.2 examples/sec; 4.941 sec/batch)
2018-05-03 04:00:16.140901: step 27720, loss = 19.31 (3.4 examples/sec; 4.738 sec/batch)
2018-05-03 04:01:05.241421: step 27730, loss = 18.90 (3.2 examples/sec; 4.998 sec/batch)
2018-05-03 04:01:54.512863: step 27740, loss = 18.96 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 04:02:43.670484: step 27750, loss = 19.05 (3.4 examples/sec; 4.713 sec/batch)
2018-05-03 04:03:32.724716: step 27760, loss = 18.85 (3.2 examples/sec; 5.013 sec/batch)
2018-05-03 04:04:18.793903: step 27770, loss = 19.34 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 04:05:08.567569: step 27780, loss = 19.12 (3.2 examples/sec; 4.953 sec/batch)
2018-05-03 04:05:58.522737: step 27790, loss = 18.81 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 04:06:47.552602: step 27800, loss = 18.86 (3.4 examples/sec; 4.734 sec/batch)
2018-05-03 04:07:40.233855: step 27810, loss = 18.88 (3.3 examples/sec; 4.849 sec/batch)
2018-05-03 04:08:29.588022: step 27820, loss = 19.04 (3.2 examples/sec; 5.002 sec/batch)
2018-05-03 04:09:18.353358: step 27830, loss = 18.90 (3.2 examples/sec; 4.965 sec/batch)
2018-05-03 04:10:07.310646: step 27840, loss = 19.51 (3.3 examples/sec; 4.901 sec/batch)
2018-05-03 04:10:56.527504: step 27850, loss = 18.79 (3.2 examples/sec; 4.939 sec/batch)
2018-05-03 04:11:45.217022: step 27860, loss = 19.64 (3.4 examples/sec; 4.708 sec/batch)
2018-05-03 04:12:34.031557: step 27870, loss = 18.49 (3.3 examples/sec; 4.910 sec/batch)
2018-05-03 04:13:23.377112: step 27880, loss = 18.92 (3.2 examples/sec; 5.054 sec/batch)
2018-05-03 04:14:11.529635: step 27890, loss = 19.39 (4.2 examples/sec; 3.829 sec/batch)
2018-05-03 04:14:59.541981: step 27900, loss = 18.60 (3.2 examples/sec; 4.960 sec/batch)
2018-05-03 04:15:52.050304: step 27910, loss = 18.81 (3.2 examples/sec; 5.026 sec/batch)
2018-05-03 04:16:41.593135: step 27920, loss = 19.29 (3.2 examples/sec; 4.983 sec/batch)
2018-05-03 04:17:30.943313: step 27930, loss = 18.65 (3.3 examples/sec; 4.899 sec/batch)
2018-05-03 04:18:20.171112: step 27940, loss = 19.67 (3.3 examples/sec; 4.841 sec/batch)
2018-05-03 04:19:09.264722: step 27950, loss = 19.21 (3.2 examples/sec; 4.967 sec/batch)
2018-05-03 04:19:57.940220: step 27960, loss = 19.20 (3.2 examples/sec; 4.965 sec/batch)
2018-05-03 04:20:47.598240: step 27970, loss = 18.92 (3.2 examples/sec; 5.013 sec/batch)
2018-05-03 04:21:37.418556: step 27980, loss = 19.61 (3.1 examples/sec; 5.127 sec/batch)
2018-05-03 04:22:26.463277: step 27990, loss = 19.61 (3.3 examples/sec; 4.904 sec/batch)
2018-05-03 04:23:15.467831: step 28000, loss = 19.52 (3.2 examples/sec; 5.027 sec/batch)
2018-05-03 04:24:07.318901: step 28010, loss = 19.50 (3.5 examples/sec; 4.625 sec/batch)
2018-05-03 04:24:53.177939: step 28020, loss = 18.28 (3.4 examples/sec; 4.667 sec/batch)
2018-05-03 04:25:42.265355: step 28030, loss = 18.53 (3.1 examples/sec; 5.150 sec/batch)
2018-05-03 04:26:31.623456: step 28040, loss = 18.82 (3.2 examples/sec; 5.061 sec/batch)
2018-05-03 04:27:21.176140: step 28050, loss = 18.89 (3.3 examples/sec; 4.829 sec/batch)
2018-05-03 04:28:10.270314: step 28060, loss = 18.76 (3.3 examples/sec; 4.861 sec/batch)
2018-05-03 04:28:59.678354: step 28070, loss = 19.37 (3.3 examples/sec; 4.821 sec/batch)
2018-05-03 04:29:49.144780: step 28080, loss = 19.16 (3.2 examples/sec; 4.971 sec/batch)
2018-05-03 04:30:38.301406: step 28090, loss = 19.05 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 04:31:27.728637: step 28100, loss = 18.48 (3.3 examples/sec; 4.797 sec/batch)
2018-05-03 04:32:21.109212: step 28110, loss = 18.67 (3.3 examples/sec; 4.893 sec/batch)
2018-05-03 04:33:11.176047: step 28120, loss = 19.87 (3.3 examples/sec; 4.864 sec/batch)
2018-05-03 04:34:00.792284: step 28130, loss = 18.87 (3.3 examples/sec; 4.916 sec/batch)
2018-05-03 04:34:47.730787: step 28140, loss = 19.62 (4.2 examples/sec; 3.772 sec/batch)
2018-05-03 04:35:39.871671: step 28150, loss = 19.54 (2.9 examples/sec; 5.560 sec/batch)
2018-05-03 04:36:28.960083: step 28160, loss = 19.37 (3.2 examples/sec; 4.975 sec/batch)
2018-05-03 04:37:18.443511: step 28170, loss = 18.09 (3.2 examples/sec; 5.026 sec/batch)
2018-05-03 04:38:08.327659: step 28180, loss = 19.11 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 04:38:57.444939: step 28190, loss = 19.57 (3.3 examples/sec; 4.875 sec/batch)
2018-05-03 04:39:46.864655: step 28200, loss = 19.88 (3.2 examples/sec; 4.951 sec/batch)
2018-05-03 04:40:39.408083: step 28210, loss = 19.90 (3.1 examples/sec; 5.137 sec/batch)
2018-05-03 04:41:29.064446: step 28220, loss = 18.61 (3.2 examples/sec; 5.044 sec/batch)
2018-05-03 04:42:18.249840: step 28230, loss = 19.18 (3.2 examples/sec; 5.007 sec/batch)
2018-05-03 04:43:07.773794: step 28240, loss = 18.62 (3.1 examples/sec; 5.089 sec/batch)
2018-05-03 04:43:57.289943: step 28250, loss = 18.80 (3.2 examples/sec; 5.055 sec/batch)
2018-05-03 04:44:46.997562: step 28260, loss = 18.80 (3.3 examples/sec; 4.847 sec/batch)
2018-05-03 04:45:33.009968: step 28270, loss = 19.20 (3.4 examples/sec; 4.654 sec/batch)
2018-05-03 04:46:22.142040: step 28280, loss = 18.49 (3.3 examples/sec; 4.894 sec/batch)
2018-05-03 04:47:12.203956: step 28290, loss = 18.69 (3.3 examples/sec; 4.803 sec/batch)
2018-05-03 04:48:01.645819: step 28300, loss = 18.99 (3.4 examples/sec; 4.769 sec/batch)
2018-05-03 04:48:54.255065: step 28310, loss = 19.44 (3.2 examples/sec; 4.935 sec/batch)
2018-05-03 04:49:43.664075: step 28320, loss = 18.94 (3.2 examples/sec; 4.952 sec/batch)
2018-05-03 04:50:33.187287: step 28330, loss = 18.75 (3.3 examples/sec; 4.912 sec/batch)
2018-05-03 04:51:22.731373: step 28340, loss = 18.47 (3.3 examples/sec; 4.863 sec/batch)
2018-05-03 04:52:11.472094: step 28350, loss = 19.26 (3.3 examples/sec; 4.777 sec/batch)
2018-05-03 04:53:00.261401: step 28360, loss = 18.95 (3.3 examples/sec; 4.778 sec/batch)
2018-05-03 04:53:49.996626: step 28370, loss = 18.28 (3.2 examples/sec; 5.008 sec/batch)
2018-05-03 04:54:39.033371: step 28380, loss = 19.94 (3.4 examples/sec; 4.764 sec/batch)
2018-05-03 04:55:24.859112: step 28390, loss = 18.61 (4.2 examples/sec; 3.781 sec/batch)
2018-05-03 04:56:14.115597: step 28400, loss = 18.38 (3.2 examples/sec; 5.015 sec/batch)
2018-05-03 04:57:06.713647: step 28410, loss = 19.03 (3.2 examples/sec; 4.927 sec/batch)
2018-05-03 04:57:55.512011: step 28420, loss = 18.87 (3.3 examples/sec; 4.815 sec/batch)
2018-05-03 04:58:44.454285: step 28430, loss = 19.66 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 04:59:33.474623: step 28440, loss = 19.66 (3.2 examples/sec; 5.009 sec/batch)
2018-05-03 05:00:22.791498: step 28450, loss = 18.44 (3.3 examples/sec; 4.917 sec/batch)
2018-05-03 05:01:11.905581: step 28460, loss = 18.36 (3.5 examples/sec; 4.573 sec/batch)
2018-05-03 05:02:01.372076: step 28470, loss = 19.16 (3.3 examples/sec; 4.867 sec/batch)
2018-05-03 05:02:50.983995: step 28480, loss = 19.35 (3.1 examples/sec; 5.102 sec/batch)
2018-05-03 05:03:40.047655: step 28490, loss = 19.72 (3.3 examples/sec; 4.806 sec/batch)
2018-05-03 05:04:29.004183: step 28500, loss = 18.51 (3.2 examples/sec; 4.956 sec/batch)
2018-05-03 05:05:22.250311: step 28510, loss = 19.72 (3.2 examples/sec; 4.990 sec/batch)
2018-05-03 05:06:09.626424: step 28520, loss = 19.10 (3.3 examples/sec; 4.911 sec/batch)
2018-05-03 05:06:58.520215: step 28530, loss = 18.58 (3.3 examples/sec; 4.825 sec/batch)
2018-05-03 05:07:48.198656: step 28540, loss = 18.54 (3.3 examples/sec; 4.886 sec/batch)
2018-05-03 05:08:37.363020: step 28550, loss = 18.94 (3.3 examples/sec; 4.794 sec/batch)
2018-05-03 05:09:26.198253: step 28560, loss = 18.62 (3.2 examples/sec; 4.972 sec/batch)
2018-05-03 05:10:15.636338: step 28570, loss = 18.90 (3.3 examples/sec; 4.832 sec/batch)
2018-05-03 05:11:05.079730: step 28580, loss = 18.48 (3.3 examples/sec; 4.906 sec/batch)
2018-05-03 05:11:53.905586: step 28590, loss = 19.09 (3.3 examples/sec; 4.810 sec/batch)
2018-05-03 05:12:43.213065: step 28600, loss = 19.00 (3.3 examples/sec; 4.824 sec/batch)
2018-05-03 05:13:36.188361: step 28610, loss = 19.14 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 05:14:25.863259: step 28620, loss = 18.37 (3.3 examples/sec; 4.780 sec/batch)
2018-05-03 05:15:15.278316: step 28630, loss = 18.88 (3.1 examples/sec; 5.096 sec/batch)
2018-05-03 05:16:01.775515: step 28640, loss = 19.26 (3.4 examples/sec; 4.638 sec/batch)
2018-05-03 05:16:51.383203: step 28650, loss = 18.42 (3.2 examples/sec; 4.987 sec/batch)
2018-05-03 05:17:40.930919: step 28660, loss = 19.12 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 05:18:30.604273: step 28670, loss = 18.48 (3.3 examples/sec; 4.916 sec/batch)
2018-05-03 05:19:19.875775: step 28680, loss = 19.22 (3.2 examples/sec; 4.924 sec/batch)
2018-05-03 05:20:09.210824: step 28690, loss = 19.26 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 05:20:58.504520: step 28700, loss = 18.48 (3.2 examples/sec; 4.945 sec/batch)
2018-05-03 05:21:51.510892: step 28710, loss = 18.54 (3.3 examples/sec; 4.846 sec/batch)
2018-05-03 05:22:40.345780: step 28720, loss = 19.87 (3.3 examples/sec; 4.784 sec/batch)
2018-05-03 05:23:29.328081: step 28730, loss = 18.64 (3.3 examples/sec; 4.877 sec/batch)
2018-05-03 05:24:18.683098: step 28740, loss = 18.87 (3.3 examples/sec; 4.826 sec/batch)
2018-05-03 05:25:08.288663: step 28750, loss = 18.39 (3.3 examples/sec; 4.871 sec/batch)
2018-05-03 05:25:57.337172: step 28760, loss = 18.99 (3.3 examples/sec; 4.915 sec/batch)
2018-05-03 05:26:43.742028: step 28770, loss = 19.19 (3.2 examples/sec; 5.001 sec/batch)
2018-05-03 05:27:32.803741: step 28780, loss = 18.57 (3.2 examples/sec; 4.949 sec/batch)
2018-05-03 05:28:22.602984: step 28790, loss = 18.69 (3.3 examples/sec; 4.905 sec/batch)
2018-05-03 05:29:11.601410: step 28800, loss = 18.37 (3.4 examples/sec; 4.768 sec/batch)
2018-05-03 05:30:04.227732: step 28810, loss = 19.03 (3.3 examples/sec; 4.883 sec/batch)
2018-05-03 05:30:53.299217: step 28820, loss = 19.11 (3.3 examples/sec; 4.779 sec/batch)
2018-05-03 05:31:42.372933: step 28830, loss = 18.79 (3.3 examples/sec; 4.818 sec/batch)
2018-05-03 05:32:31.361842: step 28840, loss = 18.88 (3.2 examples/sec; 5.040 sec/batch)
2018-05-03 05:33:20.880570: step 28850, loss = 18.13 (3.3 examples/sec; 4.844 sec/batch)
2018-05-03 05:34:09.603654: step 28860, loss = 19.10 (3.3 examples/sec; 4.809 sec/batch)
2018-05-03 05:34:58.844733: step 28870, loss = 18.67 (3.2 examples/sec; 4.974 sec/batch)
2018-05-03 05:35:47.991744: step 28880, loss = 18.61 (3.2 examples/sec; 4.960 sec/batch)
2018-05-03 05:36:34.182207: step 28890, loss = 18.49 (3.6 examples/sec; 4.481 sec/batch)
2018-05-03 05:37:23.654996: step 28900, loss = 18.51 (3.2 examples/sec; 4.947 sec/batch)
2018-05-03 05:38:16.330686: step 28910, loss = 19.17 (3.1 examples/sec; 5.113 sec/batch)
2018-05-03 05:39:05.382121: step 28920, loss = 19.13 (3.3 examples/sec; 4.791 sec/batch)
2018-05-03 05:39:54.925102: step 28930, loss = 18.43 (3.2 examples/sec; 5.063 sec/batch)
2018-05-03 05:40:44.076957: step 28940, loss = 19.27 (3.2 examples/sec; 4.991 sec/batch)
2018-05-03 05:41:33.205641: step 28950, loss = 19.12 (3.3 examples/sec; 4.842 sec/batch)
2018-05-03 05:42:22.191202: step 28960, loss = 18.83 (3.3 examples/sec; 4.896 sec/batch)
2018-05-03 05:43:11.173702: step 28970, loss = 19.24 (3.4 examples/sec; 4.672 sec/batch)
2018-05-03 05:44:01.085010: step 28980, loss = 19.48 (3.3 examples/sec; 4.816 sec/batch)
2018-05-03 05:44:50.301992: step 28990, loss = 18.99 (3.3 examples/sec; 4.857 sec/batch)
2018-05-03 05:45:39.813592: step 29000, loss = 18.28 (3.2 examples/sec; 4.996 sec/batch)
2018-05-03 05:46:32.892110: step 29010, loss = 18.97 (3.2 examples/sec; 5.058 sec/batch)
2018-05-03 05:47:19.796949: step 29020, loss = 19.82 (3.3 examples/sec; 4.789 sec/batch)
2018-05-03 05:48:09.088497: step 29030, loss = 19.04 (3.2 examples/sec; 4.964 sec/batch)
2018-05-03 05:48:58.449650: step 29040, loss = 19.27 (3.2 examples/sec; 4.988 sec/batch)
2018-05-03 05:49:47.532327: step 29050, loss = 18.81 (3.2 examples/sec; 4.956 sec/batch)
2018-05-03 05:50:37.353271: step 29060, loss = 18.63 (3.2 examples/sec; 4.952 sec/batch)
2018-05-03 05:51:26.774427: step 29070, loss = 18.50 (3.2 examples/sec; 4.960 sec/batch)
2018-05-03 05:52:15.975786: step 29080, loss = 18.98 (3.3 examples/sec; 4.862 sec/batch)
2018-05-03 05:53:05.376404: step 29090, loss = 18.96 (3.2 examples/sec; 4.953 sec/batch)
2018-05-03 05:53:54.057396: step 29100, loss = 18.90 (3.3 examples/sec; 4.859 sec/batch)
2018-05-03 05:54:46.778498: step 29110, loss = 19.27 (3.3 examples/sec; 4.834 sec/batch)
2018-05-03 05:55:36.041895: step 29120, loss = 18.42 (3.4 examples/sec; 4.720 sec/batch)
2018-05-03 05:56:25.165986: step 29130, loss = 19.08 (3.3 examples/sec; 4.880 sec/batch)
2018-05-03 05:57:11.966493: step 29140, loss = 19.40 (3.5 examples/sec; 4.542 sec/batch)
2018-05-03 05:58:01.544481: step 29150, loss = 18.85 (3.2 examples/sec; 5.041 sec/batch)
2018-05-03 05:58:50.699846: step 29160, loss = 19.24 (3.2 examples/sec; 5.035 sec/batch)
2018-05-03 05:59:40.492305: step 29170, loss = 19.17 (3.2 examples/sec; 4.951 sec/batch)
2018-05-03 06:00:29.872513: step 29180, loss = 18.67 (3.2 examples/sec; 4.953 sec/batch)
2018-05-03 06:01:19.118708: step 29190, loss = 18.59 (3.2 examples/sec; 5.064 sec/batch)
2018-05-03 06:02:08.385037: step 29200, loss = 18.97 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 06:03:01.435348: step 29210, loss = 19.39 (3.2 examples/sec; 4.995 sec/batch)
2018-05-03 06:03:51.035685: step 29220, loss = 19.50 (3.3 examples/sec; 4.882 sec/batch)
2018-05-03 06:04:41.237142: step 29230, loss = 18.65 (3.3 examples/sec; 4.923 sec/batch)
2018-05-03 06:05:30.314177: step 29240, loss = 19.25 (3.3 examples/sec; 4.868 sec/batch)
2018-05-03 06:06:19.485460: step 29250, loss = 18.51 (3.3 examples/sec; 4.821 sec/batch)
2018-05-03 06:07:09.233903: step 29260, loss = 19.15 (3.3 examples/sec; 4.868 sec/batch)
2018-05-03 06:07:55.957975: step 29270, loss = 18.51 (3.2 examples/sec; 4.926 sec/batch)
2018-05-03 06:08:45.567942: step 29280, loss = 18.97 (3.2 examples/sec; 5.068 sec/batch)
2018-05-03 06:09:35.073949: step 29290, loss = 18.74 (3.3 examples/sec; 4.799 sec/batch)
2018-05-03 06:10:24.020098: step 29300, loss = 18.56 (3.2 examples/sec; 4.979 sec/batch)
2018-05-03 06:11:16.501754: step 29310, loss = 19.57 (3.3 examples/sec; 4.905 sec/batch)
2018-05-03 06:12:06.343760: step 29320, loss = 19.00 (3.2 examples/sec; 4.981 sec/batch)
2018-05-03 06:12:55.741632: step 29330, loss = 18.68 (3.2 examples/sec; 4.950 sec/batch)
2018-05-03 06:13:44.904298: step 29340, loss = 18.93 (3.2 examples/sec; 4.951 sec/batch)
2018-05-03 06:14:34.216253: step 29350, loss = 18.91 (3.3 examples/sec; 4.867 sec/batch)
2018-05-03 06:15:23.539326: step 29360, loss = 19.34 (3.3 examples/sec; 4.919 sec/batch)
2018-05-03 06:16:13.216790: step 29370, loss = 19.11 (3.2 examples/sec; 4.958 sec/batch)
2018-05-03 06:17:02.861733: step 29380, loss = 18.83 (3.2 examples/sec; 4.966 sec/batch)
2018-05-03 06:17:49.103773: step 29390, loss = 18.13 (3.2 examples/sec; 4.995 sec/batch)
2018-05-03 06:18:38.369441: step 29400, loss = 18.68 (3.3 examples/sec; 4.916 sec/batch)
2018-05-03 06:19:30.924718: step 29410, loss = 19.11 (3.3 examples/sec; 4.803 sec/batch)
2018-05-03 06:20:20.127696: step 29420, loss = 19.12 (3.2 examples/sec; 5.065 sec/batch)
2018-05-03 06:21:09.197614: step 29430, loss = 18.46 (3.3 examples/sec; 4.906 sec/batch)
2018-05-03 06:21:58.936088: step 29440, loss = 19.32 (3.2 examples/sec; 5.017 sec/batch)
2018-05-03 06:22:48.606183: step 29450, loss = 18.43 (3.0 examples/sec; 5.252 sec/batch)
2018-05-03 06:23:38.035481: step 29460, loss = 18.27 (3.2 examples/sec; 4.981 sec/batch)
2018-05-03 06:24:27.338290: step 29470, loss = 18.79 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 06:25:16.811039: step 29480, loss = 18.63 (3.1 examples/sec; 5.131 sec/batch)
2018-05-03 06:26:05.661616: step 29490, loss = 18.72 (3.2 examples/sec; 4.951 sec/batch)
2018-05-03 06:26:55.939717: step 29500, loss = 19.21 (3.1 examples/sec; 5.138 sec/batch)
2018-05-03 06:27:49.462758: step 29510, loss = 19.02 (3.2 examples/sec; 5.027 sec/batch)
2018-05-03 06:28:36.417618: step 29520, loss = 19.53 (3.2 examples/sec; 5.060 sec/batch)
2018-05-03 06:29:26.068780: step 29530, loss = 18.77 (3.3 examples/sec; 4.835 sec/batch)
2018-05-03 06:30:17.256457: step 29540, loss = 19.57 (3.1 examples/sec; 5.100 sec/batch)
2018-05-03 06:31:07.741945: step 29550, loss = 18.43 (3.2 examples/sec; 5.036 sec/batch)
2018-05-03 06:31:57.089053: step 29560, loss = 19.07 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 06:32:47.330271: step 29570, loss = 19.15 (3.2 examples/sec; 5.057 sec/batch)
2018-05-03 06:33:36.922243: step 29580, loss = 18.44 (3.3 examples/sec; 4.828 sec/batch)
2018-05-03 06:34:26.380712: step 29590, loss = 19.06 (3.3 examples/sec; 4.859 sec/batch)
2018-05-03 06:35:16.261916: step 29600, loss = 18.44 (3.0 examples/sec; 5.297 sec/batch)
2018-05-03 06:36:09.215171: step 29610, loss = 18.80 (3.3 examples/sec; 4.837 sec/batch)
2018-05-03 06:36:59.473573: step 29620, loss = 19.58 (3.1 examples/sec; 5.107 sec/batch)
2018-05-03 06:37:49.309822: step 29630, loss = 18.71 (3.2 examples/sec; 4.953 sec/batch)
2018-05-03 06:38:35.925711: step 29640, loss = 18.69 (3.2 examples/sec; 5.001 sec/batch)
2018-05-03 06:39:25.504703: step 29650, loss = 18.78 (3.3 examples/sec; 4.817 sec/batch)
2018-05-03 06:40:14.781470: step 29660, loss = 19.28 (3.3 examples/sec; 4.853 sec/batch)
2018-05-03 06:41:04.396594: step 29670, loss = 19.26 (3.0 examples/sec; 5.253 sec/batch)
2018-05-03 06:41:55.621500: step 29680, loss = 19.13 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 06:42:45.755833: step 29690, loss = 18.88 (3.1 examples/sec; 5.151 sec/batch)
2018-05-03 06:43:35.340896: step 29700, loss = 19.31 (3.2 examples/sec; 5.003 sec/batch)
2018-05-03 06:44:28.627298: step 29710, loss = 18.94 (3.3 examples/sec; 4.916 sec/batch)
2018-05-03 06:45:18.480037: step 29720, loss = 18.62 (3.3 examples/sec; 4.852 sec/batch)
2018-05-03 06:46:08.544807: step 29730, loss = 19.19 (3.3 examples/sec; 4.919 sec/batch)
2018-05-03 06:46:57.720823: step 29740, loss = 19.37 (3.2 examples/sec; 5.023 sec/batch)
2018-05-03 06:47:47.819906: step 29750, loss = 19.70 (3.2 examples/sec; 5.011 sec/batch)
2018-05-03 06:48:34.849800: step 29760, loss = 19.58 (3.2 examples/sec; 4.950 sec/batch)
2018-05-03 06:49:24.443817: step 29770, loss = 19.16 (3.3 examples/sec; 4.840 sec/batch)
2018-05-03 06:50:13.253114: step 29780, loss = 18.88 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 06:51:02.430086: step 29790, loss = 18.54 (3.2 examples/sec; 4.940 sec/batch)
2018-05-03 06:51:51.460149: step 29800, loss = 19.25 (3.2 examples/sec; 4.969 sec/batch)
2018-05-03 06:52:44.420279: step 29810, loss = 18.79 (3.3 examples/sec; 4.880 sec/batch)
2018-05-03 06:53:34.145440: step 29820, loss = 18.80 (3.3 examples/sec; 4.914 sec/batch)
2018-05-03 06:54:23.648027: step 29830, loss = 18.82 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 06:55:13.512474: step 29840, loss = 18.86 (3.2 examples/sec; 5.038 sec/batch)
2018-05-03 06:56:02.834386: step 29850, loss = 18.76 (3.4 examples/sec; 4.771 sec/batch)
2018-05-03 06:56:52.471942: step 29860, loss = 18.81 (3.3 examples/sec; 4.858 sec/batch)
2018-05-03 06:57:41.990877: step 29870, loss = 18.22 (3.4 examples/sec; 4.773 sec/batch)
2018-05-03 06:58:31.724630: step 29880, loss = 19.67 (3.1 examples/sec; 5.201 sec/batch)
2018-05-03 06:59:17.391747: step 29890, loss = 18.68 (3.2 examples/sec; 4.943 sec/batch)
2018-05-03 07:00:06.550080: step 29900, loss = 18.82 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 07:00:59.917353: step 29910, loss = 18.83 (3.2 examples/sec; 4.956 sec/batch)
2018-05-03 07:01:49.220663: step 29920, loss = 18.66 (3.2 examples/sec; 4.965 sec/batch)
2018-05-03 07:02:38.180095: step 29930, loss = 18.29 (3.2 examples/sec; 4.982 sec/batch)
2018-05-03 07:03:27.458246: step 29940, loss = 18.40 (3.3 examples/sec; 4.799 sec/batch)
2018-05-03 07:04:17.254087: step 29950, loss = 18.99 (3.1 examples/sec; 5.129 sec/batch)
2018-05-03 07:05:07.236277: step 29960, loss = 18.68 (3.2 examples/sec; 4.967 sec/batch)
2018-05-03 07:05:56.481989: step 29970, loss = 19.20 (3.2 examples/sec; 4.923 sec/batch)
2018-05-03 07:06:45.257475: step 29980, loss = 19.33 (3.1 examples/sec; 5.085 sec/batch)
2018-05-03 07:07:34.077022: step 29990, loss = 18.39 (3.5 examples/sec; 4.618 sec/batch)
2018-05-03 07:08:23.527842: step 30000, loss = 18.21 (3.2 examples/sec; 4.948 sec/batch)
2018-05-03 07:09:13.712387: step 30010, loss = 18.37 (3.2 examples/sec; 4.993 sec/batch)
2018-05-03 07:10:03.208764: step 30020, loss = 19.99 (3.4 examples/sec; 4.746 sec/batch)
2018-05-03 07:10:53.085187: step 30030, loss = 18.59 (3.2 examples/sec; 5.063 sec/batch)
2018-05-03 07:11:42.724548: step 30040, loss = 18.45 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 07:12:31.866047: step 30050, loss = 18.35 (3.2 examples/sec; 4.991 sec/batch)
2018-05-03 07:13:21.772413: step 30060, loss = 19.33 (3.3 examples/sec; 4.886 sec/batch)
2018-05-03 07:14:11.570259: step 30070, loss = 19.54 (3.2 examples/sec; 5.007 sec/batch)
2018-05-03 07:15:01.204623: step 30080, loss = 18.62 (3.2 examples/sec; 5.075 sec/batch)
2018-05-03 07:15:51.226397: step 30090, loss = 19.31 (3.3 examples/sec; 4.887 sec/batch)
2018-05-03 07:16:40.765308: step 30100, loss = 18.38 (3.3 examples/sec; 4.849 sec/batch)
2018-05-03 07:17:34.068927: step 30110, loss = 18.81 (3.2 examples/sec; 5.041 sec/batch)
2018-05-03 07:18:24.034786: step 30120, loss = 19.00 (3.2 examples/sec; 4.994 sec/batch)
2018-05-03 07:19:12.807771: step 30130, loss = 18.77 (4.2 examples/sec; 3.805 sec/batch)
2018-05-03 07:20:00.491043: step 30140, loss = 18.96 (3.2 examples/sec; 4.942 sec/batch)
2018-05-03 07:20:50.512194: step 30150, loss = 19.20 (3.3 examples/sec; 4.917 sec/batch)
2018-05-03 07:21:40.308950: step 30160, loss = 18.55 (3.2 examples/sec; 5.002 sec/batch)
2018-05-03 07:22:30.365029: step 30170, loss = 18.43 (3.3 examples/sec; 4.903 sec/batch)
2018-05-03 07:23:20.365407: step 30180, loss = 18.80 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 07:24:09.460942: step 30190, loss = 18.52 (3.2 examples/sec; 4.965 sec/batch)
2018-05-03 07:24:59.340485: step 30200, loss = 18.59 (3.1 examples/sec; 5.089 sec/batch)
2018-05-03 07:25:51.723313: step 30210, loss = 18.58 (3.2 examples/sec; 4.990 sec/batch)
2018-05-03 07:26:41.188737: step 30220, loss = 18.61 (3.2 examples/sec; 4.959 sec/batch)
2018-05-03 07:27:31.412034: step 30230, loss = 19.40 (3.2 examples/sec; 5.072 sec/batch)
2018-05-03 07:28:21.434974: step 30240, loss = 18.82 (3.2 examples/sec; 4.995 sec/batch)
2018-05-03 07:29:11.145382: step 30250, loss = 19.43 (3.2 examples/sec; 4.935 sec/batch)
2018-05-03 07:29:57.507319: step 30260, loss = 19.55 (3.3 examples/sec; 4.910 sec/batch)
2018-05-03 07:30:47.317997: step 30270, loss = 18.96 (3.3 examples/sec; 4.849 sec/batch)
2018-05-03 07:31:36.442556: step 30280, loss = 19.17 (3.2 examples/sec; 5.003 sec/batch)
2018-05-03 07:32:25.969312: step 30290, loss = 18.80 (3.3 examples/sec; 4.889 sec/batch)
2018-05-03 07:33:15.303299: step 30300, loss = 18.52 (3.2 examples/sec; 4.963 sec/batch)
2018-05-03 07:34:08.514376: step 30310, loss = 19.76 (3.1 examples/sec; 5.150 sec/batch)
2018-05-03 07:34:57.348018: step 30320, loss = 18.44 (3.4 examples/sec; 4.771 sec/batch)
2018-05-03 07:35:47.385440: step 30330, loss = 18.70 (3.2 examples/sec; 4.999 sec/batch)
2018-05-03 07:36:37.591995: step 30340, loss = 18.82 (3.2 examples/sec; 5.073 sec/batch)
2018-05-03 07:37:26.485901: step 30350, loss = 19.71 (3.4 examples/sec; 4.718 sec/batch)
2018-05-03 07:38:16.462129: step 30360, loss = 18.42 (3.1 examples/sec; 5.129 sec/batch)
2018-05-03 07:39:06.071649: step 30370, loss = 18.32 (3.3 examples/sec; 4.827 sec/batch)
2018-05-03 07:39:52.734311: step 30380, loss = 18.81 (3.7 examples/sec; 4.296 sec/batch)
2018-05-03 07:40:42.566014: step 30390, loss = 18.24 (3.2 examples/sec; 4.936 sec/batch)
2018-05-03 07:41:32.187021: step 30400, loss = 18.64 (3.2 examples/sec; 4.939 sec/batch)
2018-05-03 07:42:25.589468: step 30410, loss = 18.97 (3.1 examples/sec; 5.161 sec/batch)
2018-05-03 07:43:15.167621: step 30420, loss = 18.41 (3.3 examples/sec; 4.829 sec/batch)
2018-05-03 07:44:04.571744: step 30430, loss = 18.83 (3.2 examples/sec; 5.062 sec/batch)
2018-05-03 07:44:53.867789: step 30440, loss = 19.22 (3.2 examples/sec; 4.977 sec/batch)
2018-05-03 07:45:43.449916: step 30450, loss = 18.77 (3.2 examples/sec; 5.022 sec/batch)
2018-05-03 07:46:33.060945: step 30460, loss = 18.63 (3.3 examples/sec; 4.920 sec/batch)
2018-05-03 07:47:22.251603: step 30470, loss = 18.58 (3.3 examples/sec; 4.869 sec/batch)
2018-05-03 07:48:11.832778: step 30480, loss = 18.54 (3.2 examples/sec; 4.964 sec/batch)
2018-05-03 07:49:01.330727: step 30490, loss = 19.04 (3.1 examples/sec; 5.091 sec/batch)
2018-05-03 07:49:50.807347: step 30500, loss = 18.81 (3.2 examples/sec; 5.039 sec/batch)
2018-05-03 07:50:41.848964: step 30510, loss = 18.37 (3.2 examples/sec; 5.023 sec/batch)
2018-05-03 07:51:31.400278: step 30520, loss = 18.33 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 07:52:21.286790: step 30530, loss = 19.57 (3.3 examples/sec; 4.914 sec/batch)
2018-05-03 07:53:10.952999: step 30540, loss = 18.59 (3.3 examples/sec; 4.856 sec/batch)
2018-05-03 07:54:01.009785: step 30550, loss = 18.93 (3.2 examples/sec; 5.025 sec/batch)
2018-05-03 07:54:50.492879: step 30560, loss = 18.57 (3.2 examples/sec; 4.928 sec/batch)
2018-05-03 07:55:40.353794: step 30570, loss = 18.40 (3.2 examples/sec; 5.015 sec/batch)
2018-05-03 07:56:29.837541: step 30580, loss = 18.66 (3.3 examples/sec; 4.844 sec/batch)
2018-05-03 07:57:19.792664: step 30590, loss = 19.13 (3.2 examples/sec; 5.023 sec/batch)
2018-05-03 07:58:09.790432: step 30600, loss = 18.77 (3.2 examples/sec; 4.991 sec/batch)
2018-05-03 07:59:03.014296: step 30610, loss = 18.69 (3.2 examples/sec; 4.952 sec/batch)
2018-05-03 07:59:52.029792: step 30620, loss = 18.72 (3.4 examples/sec; 4.737 sec/batch)
2018-05-03 08:00:38.488988: step 30630, loss = 18.47 (3.2 examples/sec; 5.044 sec/batch)
2018-05-03 08:01:27.900118: step 30640, loss = 19.02 (3.3 examples/sec; 4.863 sec/batch)
2018-05-03 08:02:17.018365: step 30650, loss = 19.34 (3.3 examples/sec; 4.864 sec/batch)
2018-05-03 08:03:06.590448: step 30660, loss = 18.56 (3.3 examples/sec; 4.913 sec/batch)
2018-05-03 08:03:55.970238: step 30670, loss = 19.57 (3.3 examples/sec; 4.810 sec/batch)
2018-05-03 08:04:45.117153: step 30680, loss = 18.63 (3.4 examples/sec; 4.762 sec/batch)
2018-05-03 08:05:34.619863: step 30690, loss = 19.85 (3.2 examples/sec; 4.968 sec/batch)
2018-05-03 08:06:24.159885: step 30700, loss = 18.89 (3.2 examples/sec; 4.979 sec/batch)
2018-05-03 08:07:17.280812: step 30710, loss = 18.53 (3.3 examples/sec; 4.865 sec/batch)
2018-05-03 08:08:07.080336: step 30720, loss = 18.45 (3.3 examples/sec; 4.851 sec/batch)
2018-05-03 08:08:56.953414: step 30730, loss = 18.98 (3.3 examples/sec; 4.916 sec/batch)
2018-05-03 08:09:46.116381: step 30740, loss = 19.34 (3.3 examples/sec; 4.872 sec/batch)
2018-05-03 08:10:32.934907: step 30750, loss = 20.09 (4.2 examples/sec; 3.783 sec/batch)
2018-05-03 08:11:22.299957: step 30760, loss = 18.79 (3.1 examples/sec; 5.080 sec/batch)
2018-05-03 08:12:11.779121: step 30770, loss = 18.45 (3.4 examples/sec; 4.765 sec/batch)
2018-05-03 08:13:01.228534: step 30780, loss = 18.86 (3.2 examples/sec; 4.947 sec/batch)
2018-05-03 08:13:50.601401: step 30790, loss = 18.43 (3.2 examples/sec; 5.022 sec/batch)
2018-05-03 08:14:40.000153: step 30800, loss = 18.96 (3.2 examples/sec; 4.948 sec/batch)
2018-05-03 08:15:32.723078: step 30810, loss = 18.28 (3.3 examples/sec; 4.901 sec/batch)
2018-05-03 08:16:22.367479: step 30820, loss = 18.91 (3.3 examples/sec; 4.873 sec/batch)
2018-05-03 08:17:11.673220: step 30830, loss = 18.97 (3.2 examples/sec; 4.936 sec/batch)
2018-05-03 08:18:01.706652: step 30840, loss = 18.84 (3.1 examples/sec; 5.092 sec/batch)
2018-05-03 08:18:51.334300: step 30850, loss = 18.68 (3.2 examples/sec; 4.994 sec/batch)
2018-05-03 08:19:40.542848: step 30860, loss = 19.38 (3.3 examples/sec; 4.827 sec/batch)
2018-05-03 08:20:29.823163: step 30870, loss = 19.08 (3.2 examples/sec; 5.007 sec/batch)
2018-05-03 08:21:16.841848: step 30880, loss = 18.47 (3.2 examples/sec; 5.015 sec/batch)
2018-05-03 08:22:06.781321: step 30890, loss = 18.52 (3.2 examples/sec; 4.996 sec/batch)
2018-05-03 08:22:55.426443: step 30900, loss = 18.56 (3.2 examples/sec; 4.948 sec/batch)
2018-05-03 08:23:48.508409: step 30910, loss = 18.97 (3.2 examples/sec; 4.948 sec/batch)
2018-05-03 08:24:38.049672: step 30920, loss = 18.81 (3.2 examples/sec; 5.031 sec/batch)
2018-05-03 08:25:28.136119: step 30930, loss = 18.52 (3.2 examples/sec; 4.999 sec/batch)
2018-05-03 08:26:17.871484: step 30940, loss = 19.05 (3.2 examples/sec; 5.012 sec/batch)
2018-05-03 08:27:06.695482: step 30950, loss = 18.32 (3.3 examples/sec; 4.794 sec/batch)
2018-05-03 08:27:55.665333: step 30960, loss = 18.50 (3.1 examples/sec; 5.096 sec/batch)
2018-05-03 08:28:44.990391: step 30970, loss = 18.83 (3.2 examples/sec; 5.060 sec/batch)
2018-05-03 08:29:33.677951: step 30980, loss = 19.06 (3.2 examples/sec; 5.028 sec/batch)
2018-05-03 08:30:22.649015: step 30990, loss = 18.32 (3.2 examples/sec; 5.017 sec/batch)
2018-05-03 08:31:08.980087: step 31000, loss = 18.92 (3.5 examples/sec; 4.612 sec/batch)
2018-05-03 08:32:02.009928: step 31010, loss = 18.62 (3.3 examples/sec; 4.887 sec/batch)
2018-05-03 08:32:51.686450: step 31020, loss = 18.68 (3.3 examples/sec; 4.805 sec/batch)
2018-05-03 08:33:41.261824: step 31030, loss = 18.44 (3.3 examples/sec; 4.915 sec/batch)
2018-05-03 08:34:31.674034: step 31040, loss = 18.18 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 08:35:21.106886: step 31050, loss = 19.56 (3.3 examples/sec; 4.868 sec/batch)
2018-05-03 08:36:10.267040: step 31060, loss = 18.49 (3.3 examples/sec; 4.921 sec/batch)
2018-05-03 08:36:59.432954: step 31070, loss = 19.40 (3.3 examples/sec; 4.813 sec/batch)
2018-05-03 08:37:49.315372: step 31080, loss = 18.49 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 08:38:38.641869: step 31090, loss = 18.77 (3.3 examples/sec; 4.894 sec/batch)
2018-05-03 08:39:27.635082: step 31100, loss = 19.24 (3.2 examples/sec; 4.979 sec/batch)
2018-05-03 08:40:20.428112: step 31110, loss = 18.63 (3.3 examples/sec; 4.919 sec/batch)
2018-05-03 08:41:10.282320: step 31120, loss = 17.98 (3.2 examples/sec; 4.930 sec/batch)
2018-05-03 08:41:56.668650: step 31130, loss = 18.09 (3.3 examples/sec; 4.843 sec/batch)
2018-05-03 08:42:46.397205: step 31140, loss = 18.99 (3.3 examples/sec; 4.917 sec/batch)
2018-05-03 08:43:35.183212: step 31150, loss = 18.79 (3.3 examples/sec; 4.886 sec/batch)
2018-05-03 08:44:24.420949: step 31160, loss = 18.87 (3.2 examples/sec; 4.957 sec/batch)
2018-05-03 08:45:13.534989: step 31170, loss = 18.44 (3.3 examples/sec; 4.810 sec/batch)
2018-05-03 08:46:02.867551: step 31180, loss = 19.80 (3.3 examples/sec; 4.877 sec/batch)
2018-05-03 08:46:51.352058: step 31190, loss = 18.48 (3.3 examples/sec; 4.787 sec/batch)
2018-05-03 08:47:40.937288: step 31200, loss = 18.67 (3.4 examples/sec; 4.718 sec/batch)
2018-05-03 08:48:33.820006: step 31210, loss = 18.50 (3.3 examples/sec; 4.796 sec/batch)
2018-05-03 08:49:23.526435: step 31220, loss = 18.42 (3.3 examples/sec; 4.827 sec/batch)
2018-05-03 08:50:13.058820: step 31230, loss = 18.58 (3.3 examples/sec; 4.894 sec/batch)
2018-05-03 08:51:02.081710: step 31240, loss = 18.75 (3.3 examples/sec; 4.842 sec/batch)
2018-05-03 08:51:48.356976: step 31250, loss = 18.26 (3.3 examples/sec; 4.868 sec/batch)
2018-05-03 08:52:38.235722: step 31260, loss = 18.44 (3.2 examples/sec; 4.966 sec/batch)
2018-05-03 08:53:27.368583: step 31270, loss = 18.51 (3.3 examples/sec; 4.897 sec/batch)
2018-05-03 08:54:16.400376: step 31280, loss = 18.50 (3.3 examples/sec; 4.894 sec/batch)
2018-05-03 08:55:05.614391: step 31290, loss = 18.76 (3.2 examples/sec; 4.997 sec/batch)
2018-05-03 08:55:55.506481: step 31300, loss = 18.79 (3.3 examples/sec; 4.866 sec/batch)
2018-05-03 08:56:48.492179: step 31310, loss = 18.57 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 08:57:38.195383: step 31320, loss = 18.37 (3.2 examples/sec; 4.986 sec/batch)
2018-05-03 08:58:27.351313: step 31330, loss = 18.36 (3.3 examples/sec; 4.919 sec/batch)
2018-05-03 08:59:16.492055: step 31340, loss = 18.89 (3.3 examples/sec; 4.893 sec/batch)
2018-05-03 09:00:05.305295: step 31350, loss = 19.09 (3.3 examples/sec; 4.799 sec/batch)
2018-05-03 09:00:54.557120: step 31360, loss = 19.25 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 09:01:44.513299: step 31370, loss = 18.73 (3.3 examples/sec; 4.792 sec/batch)
2018-05-03 09:02:31.373993: step 31380, loss = 18.29 (3.2 examples/sec; 4.926 sec/batch)
2018-05-03 09:03:21.440437: step 31390, loss = 19.59 (3.2 examples/sec; 4.972 sec/batch)
2018-05-03 09:04:10.733731: step 31400, loss = 19.11 (3.2 examples/sec; 5.016 sec/batch)
2018-05-03 09:05:03.350654: step 31410, loss = 18.47 (3.3 examples/sec; 4.832 sec/batch)
2018-05-03 09:05:52.783648: step 31420, loss = 19.24 (3.2 examples/sec; 5.012 sec/batch)
2018-05-03 09:06:42.280200: step 31430, loss = 18.85 (3.2 examples/sec; 4.981 sec/batch)
2018-05-03 09:07:31.515950: step 31440, loss = 18.59 (3.2 examples/sec; 5.053 sec/batch)
2018-05-03 09:08:20.250549: step 31450, loss = 18.16 (3.3 examples/sec; 4.921 sec/batch)
2018-05-03 09:09:09.469892: step 31460, loss = 18.91 (3.3 examples/sec; 4.821 sec/batch)
2018-05-03 09:09:58.700765: step 31470, loss = 18.43 (3.2 examples/sec; 4.994 sec/batch)
2018-05-03 09:10:48.651564: step 31480, loss = 19.03 (3.3 examples/sec; 4.780 sec/batch)
2018-05-03 09:11:38.018921: step 31490, loss = 18.80 (3.2 examples/sec; 4.997 sec/batch)
2018-05-03 09:12:23.615061: step 31500, loss = 18.45 (3.3 examples/sec; 4.900 sec/batch)
2018-05-03 09:13:16.511674: step 31510, loss = 18.49 (3.2 examples/sec; 4.991 sec/batch)
2018-05-03 09:14:06.209993: step 31520, loss = 18.46 (3.3 examples/sec; 4.913 sec/batch)
2018-05-03 09:14:55.768525: step 31530, loss = 18.85 (3.1 examples/sec; 5.131 sec/batch)
2018-05-03 09:15:45.227507: step 31540, loss = 18.27 (3.3 examples/sec; 4.862 sec/batch)
2018-05-03 09:16:34.740160: step 31550, loss = 18.31 (3.2 examples/sec; 5.058 sec/batch)
2018-05-03 09:17:23.527300: step 31560, loss = 19.04 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 09:18:12.430222: step 31570, loss = 18.61 (3.3 examples/sec; 4.817 sec/batch)
2018-05-03 09:19:01.847288: step 31580, loss = 18.63 (3.2 examples/sec; 5.027 sec/batch)
2018-05-03 09:19:52.355416: step 31590, loss = 18.44 (3.2 examples/sec; 5.015 sec/batch)
2018-05-03 09:20:41.962912: step 31600, loss = 18.67 (3.2 examples/sec; 4.926 sec/batch)
2018-05-03 09:21:35.440976: step 31610, loss = 18.13 (3.1 examples/sec; 5.177 sec/batch)
2018-05-03 09:22:23.458967: step 31620, loss = 18.80 (4.1 examples/sec; 3.872 sec/batch)
2018-05-03 09:23:11.752716: step 31630, loss = 18.72 (3.2 examples/sec; 4.968 sec/batch)
2018-05-03 09:24:01.026149: step 31640, loss = 18.80 (3.3 examples/sec; 4.809 sec/batch)
2018-05-03 09:24:50.196771: step 31650, loss = 18.96 (3.3 examples/sec; 4.868 sec/batch)
2018-05-03 09:25:39.656997: step 31660, loss = 19.32 (3.3 examples/sec; 4.900 sec/batch)
2018-05-03 09:26:28.811491: step 31670, loss = 18.51 (3.2 examples/sec; 5.064 sec/batch)
2018-05-03 09:27:18.017674: step 31680, loss = 18.28 (3.3 examples/sec; 4.849 sec/batch)
2018-05-03 09:28:07.240800: step 31690, loss = 18.13 (3.2 examples/sec; 5.075 sec/batch)
2018-05-03 09:28:57.105829: step 31700, loss = 18.58 (3.2 examples/sec; 5.043 sec/batch)
2018-05-03 09:29:50.036874: step 31710, loss = 18.38 (3.3 examples/sec; 4.827 sec/batch)
2018-05-03 09:30:39.616532: step 31720, loss = 19.29 (3.3 examples/sec; 4.908 sec/batch)
2018-05-03 09:31:29.444631: step 31730, loss = 18.75 (3.2 examples/sec; 5.001 sec/batch)
2018-05-03 09:32:18.968132: step 31740, loss = 19.35 (3.2 examples/sec; 5.025 sec/batch)
2018-05-03 09:33:05.439819: step 31750, loss = 18.91 (3.3 examples/sec; 4.840 sec/batch)
2018-05-03 09:33:54.787797: step 31760, loss = 19.12 (3.2 examples/sec; 5.076 sec/batch)
2018-05-03 09:34:44.375382: step 31770, loss = 18.38 (3.3 examples/sec; 4.858 sec/batch)
2018-05-03 09:35:33.103344: step 31780, loss = 19.07 (3.3 examples/sec; 4.901 sec/batch)
2018-05-03 09:36:22.346847: step 31790, loss = 18.81 (3.2 examples/sec; 4.942 sec/batch)
2018-05-03 09:37:11.380729: step 31800, loss = 18.51 (3.2 examples/sec; 4.934 sec/batch)
2018-05-03 09:38:04.041460: step 31810, loss = 18.64 (3.2 examples/sec; 4.993 sec/batch)
2018-05-03 09:38:53.361521: step 31820, loss = 18.48 (3.3 examples/sec; 4.855 sec/batch)
2018-05-03 09:39:43.416526: step 31830, loss = 18.35 (3.2 examples/sec; 4.948 sec/batch)
2018-05-03 09:40:32.728391: step 31840, loss = 18.63 (3.2 examples/sec; 5.023 sec/batch)
2018-05-03 09:41:22.505182: step 31850, loss = 18.82 (3.1 examples/sec; 5.114 sec/batch)
2018-05-03 09:42:12.583157: step 31860, loss = 18.06 (3.2 examples/sec; 4.958 sec/batch)
2018-05-03 09:42:59.957414: step 31870, loss = 18.19 (4.2 examples/sec; 3.815 sec/batch)
2018-05-03 09:43:48.455165: step 31880, loss = 19.28 (3.2 examples/sec; 5.034 sec/batch)
2018-05-03 09:44:38.089661: step 31890, loss = 19.37 (3.2 examples/sec; 4.979 sec/batch)
2018-05-03 09:45:27.287985: step 31900, loss = 18.38 (3.3 examples/sec; 4.852 sec/batch)
2018-05-03 09:46:20.636346: step 31910, loss = 18.37 (3.2 examples/sec; 4.950 sec/batch)
2018-05-03 09:47:10.451997: step 31920, loss = 18.40 (3.3 examples/sec; 4.878 sec/batch)
2018-05-03 09:48:00.523571: step 31930, loss = 19.85 (3.2 examples/sec; 5.071 sec/batch)
2018-05-03 09:48:49.901120: step 31940, loss = 18.32 (3.3 examples/sec; 4.873 sec/batch)
2018-05-03 09:49:39.451352: step 31950, loss = 18.26 (3.3 examples/sec; 4.905 sec/batch)
2018-05-03 09:50:28.716543: step 31960, loss = 19.61 (3.2 examples/sec; 4.991 sec/batch)
2018-05-03 09:51:18.231212: step 31970, loss = 18.98 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 09:52:07.341188: step 31980, loss = 18.61 (3.2 examples/sec; 4.982 sec/batch)
2018-05-03 09:52:57.103150: step 31990, loss = 19.05 (3.1 examples/sec; 5.180 sec/batch)
2018-05-03 09:53:43.571941: step 32000, loss = 18.40 (3.3 examples/sec; 4.866 sec/batch)
2018-05-03 09:54:36.397788: step 32010, loss = 18.64 (3.2 examples/sec; 4.949 sec/batch)
2018-05-03 09:55:24.503567: step 32020, loss = 19.28 (3.4 examples/sec; 4.679 sec/batch)
2018-05-03 09:56:13.753809: step 32030, loss = 19.10 (3.3 examples/sec; 4.914 sec/batch)
2018-05-03 09:57:03.053693: step 32040, loss = 18.45 (3.3 examples/sec; 4.829 sec/batch)
2018-05-03 09:57:52.273101: step 32050, loss = 18.20 (3.3 examples/sec; 4.827 sec/batch)
2018-05-03 09:58:41.561928: step 32060, loss = 18.71 (3.2 examples/sec; 5.053 sec/batch)
2018-05-03 09:59:31.105145: step 32070, loss = 18.72 (3.4 examples/sec; 4.655 sec/batch)
2018-05-03 10:00:20.938117: step 32080, loss = 19.17 (3.2 examples/sec; 5.025 sec/batch)
2018-05-03 10:01:10.631012: step 32090, loss = 18.74 (3.2 examples/sec; 4.970 sec/batch)
2018-05-03 10:01:59.840987: step 32100, loss = 18.42 (3.2 examples/sec; 4.974 sec/batch)
2018-05-03 10:02:52.677944: step 32110, loss = 18.48 (3.3 examples/sec; 4.907 sec/batch)
2018-05-03 10:03:39.276386: step 32120, loss = 18.32 (3.3 examples/sec; 4.778 sec/batch)
2018-05-03 10:04:28.582634: step 32130, loss = 18.73 (3.2 examples/sec; 4.992 sec/batch)
2018-05-03 10:05:18.146138: step 32140, loss = 18.47 (3.3 examples/sec; 4.911 sec/batch)
2018-05-03 10:06:07.690317: step 32150, loss = 18.61 (3.3 examples/sec; 4.903 sec/batch)
2018-05-03 10:06:57.205305: step 32160, loss = 18.73 (3.2 examples/sec; 4.968 sec/batch)
2018-05-03 10:07:47.441010: step 32170, loss = 18.91 (3.2 examples/sec; 4.945 sec/batch)
2018-05-03 10:08:36.815397: step 32180, loss = 19.59 (3.2 examples/sec; 5.004 sec/batch)
2018-05-03 10:09:26.032049: step 32190, loss = 18.18 (3.3 examples/sec; 4.833 sec/batch)
2018-05-03 10:10:15.737950: step 32200, loss = 18.57 (3.2 examples/sec; 5.011 sec/batch)
2018-05-03 10:11:08.034090: step 32210, loss = 18.38 (3.3 examples/sec; 4.827 sec/batch)
2018-05-03 10:11:57.584062: step 32220, loss = 18.24 (3.2 examples/sec; 4.951 sec/batch)
2018-05-03 10:12:47.453518: step 32230, loss = 18.35 (3.2 examples/sec; 4.935 sec/batch)
2018-05-03 10:13:37.226648: step 32240, loss = 18.59 (3.2 examples/sec; 4.940 sec/batch)
2018-05-03 10:14:23.499559: step 32250, loss = 18.43 (3.3 examples/sec; 4.848 sec/batch)
2018-05-03 10:15:13.718744: step 32260, loss = 18.82 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 10:16:02.955680: step 32270, loss = 18.88 (3.3 examples/sec; 4.868 sec/batch)
2018-05-03 10:16:53.113005: step 32280, loss = 18.59 (3.2 examples/sec; 5.004 sec/batch)
2018-05-03 10:17:43.045031: step 32290, loss = 19.00 (3.2 examples/sec; 5.015 sec/batch)
2018-05-03 10:18:32.813600: step 32300, loss = 18.69 (3.2 examples/sec; 4.984 sec/batch)
2018-05-03 10:19:25.729897: step 32310, loss = 19.00 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 10:20:14.789695: step 32320, loss = 18.76 (3.3 examples/sec; 4.917 sec/batch)
2018-05-03 10:21:03.679050: step 32330, loss = 18.23 (3.3 examples/sec; 4.863 sec/batch)
2018-05-03 10:21:53.621956: step 32340, loss = 18.18 (3.2 examples/sec; 5.058 sec/batch)
2018-05-03 10:22:43.423999: step 32350, loss = 19.00 (3.3 examples/sec; 4.846 sec/batch)
2018-05-03 10:23:32.560142: step 32360, loss = 18.17 (3.3 examples/sec; 4.852 sec/batch)
2018-05-03 10:24:19.300695: step 32370, loss = 18.86 (3.3 examples/sec; 4.838 sec/batch)
2018-05-03 10:25:09.139112: step 32380, loss = 18.82 (3.2 examples/sec; 5.056 sec/batch)
2018-05-03 10:25:58.844329: step 32390, loss = 18.71 (3.2 examples/sec; 5.005 sec/batch)
2018-05-03 10:26:48.212077: step 32400, loss = 18.51 (3.2 examples/sec; 5.054 sec/batch)
2018-05-03 10:27:42.031945: step 32410, loss = 18.44 (3.1 examples/sec; 5.097 sec/batch)
2018-05-03 10:28:31.788058: step 32420, loss = 18.56 (3.3 examples/sec; 4.821 sec/batch)
2018-05-03 10:29:21.390019: step 32430, loss = 19.15 (3.2 examples/sec; 4.928 sec/batch)
2018-05-03 10:30:10.514996: step 32440, loss = 18.23 (3.1 examples/sec; 5.125 sec/batch)
2018-05-03 10:30:59.450115: step 32450, loss = 18.34 (3.2 examples/sec; 4.944 sec/batch)
2018-05-03 10:31:49.330222: step 32460, loss = 18.61 (3.2 examples/sec; 5.005 sec/batch)
2018-05-03 10:32:38.912274: step 32470, loss = 18.84 (3.2 examples/sec; 5.009 sec/batch)
2018-05-03 10:33:28.398516: step 32480, loss = 18.41 (3.4 examples/sec; 4.735 sec/batch)
2018-05-03 10:34:16.200033: step 32490, loss = 17.95 (4.2 examples/sec; 3.797 sec/batch)
2018-05-03 10:35:05.038747: step 32500, loss = 18.57 (3.1 examples/sec; 5.117 sec/batch)
2018-05-03 10:35:58.432746: step 32510, loss = 18.48 (3.2 examples/sec; 5.068 sec/batch)
2018-05-03 10:36:48.151414: step 32520, loss = 18.47 (3.3 examples/sec; 4.782 sec/batch)
2018-05-03 10:37:36.884388: step 32530, loss = 18.35 (3.2 examples/sec; 5.046 sec/batch)
2018-05-03 10:38:27.074675: step 32540, loss = 18.70 (3.1 examples/sec; 5.156 sec/batch)
2018-05-03 10:39:16.285903: step 32550, loss = 18.64 (3.2 examples/sec; 4.947 sec/batch)
2018-05-03 10:40:05.217345: step 32560, loss = 18.71 (3.3 examples/sec; 4.825 sec/batch)
2018-05-03 10:40:54.869077: step 32570, loss = 18.33 (3.3 examples/sec; 4.821 sec/batch)
2018-05-03 10:41:44.271350: step 32580, loss = 18.58 (3.2 examples/sec; 5.007 sec/batch)
2018-05-03 10:42:33.301836: step 32590, loss = 18.26 (3.3 examples/sec; 4.782 sec/batch)
2018-05-03 10:43:21.996150: step 32600, loss = 19.11 (3.3 examples/sec; 4.900 sec/batch)
2018-05-03 10:44:15.182809: step 32610, loss = 18.47 (3.2 examples/sec; 4.979 sec/batch)
2018-05-03 10:45:02.057705: step 32620, loss = 18.92 (3.2 examples/sec; 4.995 sec/batch)
2018-05-03 10:45:51.310283: step 32630, loss = 19.19 (3.4 examples/sec; 4.640 sec/batch)
2018-05-03 10:46:40.562314: step 32640, loss = 18.69 (3.2 examples/sec; 4.966 sec/batch)
2018-05-03 10:47:30.077113: step 32650, loss = 18.43 (3.2 examples/sec; 5.015 sec/batch)
2018-05-03 10:48:19.677919: step 32660, loss = 19.14 (3.2 examples/sec; 5.008 sec/batch)
2018-05-03 10:49:09.983522: step 32670, loss = 18.68 (3.2 examples/sec; 5.053 sec/batch)
2018-05-03 10:49:59.704998: step 32680, loss = 18.77 (3.2 examples/sec; 4.966 sec/batch)
2018-05-03 10:50:49.178396: step 32690, loss = 19.11 (3.3 examples/sec; 4.835 sec/batch)
2018-05-03 10:51:38.273125: step 32700, loss = 18.04 (3.2 examples/sec; 5.000 sec/batch)
2018-05-03 10:52:31.492985: step 32710, loss = 18.70 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 10:53:21.001239: step 32720, loss = 18.89 (3.3 examples/sec; 4.915 sec/batch)
2018-05-03 10:54:10.642578: step 32730, loss = 19.07 (3.2 examples/sec; 4.960 sec/batch)
2018-05-03 10:54:56.945741: step 32740, loss = 18.79 (3.7 examples/sec; 4.313 sec/batch)
2018-05-03 10:55:46.629900: step 32750, loss = 18.39 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 10:56:36.555669: step 32760, loss = 18.83 (3.1 examples/sec; 5.183 sec/batch)
2018-05-03 10:57:26.092770: step 32770, loss = 18.53 (3.4 examples/sec; 4.735 sec/batch)
2018-05-03 10:58:16.194871: step 32780, loss = 18.53 (3.1 examples/sec; 5.084 sec/batch)
2018-05-03 10:59:05.589812: step 32790, loss = 18.38 (3.3 examples/sec; 4.849 sec/batch)
2018-05-03 10:59:54.809693: step 32800, loss = 18.59 (3.1 examples/sec; 5.093 sec/batch)
2018-05-03 11:00:47.530399: step 32810, loss = 18.49 (3.3 examples/sec; 4.895 sec/batch)
2018-05-03 11:01:36.741460: step 32820, loss = 18.16 (3.1 examples/sec; 5.082 sec/batch)
2018-05-03 11:02:25.991005: step 32830, loss = 18.95 (3.4 examples/sec; 4.733 sec/batch)
2018-05-03 11:03:14.947917: step 32840, loss = 18.53 (3.2 examples/sec; 4.968 sec/batch)
2018-05-03 11:04:04.048895: step 32850, loss = 18.49 (3.2 examples/sec; 4.963 sec/batch)
2018-05-03 11:04:53.078183: step 32860, loss = 18.42 (3.3 examples/sec; 4.787 sec/batch)
2018-05-03 11:05:39.652136: step 32870, loss = 18.94 (3.3 examples/sec; 4.841 sec/batch)
2018-05-03 11:06:29.506873: step 32880, loss = 18.37 (3.2 examples/sec; 4.993 sec/batch)
2018-05-03 11:07:18.818755: step 32890, loss = 19.14 (3.2 examples/sec; 4.946 sec/batch)
2018-05-03 11:08:07.933907: step 32900, loss = 18.75 (3.4 examples/sec; 4.750 sec/batch)
2018-05-03 11:09:00.857653: step 32910, loss = 19.06 (3.1 examples/sec; 5.158 sec/batch)
2018-05-03 11:09:50.342827: step 32920, loss = 18.45 (3.3 examples/sec; 4.888 sec/batch)
2018-05-03 11:10:39.626730: step 32930, loss = 18.53 (3.2 examples/sec; 4.965 sec/batch)
2018-05-03 11:11:28.789155: step 32940, loss = 18.50 (3.2 examples/sec; 5.007 sec/batch)
2018-05-03 11:12:18.573265: step 32950, loss = 18.45 (3.2 examples/sec; 5.007 sec/batch)
2018-05-03 11:13:07.768731: step 32960, loss = 19.29 (3.1 examples/sec; 5.120 sec/batch)
2018-05-03 11:13:57.245652: step 32970, loss = 18.42 (3.2 examples/sec; 5.061 sec/batch)
2018-05-03 11:14:46.439509: step 32980, loss = 18.65 (3.2 examples/sec; 4.946 sec/batch)
2018-05-03 11:15:32.771235: step 32990, loss = 18.43 (3.5 examples/sec; 4.521 sec/batch)
2018-05-03 11:16:22.164053: step 33000, loss = 18.52 (3.3 examples/sec; 4.877 sec/batch)
2018-05-03 11:17:14.701818: step 33010, loss = 18.35 (3.4 examples/sec; 4.758 sec/batch)
2018-05-03 11:18:04.846415: step 33020, loss = 18.94 (3.3 examples/sec; 4.917 sec/batch)
2018-05-03 11:18:53.825072: step 33030, loss = 18.18 (3.2 examples/sec; 4.960 sec/batch)
2018-05-03 11:19:42.720469: step 33040, loss = 18.62 (3.3 examples/sec; 4.901 sec/batch)
2018-05-03 11:20:31.963543: step 33050, loss = 18.10 (3.1 examples/sec; 5.079 sec/batch)
2018-05-03 11:21:21.432417: step 33060, loss = 18.36 (3.1 examples/sec; 5.086 sec/batch)
2018-05-03 11:22:11.178541: step 33070, loss = 18.42 (3.2 examples/sec; 4.964 sec/batch)
2018-05-03 11:23:01.202503: step 33080, loss = 19.59 (3.1 examples/sec; 5.137 sec/batch)
2018-05-03 11:23:50.338350: step 33090, loss = 18.96 (3.2 examples/sec; 5.043 sec/batch)
2018-05-03 11:24:40.307972: step 33100, loss = 18.83 (3.1 examples/sec; 5.132 sec/batch)
2018-05-03 11:25:33.267194: step 33110, loss = 18.25 (3.2 examples/sec; 4.968 sec/batch)
2018-05-03 11:26:19.307372: step 33120, loss = 18.52 (3.3 examples/sec; 4.814 sec/batch)
2018-05-03 11:27:08.084287: step 33130, loss = 19.04 (3.3 examples/sec; 4.823 sec/batch)
2018-05-03 11:27:57.274517: step 33140, loss = 18.34 (3.2 examples/sec; 5.058 sec/batch)
2018-05-03 11:28:47.061180: step 33150, loss = 18.75 (3.1 examples/sec; 5.211 sec/batch)
2018-05-03 11:29:36.685404: step 33160, loss = 18.89 (3.3 examples/sec; 4.899 sec/batch)
2018-05-03 11:30:26.178986: step 33170, loss = 18.16 (3.3 examples/sec; 4.922 sec/batch)
2018-05-03 11:31:15.407053: step 33180, loss = 18.62 (3.3 examples/sec; 4.814 sec/batch)
2018-05-03 11:32:04.528404: step 33190, loss = 18.59 (3.3 examples/sec; 4.884 sec/batch)
2018-05-03 11:32:54.266218: step 33200, loss = 19.15 (3.2 examples/sec; 5.018 sec/batch)
2018-05-03 11:33:47.241379: step 33210, loss = 18.81 (3.2 examples/sec; 5.023 sec/batch)
2018-05-03 11:34:36.535044: step 33220, loss = 19.30 (3.2 examples/sec; 4.957 sec/batch)
2018-05-03 11:35:26.122425: step 33230, loss = 18.08 (3.1 examples/sec; 5.121 sec/batch)
2018-05-03 11:36:12.746114: step 33240, loss = 18.47 (3.3 examples/sec; 4.907 sec/batch)
2018-05-03 11:37:02.514341: step 33250, loss = 18.59 (3.3 examples/sec; 4.828 sec/batch)
2018-05-03 11:37:52.442394: step 33260, loss = 19.30 (3.2 examples/sec; 5.058 sec/batch)
2018-05-03 11:38:42.488021: step 33270, loss = 18.69 (3.3 examples/sec; 4.800 sec/batch)
2018-05-03 11:39:32.059182: step 33280, loss = 18.32 (3.1 examples/sec; 5.118 sec/batch)
2018-05-03 11:40:21.446956: step 33290, loss = 18.23 (3.1 examples/sec; 5.111 sec/batch)
2018-05-03 11:41:10.142382: step 33300, loss = 18.58 (3.3 examples/sec; 4.909 sec/batch)
2018-05-03 11:42:02.925714: step 33310, loss = 18.80 (3.2 examples/sec; 4.938 sec/batch)
2018-05-03 11:42:52.866283: step 33320, loss = 18.40 (3.3 examples/sec; 4.911 sec/batch)
2018-05-03 11:43:42.334715: step 33330, loss = 18.90 (3.2 examples/sec; 4.996 sec/batch)
2018-05-03 11:44:32.252050: step 33340, loss = 18.19 (3.2 examples/sec; 5.000 sec/batch)
2018-05-03 11:45:22.663524: step 33350, loss = 18.20 (3.1 examples/sec; 5.226 sec/batch)
2018-05-03 11:46:11.037259: step 33360, loss = 18.56 (4.0 examples/sec; 3.992 sec/batch)
2018-05-03 11:46:58.095612: step 33370, loss = 18.52 (3.2 examples/sec; 4.939 sec/batch)
2018-05-03 11:47:47.229356: step 33380, loss = 18.39 (3.2 examples/sec; 5.010 sec/batch)
2018-05-03 11:48:36.854791: step 33390, loss = 18.54 (3.2 examples/sec; 5.069 sec/batch)
2018-05-03 11:49:26.386197: step 33400, loss = 18.65 (3.3 examples/sec; 4.884 sec/batch)
2018-05-03 11:50:19.238603: step 33410, loss = 18.71 (3.2 examples/sec; 4.993 sec/batch)
2018-05-03 11:51:08.929868: step 33420, loss = 18.90 (3.1 examples/sec; 5.098 sec/batch)
2018-05-03 11:51:57.806663: step 33430, loss = 18.50 (3.3 examples/sec; 4.817 sec/batch)
2018-05-03 11:52:47.450985: step 33440, loss = 18.23 (3.2 examples/sec; 5.017 sec/batch)
2018-05-03 11:53:37.773849: step 33450, loss = 18.50 (3.2 examples/sec; 4.980 sec/batch)
2018-05-03 11:54:27.455625: step 33460, loss = 18.84 (3.1 examples/sec; 5.087 sec/batch)
2018-05-03 11:55:16.099197: step 33470, loss = 18.53 (3.2 examples/sec; 5.003 sec/batch)
2018-05-03 11:56:05.405958: step 33480, loss = 18.19 (3.1 examples/sec; 5.099 sec/batch)
2018-05-03 11:56:51.940561: step 33490, loss = 18.35 (3.2 examples/sec; 4.932 sec/batch)
2018-05-03 11:57:41.726046: step 33500, loss = 18.10 (3.2 examples/sec; 5.044 sec/batch)
2018-05-03 11:58:34.480586: step 33510, loss = 18.48 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 11:59:24.545451: step 33520, loss = 18.28 (3.2 examples/sec; 4.938 sec/batch)
2018-05-03 12:00:13.870216: step 33530, loss = 18.54 (3.3 examples/sec; 4.900 sec/batch)
2018-05-03 12:01:03.299928: step 33540, loss = 18.35 (3.3 examples/sec; 4.847 sec/batch)
2018-05-03 12:01:53.015161: step 33550, loss = 18.16 (3.1 examples/sec; 5.147 sec/batch)
2018-05-03 12:02:42.791852: step 33560, loss = 18.59 (3.4 examples/sec; 4.760 sec/batch)
2018-05-03 12:03:32.537290: step 33570, loss = 18.52 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 12:04:22.630280: step 33580, loss = 18.65 (3.4 examples/sec; 4.730 sec/batch)
2018-05-03 12:05:12.386344: step 33590, loss = 18.16 (3.3 examples/sec; 4.817 sec/batch)
2018-05-03 12:06:01.714004: step 33600, loss = 18.38 (3.3 examples/sec; 4.891 sec/batch)
2018-05-03 12:06:51.807674: step 33610, loss = 18.89 (3.6 examples/sec; 4.472 sec/batch)
2018-05-03 12:07:41.412005: step 33620, loss = 18.71 (3.3 examples/sec; 4.851 sec/batch)
2018-05-03 12:08:31.402120: step 33630, loss = 18.68 (3.1 examples/sec; 5.137 sec/batch)
2018-05-03 12:09:20.615557: step 33640, loss = 18.59 (3.3 examples/sec; 4.804 sec/batch)
2018-05-03 12:10:09.747602: step 33650, loss = 18.64 (3.2 examples/sec; 4.931 sec/batch)
2018-05-03 12:10:58.455100: step 33660, loss = 19.10 (3.2 examples/sec; 4.956 sec/batch)
2018-05-03 12:11:47.176006: step 33670, loss = 18.41 (3.2 examples/sec; 5.043 sec/batch)
2018-05-03 12:12:36.352779: step 33680, loss = 18.56 (3.2 examples/sec; 5.050 sec/batch)
2018-05-03 12:13:25.149267: step 33690, loss = 18.39 (3.3 examples/sec; 4.886 sec/batch)
2018-05-03 12:14:15.082468: step 33700, loss = 18.25 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 12:15:08.127074: step 33710, loss = 18.79 (3.3 examples/sec; 4.818 sec/batch)
2018-05-03 12:15:57.690156: step 33720, loss = 18.15 (3.2 examples/sec; 5.010 sec/batch)
2018-05-03 12:16:47.350240: step 33730, loss = 18.64 (3.0 examples/sec; 5.341 sec/batch)
2018-05-03 12:17:33.854065: step 33740, loss = 18.47 (3.4 examples/sec; 4.719 sec/batch)
2018-05-03 12:18:23.503213: step 33750, loss = 18.68 (3.3 examples/sec; 4.900 sec/batch)
2018-05-03 12:19:13.408106: step 33760, loss = 19.24 (3.2 examples/sec; 4.925 sec/batch)
2018-05-03 12:20:02.441696: step 33770, loss = 18.71 (3.2 examples/sec; 5.001 sec/batch)
2018-05-03 12:20:51.974958: step 33780, loss = 18.21 (3.2 examples/sec; 4.933 sec/batch)
2018-05-03 12:21:41.291074: step 33790, loss = 18.40 (3.2 examples/sec; 5.006 sec/batch)
2018-05-03 12:22:30.928280: step 33800, loss = 18.95 (3.1 examples/sec; 5.135 sec/batch)
2018-05-03 12:23:24.469915: step 33810, loss = 18.62 (3.3 examples/sec; 4.813 sec/batch)
2018-05-03 12:24:13.820658: step 33820, loss = 18.64 (3.2 examples/sec; 4.999 sec/batch)
2018-05-03 12:25:03.389702: step 33830, loss = 18.82 (3.4 examples/sec; 4.752 sec/batch)
2018-05-03 12:25:52.836377: step 33840, loss = 19.14 (3.2 examples/sec; 5.037 sec/batch)
2018-05-03 12:26:42.032859: step 33850, loss = 18.70 (3.3 examples/sec; 4.872 sec/batch)
2018-05-03 12:27:28.896702: step 33860, loss = 18.62 (3.2 examples/sec; 4.984 sec/batch)
2018-05-03 12:28:18.984006: step 33870, loss = 18.08 (3.1 examples/sec; 5.127 sec/batch)
2018-05-03 12:29:08.971873: step 33880, loss = 18.40 (3.2 examples/sec; 4.967 sec/batch)
2018-05-03 12:29:58.770011: step 33890, loss = 18.76 (3.2 examples/sec; 4.998 sec/batch)
2018-05-03 12:30:48.638662: step 33900, loss = 18.90 (3.2 examples/sec; 5.004 sec/batch)
2018-05-03 12:31:42.566298: step 33910, loss = 18.18 (3.2 examples/sec; 4.949 sec/batch)
2018-05-03 12:32:32.461855: step 33920, loss = 18.27 (3.1 examples/sec; 5.124 sec/batch)
2018-05-03 12:33:22.350439: step 33930, loss = 18.94 (3.3 examples/sec; 4.901 sec/batch)
2018-05-03 12:34:12.144020: step 33940, loss = 19.13 (3.2 examples/sec; 4.933 sec/batch)
2018-05-03 12:35:01.801605: step 33950, loss = 18.45 (3.4 examples/sec; 4.717 sec/batch)
2018-05-03 12:35:52.174395: step 33960, loss = 18.42 (3.2 examples/sec; 5.023 sec/batch)
2018-05-03 12:36:41.725354: step 33970, loss = 18.33 (3.1 examples/sec; 5.140 sec/batch)
2018-05-03 12:37:29.167305: step 33980, loss = 18.82 (4.1 examples/sec; 3.856 sec/batch)
2018-05-03 12:38:18.574805: step 33990, loss = 18.76 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 12:39:08.415198: step 34000, loss = 18.59 (3.3 examples/sec; 4.903 sec/batch)
2018-05-03 12:40:01.151197: step 34010, loss = 18.67 (3.3 examples/sec; 4.847 sec/batch)
2018-05-03 12:40:50.831980: step 34020, loss = 19.42 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 12:41:40.493501: step 34030, loss = 18.67 (3.2 examples/sec; 4.979 sec/batch)
2018-05-03 12:42:30.337409: step 34040, loss = 19.22 (3.1 examples/sec; 5.079 sec/batch)
2018-05-03 12:43:19.558414: step 34050, loss = 18.96 (3.3 examples/sec; 4.855 sec/batch)
2018-05-03 12:44:08.375309: step 34060, loss = 19.25 (3.3 examples/sec; 4.870 sec/batch)
2018-05-03 12:44:57.238316: step 34070, loss = 19.48 (3.3 examples/sec; 4.866 sec/batch)
2018-05-03 12:45:46.319355: step 34080, loss = 18.45 (3.3 examples/sec; 4.824 sec/batch)
2018-05-03 12:46:36.182769: step 34090, loss = 18.80 (3.3 examples/sec; 4.878 sec/batch)
2018-05-03 12:47:25.934003: step 34100, loss = 18.74 (3.1 examples/sec; 5.152 sec/batch)
2018-05-03 12:48:15.103965: step 34110, loss = 18.63 (3.2 examples/sec; 5.000 sec/batch)
2018-05-03 12:49:04.653674: step 34120, loss = 18.43 (3.2 examples/sec; 5.041 sec/batch)
2018-05-03 12:49:54.380663: step 34130, loss = 18.42 (3.1 examples/sec; 5.080 sec/batch)
2018-05-03 12:50:43.556458: step 34140, loss = 19.47 (3.3 examples/sec; 4.853 sec/batch)
2018-05-03 12:51:33.554085: step 34150, loss = 18.21 (3.2 examples/sec; 4.970 sec/batch)
2018-05-03 12:52:23.605246: step 34160, loss = 18.07 (3.2 examples/sec; 5.064 sec/batch)
2018-05-03 12:53:14.050181: step 34170, loss = 17.92 (3.3 examples/sec; 4.913 sec/batch)
2018-05-03 12:54:03.205561: step 34180, loss = 18.15 (3.3 examples/sec; 4.838 sec/batch)
2018-05-03 12:54:53.418955: step 34190, loss = 18.89 (3.3 examples/sec; 4.895 sec/batch)
2018-05-03 12:55:42.912094: step 34200, loss = 18.30 (3.3 examples/sec; 4.863 sec/batch)
2018-05-03 12:56:36.219458: step 34210, loss = 18.61 (3.3 examples/sec; 4.866 sec/batch)
2018-05-03 12:57:26.272787: step 34220, loss = 18.45 (3.3 examples/sec; 4.861 sec/batch)
2018-05-03 12:58:12.761895: step 34230, loss = 18.60 (3.1 examples/sec; 5.098 sec/batch)
2018-05-03 12:59:02.964764: step 34240, loss = 18.88 (3.2 examples/sec; 4.985 sec/batch)
2018-05-03 12:59:52.623195: step 34250, loss = 18.55 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 13:00:42.282259: step 34260, loss = 18.39 (3.3 examples/sec; 4.910 sec/batch)
2018-05-03 13:01:32.225767: step 34270, loss = 18.22 (3.2 examples/sec; 5.073 sec/batch)
2018-05-03 13:02:21.988209: step 34280, loss = 18.51 (3.3 examples/sec; 4.898 sec/batch)
2018-05-03 13:03:12.159501: step 34290, loss = 18.43 (3.1 examples/sec; 5.139 sec/batch)
2018-05-03 13:04:02.013461: step 34300, loss = 18.65 (3.2 examples/sec; 5.066 sec/batch)
2018-05-03 13:04:55.166052: step 34310, loss = 18.35 (3.3 examples/sec; 4.909 sec/batch)
2018-05-03 13:05:44.739399: step 34320, loss = 18.48 (3.2 examples/sec; 4.939 sec/batch)
2018-05-03 13:06:35.134080: step 34330, loss = 18.69 (3.1 examples/sec; 5.128 sec/batch)
2018-05-03 13:07:24.967279: step 34340, loss = 18.88 (3.2 examples/sec; 4.945 sec/batch)
2018-05-03 13:08:11.379260: step 34350, loss = 18.26 (3.6 examples/sec; 4.474 sec/batch)
2018-05-03 13:09:01.621807: step 34360, loss = 18.98 (3.2 examples/sec; 4.969 sec/batch)
2018-05-03 13:09:51.534544: step 34370, loss = 18.04 (3.3 examples/sec; 4.839 sec/batch)
2018-05-03 13:10:41.408973: step 34380, loss = 18.94 (3.3 examples/sec; 4.827 sec/batch)
2018-05-03 13:11:31.134706: step 34390, loss = 18.52 (3.1 examples/sec; 5.083 sec/batch)
2018-05-03 13:12:20.708367: step 34400, loss = 18.45 (3.3 examples/sec; 4.911 sec/batch)
2018-05-03 13:13:13.698176: step 34410, loss = 18.72 (3.2 examples/sec; 4.963 sec/batch)
2018-05-03 13:14:03.420573: step 34420, loss = 18.02 (3.3 examples/sec; 4.920 sec/batch)
2018-05-03 13:14:54.078495: step 34430, loss = 18.28 (3.2 examples/sec; 4.974 sec/batch)
2018-05-03 13:15:43.183869: step 34440, loss = 18.64 (3.3 examples/sec; 4.811 sec/batch)
2018-05-03 13:16:32.299088: step 34450, loss = 18.40 (3.3 examples/sec; 4.826 sec/batch)
2018-05-03 13:17:22.476029: step 34460, loss = 18.33 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 13:18:12.158721: step 34470, loss = 18.92 (3.1 examples/sec; 5.080 sec/batch)
2018-05-03 13:18:58.623078: step 34480, loss = 19.74 (3.2 examples/sec; 4.958 sec/batch)
2018-05-03 13:19:48.097721: step 34490, loss = 18.97 (3.3 examples/sec; 4.873 sec/batch)
2018-05-03 13:20:38.090695: step 34500, loss = 18.31 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 13:21:31.697293: step 34510, loss = 19.63 (3.3 examples/sec; 4.896 sec/batch)
2018-05-03 13:22:21.219102: step 34520, loss = 18.76 (3.3 examples/sec; 4.831 sec/batch)
2018-05-03 13:23:10.534526: step 34530, loss = 18.30 (3.2 examples/sec; 5.044 sec/batch)
2018-05-03 13:24:00.121412: step 34540, loss = 18.89 (3.2 examples/sec; 4.950 sec/batch)
2018-05-03 13:24:49.936166: step 34550, loss = 18.43 (3.3 examples/sec; 4.919 sec/batch)
2018-05-03 13:25:40.311009: step 34560, loss = 18.44 (3.0 examples/sec; 5.253 sec/batch)
2018-05-03 13:26:30.171659: step 34570, loss = 18.25 (3.2 examples/sec; 5.032 sec/batch)
2018-05-03 13:27:19.179618: step 34580, loss = 18.41 (3.3 examples/sec; 4.867 sec/batch)
2018-05-03 13:28:07.876756: step 34590, loss = 18.23 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 13:28:53.989127: step 34600, loss = 18.66 (3.3 examples/sec; 4.877 sec/batch)
2018-05-03 13:29:47.544574: step 34610, loss = 18.95 (3.1 examples/sec; 5.221 sec/batch)
2018-05-03 13:30:37.148503: step 34620, loss = 18.22 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 13:31:26.701791: step 34630, loss = 18.37 (3.3 examples/sec; 4.916 sec/batch)
2018-05-03 13:32:16.156964: step 34640, loss = 18.46 (3.2 examples/sec; 4.944 sec/batch)
2018-05-03 13:33:05.375485: step 34650, loss = 18.29 (3.2 examples/sec; 4.958 sec/batch)
2018-05-03 13:33:55.670738: step 34660, loss = 18.31 (3.0 examples/sec; 5.280 sec/batch)
2018-05-03 13:34:45.588320: step 34670, loss = 19.02 (3.1 examples/sec; 5.244 sec/batch)
2018-05-03 13:35:35.309073: step 34680, loss = 18.53 (3.3 examples/sec; 4.851 sec/batch)
2018-05-03 13:36:24.485239: step 34690, loss = 18.27 (3.3 examples/sec; 4.908 sec/batch)
2018-05-03 13:37:13.782069: step 34700, loss = 18.53 (3.3 examples/sec; 4.869 sec/batch)
2018-05-03 13:38:06.557217: step 34710, loss = 18.87 (3.3 examples/sec; 4.833 sec/batch)
2018-05-03 13:38:53.272022: step 34720, loss = 18.57 (4.2 examples/sec; 3.830 sec/batch)
2018-05-03 13:39:42.883809: step 34730, loss = 18.27 (3.2 examples/sec; 4.946 sec/batch)
2018-05-03 13:40:32.611161: step 34740, loss = 19.12 (3.3 examples/sec; 4.871 sec/batch)
2018-05-03 13:41:22.526478: step 34750, loss = 18.85 (3.3 examples/sec; 4.909 sec/batch)
2018-05-03 13:42:12.339369: step 34760, loss = 18.64 (3.3 examples/sec; 4.837 sec/batch)
2018-05-03 13:43:01.861999: step 34770, loss = 18.66 (3.1 examples/sec; 5.082 sec/batch)
2018-05-03 13:43:51.665596: step 34780, loss = 18.86 (3.2 examples/sec; 5.013 sec/batch)
2018-05-03 13:44:41.244387: step 34790, loss = 18.70 (3.3 examples/sec; 4.883 sec/batch)
2018-05-03 13:45:31.220616: step 34800, loss = 18.23 (3.2 examples/sec; 5.060 sec/batch)
2018-05-03 13:46:24.719329: step 34810, loss = 18.62 (3.2 examples/sec; 4.935 sec/batch)
2018-05-03 13:47:14.385536: step 34820, loss = 19.00 (3.2 examples/sec; 4.987 sec/batch)
2018-05-03 13:48:04.052488: step 34830, loss = 18.41 (3.2 examples/sec; 4.974 sec/batch)
2018-05-03 13:48:53.346675: step 34840, loss = 18.73 (3.3 examples/sec; 4.809 sec/batch)
2018-05-03 13:49:40.466031: step 34850, loss = 18.95 (3.2 examples/sec; 5.050 sec/batch)
2018-05-03 13:50:30.330829: step 34860, loss = 18.26 (3.3 examples/sec; 4.800 sec/batch)
2018-05-03 13:51:20.111950: step 34870, loss = 18.37 (3.1 examples/sec; 5.153 sec/batch)
2018-05-03 13:52:09.468143: step 34880, loss = 18.06 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 13:52:59.158123: step 34890, loss = 18.94 (3.3 examples/sec; 4.892 sec/batch)
2018-05-03 13:53:47.889196: step 34900, loss = 19.23 (3.4 examples/sec; 4.760 sec/batch)
2018-05-03 13:54:41.116703: step 34910, loss = 18.66 (3.2 examples/sec; 4.959 sec/batch)
2018-05-03 13:55:30.315657: step 34920, loss = 18.34 (3.2 examples/sec; 4.959 sec/batch)
2018-05-03 13:56:19.289215: step 34930, loss = 18.32 (3.3 examples/sec; 4.799 sec/batch)
2018-05-03 13:57:09.135731: step 34940, loss = 18.80 (3.2 examples/sec; 4.938 sec/batch)
2018-05-03 13:57:58.061085: step 34950, loss = 18.18 (3.2 examples/sec; 5.008 sec/batch)
2018-05-03 13:58:47.437150: step 34960, loss = 18.12 (3.2 examples/sec; 5.010 sec/batch)
2018-05-03 13:59:34.034022: step 34970, loss = 18.52 (3.2 examples/sec; 5.072 sec/batch)
2018-05-03 14:00:23.585541: step 34980, loss = 18.46 (3.1 examples/sec; 5.172 sec/batch)
2018-05-03 14:01:13.107189: step 34990, loss = 18.76 (3.3 examples/sec; 4.832 sec/batch)
2018-05-03 14:02:02.905642: step 35000, loss = 19.81 (3.2 examples/sec; 5.031 sec/batch)
2018-05-03 14:02:55.764701: step 35010, loss = 18.36 (3.3 examples/sec; 4.888 sec/batch)
2018-05-03 14:03:44.455448: step 35020, loss = 18.23 (3.4 examples/sec; 4.774 sec/batch)
2018-05-03 14:04:34.220556: step 35030, loss = 18.49 (3.3 examples/sec; 4.883 sec/batch)
2018-05-03 14:05:24.223352: step 35040, loss = 18.67 (3.2 examples/sec; 4.949 sec/batch)
2018-05-03 14:06:13.943007: step 35050, loss = 18.65 (3.2 examples/sec; 5.064 sec/batch)
2018-05-03 14:07:03.314632: step 35060, loss = 18.64 (3.2 examples/sec; 4.929 sec/batch)
2018-05-03 14:07:52.276326: step 35070, loss = 18.84 (3.2 examples/sec; 4.932 sec/batch)
2018-05-03 14:08:41.433603: step 35080, loss = 18.60 (3.3 examples/sec; 4.847 sec/batch)
2018-05-03 14:09:30.925386: step 35090, loss = 18.36 (3.2 examples/sec; 4.988 sec/batch)
2018-05-03 14:10:18.019057: step 35100, loss = 18.23 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 14:11:10.795814: step 35110, loss = 18.25 (3.2 examples/sec; 5.045 sec/batch)
2018-05-03 14:12:00.605676: step 35120, loss = 19.01 (3.2 examples/sec; 5.044 sec/batch)
2018-05-03 14:12:49.619708: step 35130, loss = 18.30 (3.1 examples/sec; 5.123 sec/batch)
2018-05-03 14:13:39.430044: step 35140, loss = 19.12 (3.1 examples/sec; 5.107 sec/batch)
2018-05-03 14:14:28.894548: step 35150, loss = 19.43 (3.2 examples/sec; 5.016 sec/batch)
2018-05-03 14:15:18.114418: step 35160, loss = 19.26 (3.3 examples/sec; 4.801 sec/batch)
2018-05-03 14:16:07.386485: step 35170, loss = 18.99 (3.3 examples/sec; 4.809 sec/batch)
2018-05-03 14:16:57.329010: step 35180, loss = 18.39 (3.1 examples/sec; 5.126 sec/batch)
2018-05-03 14:17:47.138003: step 35190, loss = 18.37 (3.3 examples/sec; 4.882 sec/batch)
2018-05-03 14:18:36.734721: step 35200, loss = 18.77 (3.2 examples/sec; 5.010 sec/batch)
2018-05-03 14:19:30.363046: step 35210, loss = 18.20 (3.2 examples/sec; 5.075 sec/batch)
2018-05-03 14:20:17.279595: step 35220, loss = 18.88 (3.2 examples/sec; 5.063 sec/batch)
2018-05-03 14:21:06.972507: step 35230, loss = 18.69 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 14:21:56.846666: step 35240, loss = 18.68 (3.2 examples/sec; 4.997 sec/batch)
2018-05-03 14:22:46.896261: step 35250, loss = 18.36 (3.3 examples/sec; 4.911 sec/batch)
2018-05-03 14:23:36.179983: step 35260, loss = 18.37 (3.3 examples/sec; 4.865 sec/batch)
2018-05-03 14:24:26.407334: step 35270, loss = 18.51 (3.2 examples/sec; 4.940 sec/batch)
2018-05-03 14:25:15.847897: step 35280, loss = 18.98 (3.2 examples/sec; 4.984 sec/batch)
2018-05-03 14:26:05.931149: step 35290, loss = 18.47 (3.2 examples/sec; 4.923 sec/batch)
2018-05-03 14:26:55.724616: step 35300, loss = 17.97 (3.3 examples/sec; 4.915 sec/batch)
2018-05-03 14:27:48.986890: step 35310, loss = 18.57 (3.2 examples/sec; 4.990 sec/batch)
2018-05-03 14:28:38.029011: step 35320, loss = 18.66 (3.3 examples/sec; 4.776 sec/batch)
2018-05-03 14:29:28.335621: step 35330, loss = 18.51 (3.2 examples/sec; 5.022 sec/batch)
2018-05-03 14:30:14.690902: step 35340, loss = 18.58 (3.2 examples/sec; 4.977 sec/batch)
2018-05-03 14:31:03.943003: step 35350, loss = 18.72 (3.2 examples/sec; 4.924 sec/batch)
2018-05-03 14:31:53.480257: step 35360, loss = 18.24 (3.2 examples/sec; 4.975 sec/batch)
2018-05-03 14:32:43.624397: step 35370, loss = 18.71 (3.3 examples/sec; 4.909 sec/batch)
2018-05-03 14:33:33.094219: step 35380, loss = 18.10 (3.3 examples/sec; 4.808 sec/batch)
2018-05-03 14:34:22.303711: step 35390, loss = 18.39 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 14:35:11.498146: step 35400, loss = 19.43 (3.4 examples/sec; 4.752 sec/batch)
2018-05-03 14:36:04.521673: step 35410, loss = 18.67 (3.3 examples/sec; 4.879 sec/batch)
2018-05-03 14:36:53.907070: step 35420, loss = 18.28 (3.2 examples/sec; 5.068 sec/batch)
2018-05-03 14:37:44.425276: step 35430, loss = 18.08 (3.1 examples/sec; 5.124 sec/batch)
2018-05-03 14:38:34.124958: step 35440, loss = 18.38 (3.2 examples/sec; 4.933 sec/batch)
2018-05-03 14:39:23.389696: step 35450, loss = 19.04 (3.2 examples/sec; 4.942 sec/batch)
2018-05-03 14:40:12.488228: step 35460, loss = 18.14 (3.2 examples/sec; 4.985 sec/batch)
2018-05-03 14:40:59.107377: step 35470, loss = 18.88 (3.2 examples/sec; 5.059 sec/batch)
2018-05-03 14:41:48.342298: step 35480, loss = 18.47 (3.4 examples/sec; 4.771 sec/batch)
2018-05-03 14:42:37.410210: step 35490, loss = 18.19 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 14:43:26.302173: step 35500, loss = 17.81 (3.3 examples/sec; 4.798 sec/batch)
2018-05-03 14:44:18.841112: step 35510, loss = 18.24 (3.2 examples/sec; 4.936 sec/batch)
2018-05-03 14:45:08.556492: step 35520, loss = 17.96 (3.3 examples/sec; 4.893 sec/batch)
2018-05-03 14:45:58.011862: step 35530, loss = 19.07 (3.3 examples/sec; 4.898 sec/batch)
2018-05-03 14:46:47.674820: step 35540, loss = 18.20 (3.2 examples/sec; 5.012 sec/batch)
2018-05-03 14:47:37.210750: step 35550, loss = 18.45 (3.2 examples/sec; 5.022 sec/batch)
2018-05-03 14:48:26.755632: step 35560, loss = 18.74 (3.2 examples/sec; 5.011 sec/batch)
2018-05-03 14:49:15.642254: step 35570, loss = 18.73 (3.2 examples/sec; 4.950 sec/batch)
2018-05-03 14:50:04.268266: step 35580, loss = 18.69 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 14:50:50.225801: step 35590, loss = 19.01 (3.2 examples/sec; 4.942 sec/batch)
2018-05-03 14:51:39.433272: step 35600, loss = 17.97 (3.3 examples/sec; 4.802 sec/batch)
2018-05-03 14:52:32.392870: step 35610, loss = 18.60 (3.2 examples/sec; 4.934 sec/batch)
2018-05-03 14:53:21.696683: step 35620, loss = 18.35 (3.3 examples/sec; 4.853 sec/batch)
2018-05-03 14:54:11.836645: step 35630, loss = 18.39 (3.3 examples/sec; 4.899 sec/batch)
2018-05-03 14:55:01.243824: step 35640, loss = 18.62 (3.2 examples/sec; 4.993 sec/batch)
2018-05-03 14:55:50.970572: step 35650, loss = 18.41 (3.3 examples/sec; 4.815 sec/batch)
2018-05-03 14:56:40.227175: step 35660, loss = 19.25 (3.3 examples/sec; 4.916 sec/batch)
2018-05-03 14:57:29.810405: step 35670, loss = 18.39 (3.2 examples/sec; 5.031 sec/batch)
2018-05-03 14:58:19.757851: step 35680, loss = 18.65 (3.2 examples/sec; 4.982 sec/batch)
2018-05-03 14:59:09.686105: step 35690, loss = 18.39 (3.3 examples/sec; 4.910 sec/batch)
2018-05-03 14:59:59.240595: step 35700, loss = 18.45 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 15:00:51.198003: step 35710, loss = 18.06 (4.1 examples/sec; 3.899 sec/batch)
2018-05-03 15:01:39.225645: step 35720, loss = 18.17 (3.2 examples/sec; 4.955 sec/batch)
2018-05-03 15:02:29.085834: step 35730, loss = 18.04 (3.1 examples/sec; 5.095 sec/batch)
2018-05-03 15:03:18.600388: step 35740, loss = 18.90 (3.2 examples/sec; 4.933 sec/batch)
2018-05-03 15:04:07.912650: step 35750, loss = 18.41 (3.2 examples/sec; 5.027 sec/batch)
2018-05-03 15:04:57.677189: step 35760, loss = 18.92 (3.2 examples/sec; 5.055 sec/batch)
2018-05-03 15:05:47.122008: step 35770, loss = 18.33 (3.3 examples/sec; 4.900 sec/batch)
2018-05-03 15:06:36.679209: step 35780, loss = 18.31 (3.3 examples/sec; 4.919 sec/batch)
2018-05-03 15:07:26.578546: step 35790, loss = 18.46 (3.1 examples/sec; 5.199 sec/batch)
2018-05-03 15:08:15.808130: step 35800, loss = 18.24 (3.2 examples/sec; 5.069 sec/batch)
2018-05-03 15:09:09.148814: step 35810, loss = 18.44 (3.2 examples/sec; 5.063 sec/batch)
2018-05-03 15:09:58.928317: step 35820, loss = 18.54 (3.2 examples/sec; 5.070 sec/batch)
2018-05-03 15:10:49.081352: step 35830, loss = 18.42 (3.2 examples/sec; 5.008 sec/batch)
2018-05-03 15:11:36.049452: step 35840, loss = 18.49 (3.1 examples/sec; 5.098 sec/batch)
2018-05-03 15:12:25.598582: step 35850, loss = 18.08 (3.3 examples/sec; 4.848 sec/batch)
2018-05-03 15:13:14.989871: step 35860, loss = 18.39 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 15:14:05.259693: step 35870, loss = 18.58 (3.1 examples/sec; 5.132 sec/batch)
2018-05-03 15:14:54.633114: step 35880, loss = 18.68 (3.2 examples/sec; 5.006 sec/batch)
2018-05-03 15:15:43.952435: step 35890, loss = 18.41 (3.2 examples/sec; 4.925 sec/batch)
2018-05-03 15:16:33.886482: step 35900, loss = 18.47 (3.2 examples/sec; 4.998 sec/batch)
2018-05-03 15:17:27.243015: step 35910, loss = 17.94 (3.2 examples/sec; 4.924 sec/batch)
2018-05-03 15:18:16.377799: step 35920, loss = 18.74 (3.3 examples/sec; 4.886 sec/batch)
2018-05-03 15:19:05.791031: step 35930, loss = 18.14 (3.2 examples/sec; 4.990 sec/batch)
2018-05-03 15:19:55.229032: step 35940, loss = 18.81 (3.2 examples/sec; 4.986 sec/batch)
2018-05-03 15:20:44.854342: step 35950, loss = 18.31 (3.3 examples/sec; 4.859 sec/batch)
2018-05-03 15:21:31.094985: step 35960, loss = 18.87 (4.2 examples/sec; 3.832 sec/batch)
2018-05-03 15:22:20.103892: step 35970, loss = 18.47 (3.3 examples/sec; 4.851 sec/batch)
2018-05-03 15:23:09.656363: step 35980, loss = 18.68 (3.3 examples/sec; 4.893 sec/batch)
2018-05-03 15:23:58.602605: step 35990, loss = 18.10 (3.2 examples/sec; 5.078 sec/batch)
2018-05-03 15:24:48.132437: step 36000, loss = 18.19 (3.2 examples/sec; 4.951 sec/batch)
2018-05-03 15:25:40.839696: step 36010, loss = 18.15 (3.3 examples/sec; 4.778 sec/batch)
2018-05-03 15:26:30.329345: step 36020, loss = 18.27 (3.2 examples/sec; 5.004 sec/batch)
2018-05-03 15:27:19.156102: step 36030, loss = 18.82 (3.3 examples/sec; 4.838 sec/batch)
2018-05-03 15:28:08.661574: step 36040, loss = 18.02 (3.3 examples/sec; 4.843 sec/batch)
2018-05-03 15:28:58.021687: step 36050, loss = 18.90 (3.3 examples/sec; 4.849 sec/batch)
2018-05-03 15:29:47.365839: step 36060, loss = 18.24 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 15:30:36.390369: step 36070, loss = 18.70 (3.3 examples/sec; 4.880 sec/batch)
2018-05-03 15:31:25.855055: step 36080, loss = 19.11 (3.2 examples/sec; 5.055 sec/batch)
2018-05-03 15:32:13.186552: step 36090, loss = 18.48 (3.1 examples/sec; 5.094 sec/batch)
2018-05-03 15:33:02.734379: step 36100, loss = 18.55 (3.3 examples/sec; 4.892 sec/batch)
2018-05-03 15:33:55.422438: step 36110, loss = 18.61 (3.3 examples/sec; 4.889 sec/batch)
2018-05-03 15:34:44.564086: step 36120, loss = 18.17 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 15:35:34.022234: step 36130, loss = 18.54 (3.2 examples/sec; 5.002 sec/batch)
2018-05-03 15:36:23.658509: step 36140, loss = 18.38 (3.2 examples/sec; 4.933 sec/batch)
2018-05-03 15:37:13.684158: step 36150, loss = 18.57 (3.3 examples/sec; 4.905 sec/batch)
2018-05-03 15:38:03.169479: step 36160, loss = 18.31 (3.2 examples/sec; 4.971 sec/batch)
2018-05-03 15:38:52.445499: step 36170, loss = 18.23 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 15:39:41.589339: step 36180, loss = 18.23 (3.3 examples/sec; 4.858 sec/batch)
2018-05-03 15:40:30.965991: step 36190, loss = 18.54 (3.3 examples/sec; 4.880 sec/batch)
2018-05-03 15:41:19.325492: step 36200, loss = 18.67 (3.3 examples/sec; 4.829 sec/batch)
2018-05-03 15:42:09.893040: step 36210, loss = 18.62 (4.2 examples/sec; 3.807 sec/batch)
2018-05-03 15:42:58.747105: step 36220, loss = 18.06 (3.3 examples/sec; 4.872 sec/batch)
2018-05-03 15:43:48.526059: step 36230, loss = 18.13 (3.2 examples/sec; 4.923 sec/batch)
2018-05-03 15:44:37.705571: step 36240, loss = 18.65 (3.2 examples/sec; 5.022 sec/batch)
2018-05-03 15:45:27.016598: step 36250, loss = 18.07 (3.1 examples/sec; 5.153 sec/batch)
2018-05-03 15:46:16.782753: step 36260, loss = 19.00 (3.3 examples/sec; 4.834 sec/batch)
2018-05-03 15:47:06.147409: step 36270, loss = 18.30 (3.2 examples/sec; 4.961 sec/batch)
2018-05-03 15:47:55.396513: step 36280, loss = 18.01 (3.2 examples/sec; 4.969 sec/batch)
2018-05-03 15:48:44.169324: step 36290, loss = 18.56 (3.3 examples/sec; 4.898 sec/batch)
2018-05-03 15:49:33.417419: step 36300, loss = 18.71 (3.2 examples/sec; 4.963 sec/batch)
2018-05-03 15:50:25.922399: step 36310, loss = 18.58 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 15:51:15.484002: step 36320, loss = 18.28 (3.2 examples/sec; 4.941 sec/batch)
2018-05-03 15:52:04.110750: step 36330, loss = 18.39 (3.3 examples/sec; 4.826 sec/batch)
2018-05-03 15:52:49.830062: step 36340, loss = 18.86 (3.3 examples/sec; 4.888 sec/batch)
2018-05-03 15:53:40.135078: step 36350, loss = 18.96 (3.2 examples/sec; 4.950 sec/batch)
2018-05-03 15:54:28.993293: step 36360, loss = 19.34 (3.3 examples/sec; 4.878 sec/batch)
2018-05-03 15:55:18.967258: step 36370, loss = 18.26 (3.2 examples/sec; 5.024 sec/batch)
2018-05-03 15:56:08.594299: step 36380, loss = 18.09 (3.2 examples/sec; 4.934 sec/batch)
2018-05-03 15:56:57.958494: step 36390, loss = 18.79 (3.3 examples/sec; 4.910 sec/batch)
2018-05-03 15:57:46.621470: step 36400, loss = 19.21 (3.4 examples/sec; 4.750 sec/batch)
2018-05-03 15:58:39.289870: step 36410, loss = 18.25 (3.3 examples/sec; 4.857 sec/batch)
2018-05-03 15:59:28.396520: step 36420, loss = 18.13 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 16:00:17.453091: step 36430, loss = 18.05 (3.3 examples/sec; 4.895 sec/batch)
2018-05-03 16:01:06.552076: step 36440, loss = 18.32 (3.3 examples/sec; 4.841 sec/batch)
2018-05-03 16:01:56.242922: step 36450, loss = 18.31 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 16:02:45.575958: step 36460, loss = 18.73 (3.2 examples/sec; 4.954 sec/batch)
2018-05-03 16:03:31.174482: step 36470, loss = 17.98 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 16:04:20.244135: step 36480, loss = 18.33 (3.2 examples/sec; 5.055 sec/batch)
2018-05-03 16:05:09.632833: step 36490, loss = 18.35 (3.3 examples/sec; 4.862 sec/batch)
2018-05-03 16:05:59.118740: step 36500, loss = 18.72 (3.1 examples/sec; 5.081 sec/batch)
2018-05-03 16:06:52.325821: step 36510, loss = 19.65 (3.2 examples/sec; 5.015 sec/batch)
2018-05-03 16:07:41.043128: step 36520, loss = 18.32 (3.3 examples/sec; 4.915 sec/batch)
2018-05-03 16:08:30.418817: step 36530, loss = 19.11 (3.3 examples/sec; 4.880 sec/batch)
2018-05-03 16:09:19.332847: step 36540, loss = 18.34 (3.3 examples/sec; 4.882 sec/batch)
2018-05-03 16:10:08.980835: step 36550, loss = 19.03 (3.2 examples/sec; 5.072 sec/batch)
2018-05-03 16:10:58.614782: step 36560, loss = 18.31 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 16:11:48.759183: step 36570, loss = 18.14 (3.3 examples/sec; 4.897 sec/batch)
2018-05-03 16:12:37.847940: step 36580, loss = 18.44 (3.2 examples/sec; 4.982 sec/batch)
2018-05-03 16:13:24.209278: step 36590, loss = 18.01 (3.3 examples/sec; 4.826 sec/batch)
2018-05-03 16:14:13.497082: step 36600, loss = 18.25 (3.3 examples/sec; 4.836 sec/batch)
2018-05-03 16:15:06.833994: step 36610, loss = 19.01 (3.1 examples/sec; 5.165 sec/batch)
2018-05-03 16:15:56.499247: step 36620, loss = 18.31 (3.1 examples/sec; 5.108 sec/batch)
2018-05-03 16:16:46.068623: step 36630, loss = 17.90 (3.4 examples/sec; 4.774 sec/batch)
2018-05-03 16:17:35.614476: step 36640, loss = 18.73 (3.2 examples/sec; 4.990 sec/batch)
2018-05-03 16:18:24.875481: step 36650, loss = 18.02 (3.2 examples/sec; 5.001 sec/batch)
2018-05-03 16:19:13.860280: step 36660, loss = 17.94 (3.3 examples/sec; 4.820 sec/batch)
2018-05-03 16:20:03.963025: step 36670, loss = 17.93 (3.1 examples/sec; 5.092 sec/batch)
2018-05-03 16:20:53.656807: step 36680, loss = 18.38 (3.4 examples/sec; 4.734 sec/batch)
2018-05-03 16:21:43.387080: step 36690, loss = 18.25 (3.3 examples/sec; 4.837 sec/batch)
2018-05-03 16:22:33.695147: step 36700, loss = 18.28 (3.0 examples/sec; 5.255 sec/batch)
2018-05-03 16:23:25.685626: step 36710, loss = 18.50 (4.1 examples/sec; 3.926 sec/batch)
2018-05-03 16:24:12.359163: step 36720, loss = 17.95 (3.2 examples/sec; 4.943 sec/batch)
2018-05-03 16:25:01.567459: step 36730, loss = 18.13 (3.3 examples/sec; 4.872 sec/batch)
2018-05-03 16:25:50.439860: step 36740, loss = 18.47 (3.2 examples/sec; 5.044 sec/batch)
2018-05-03 16:26:39.686658: step 36750, loss = 18.60 (3.3 examples/sec; 4.901 sec/batch)
2018-05-03 16:27:29.554402: step 36760, loss = 18.69 (3.2 examples/sec; 5.044 sec/batch)
2018-05-03 16:28:19.858488: step 36770, loss = 18.29 (3.2 examples/sec; 5.057 sec/batch)
2018-05-03 16:29:09.239937: step 36780, loss = 18.49 (3.2 examples/sec; 5.044 sec/batch)
2018-05-03 16:29:58.660840: step 36790, loss = 18.45 (3.3 examples/sec; 4.870 sec/batch)
2018-05-03 16:30:47.771271: step 36800, loss = 18.20 (3.3 examples/sec; 4.847 sec/batch)
2018-05-03 16:31:41.142037: step 36810, loss = 18.17 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 16:32:30.314057: step 36820, loss = 18.39 (3.2 examples/sec; 4.966 sec/batch)
2018-05-03 16:33:19.565119: step 36830, loss = 18.68 (3.2 examples/sec; 4.950 sec/batch)
2018-05-03 16:34:05.661000: step 36840, loss = 18.27 (3.3 examples/sec; 4.879 sec/batch)
2018-05-03 16:34:55.945099: step 36850, loss = 18.77 (3.2 examples/sec; 4.969 sec/batch)
2018-05-03 16:35:45.131730: step 36860, loss = 19.04 (3.4 examples/sec; 4.669 sec/batch)
2018-05-03 16:36:34.180339: step 36870, loss = 18.39 (3.3 examples/sec; 4.808 sec/batch)
2018-05-03 16:37:24.002029: step 36880, loss = 18.01 (3.3 examples/sec; 4.913 sec/batch)
2018-05-03 16:38:14.261205: step 36890, loss = 17.92 (3.3 examples/sec; 4.855 sec/batch)
2018-05-03 16:39:04.230739: step 36900, loss = 18.60 (3.1 examples/sec; 5.148 sec/batch)
2018-05-03 16:39:56.772814: step 36910, loss = 18.34 (3.3 examples/sec; 4.891 sec/batch)
2018-05-03 16:40:45.647213: step 36920, loss = 18.10 (3.3 examples/sec; 4.831 sec/batch)
2018-05-03 16:41:35.371139: step 36930, loss = 18.20 (3.2 examples/sec; 5.043 sec/batch)
2018-05-03 16:42:24.823727: step 36940, loss = 18.17 (3.3 examples/sec; 4.846 sec/batch)
2018-05-03 16:43:14.081684: step 36950, loss = 18.51 (3.2 examples/sec; 4.931 sec/batch)
2018-05-03 16:44:03.162399: step 36960, loss = 18.11 (3.5 examples/sec; 4.610 sec/batch)
2018-05-03 16:44:49.927116: step 36970, loss = 18.13 (3.2 examples/sec; 5.073 sec/batch)
2018-05-03 16:45:39.832080: step 36980, loss = 18.50 (3.2 examples/sec; 5.038 sec/batch)
2018-05-03 16:46:29.384249: step 36990, loss = 18.77 (3.2 examples/sec; 5.075 sec/batch)
2018-05-03 16:47:18.344539: step 37000, loss = 18.02 (3.2 examples/sec; 4.946 sec/batch)
2018-05-03 16:48:11.300336: step 37010, loss = 18.58 (3.3 examples/sec; 4.897 sec/batch)
2018-05-03 16:49:01.631557: step 37020, loss = 18.42 (3.2 examples/sec; 5.046 sec/batch)
2018-05-03 16:49:50.764018: step 37030, loss = 18.47 (3.3 examples/sec; 4.811 sec/batch)
2018-05-03 16:50:39.787590: step 37040, loss = 18.07 (3.2 examples/sec; 4.925 sec/batch)
2018-05-03 16:51:29.552398: step 37050, loss = 18.30 (3.2 examples/sec; 4.953 sec/batch)
2018-05-03 16:52:19.160341: step 37060, loss = 18.14 (3.2 examples/sec; 4.951 sec/batch)
2018-05-03 16:53:08.490451: step 37070, loss = 18.06 (3.4 examples/sec; 4.697 sec/batch)
2018-05-03 16:53:57.907822: step 37080, loss = 18.38 (3.2 examples/sec; 4.986 sec/batch)
2018-05-03 16:54:43.952773: step 37090, loss = 18.55 (3.4 examples/sec; 4.734 sec/batch)
2018-05-03 16:55:33.081004: step 37100, loss = 18.28 (3.2 examples/sec; 5.025 sec/batch)
2018-05-03 16:56:25.703889: step 37110, loss = 18.84 (3.2 examples/sec; 5.024 sec/batch)
2018-05-03 16:57:15.390531: step 37120, loss = 18.56 (3.3 examples/sec; 4.883 sec/batch)
2018-05-03 16:58:04.697422: step 37130, loss = 18.30 (3.2 examples/sec; 5.066 sec/batch)
2018-05-03 16:58:53.949967: step 37140, loss = 18.33 (3.2 examples/sec; 4.984 sec/batch)
2018-05-03 16:59:42.881387: step 37150, loss = 18.72 (3.3 examples/sec; 4.915 sec/batch)
2018-05-03 17:00:32.339501: step 37160, loss = 18.72 (3.3 examples/sec; 4.845 sec/batch)
2018-05-03 17:01:21.758190: step 37170, loss = 17.93 (3.2 examples/sec; 5.053 sec/batch)
2018-05-03 17:02:10.969954: step 37180, loss = 18.50 (3.2 examples/sec; 4.958 sec/batch)
2018-05-03 17:02:59.707682: step 37190, loss = 18.39 (3.3 examples/sec; 4.828 sec/batch)
2018-05-03 17:03:48.752670: step 37200, loss = 18.36 (3.3 examples/sec; 4.793 sec/batch)
2018-05-03 17:04:40.424701: step 37210, loss = 18.93 (4.2 examples/sec; 3.819 sec/batch)
2018-05-03 17:05:27.870407: step 37220, loss = 18.24 (3.0 examples/sec; 5.353 sec/batch)
2018-05-03 17:06:17.183832: step 37230, loss = 18.15 (3.3 examples/sec; 4.815 sec/batch)
2018-05-03 17:07:06.562911: step 37240, loss = 18.66 (3.2 examples/sec; 5.049 sec/batch)
2018-05-03 17:07:56.117363: step 37250, loss = 18.00 (3.2 examples/sec; 4.960 sec/batch)
2018-05-03 17:08:45.568830: step 37260, loss = 18.38 (3.2 examples/sec; 5.052 sec/batch)
2018-05-03 17:09:34.509043: step 37270, loss = 18.90 (3.4 examples/sec; 4.715 sec/batch)
2018-05-03 17:10:23.880334: step 37280, loss = 18.98 (3.2 examples/sec; 5.006 sec/batch)
2018-05-03 17:11:13.458435: step 37290, loss = 18.47 (3.3 examples/sec; 4.908 sec/batch)
2018-05-03 17:12:02.682118: step 37300, loss = 18.75 (3.2 examples/sec; 4.942 sec/batch)
2018-05-03 17:12:55.158668: step 37310, loss = 18.73 (3.2 examples/sec; 4.923 sec/batch)
2018-05-03 17:13:44.494765: step 37320, loss = 18.22 (3.1 examples/sec; 5.155 sec/batch)
2018-05-03 17:14:33.963999: step 37330, loss = 18.76 (3.2 examples/sec; 4.980 sec/batch)
2018-05-03 17:15:19.743660: step 37340, loss = 18.67 (3.2 examples/sec; 5.011 sec/batch)
2018-05-03 17:16:09.190474: step 37350, loss = 18.72 (3.3 examples/sec; 4.895 sec/batch)
2018-05-03 17:16:58.277011: step 37360, loss = 18.49 (3.2 examples/sec; 4.950 sec/batch)
2018-05-03 17:17:47.248882: step 37370, loss = 18.48 (3.4 examples/sec; 4.771 sec/batch)
2018-05-03 17:18:36.935658: step 37380, loss = 18.07 (3.2 examples/sec; 5.004 sec/batch)
2018-05-03 17:19:26.701728: step 37390, loss = 18.11 (3.2 examples/sec; 4.926 sec/batch)
2018-05-03 17:20:16.218572: step 37400, loss = 18.30 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 17:21:09.535737: step 37410, loss = 18.58 (3.3 examples/sec; 4.914 sec/batch)
2018-05-03 17:21:59.070845: step 37420, loss = 17.94 (3.4 examples/sec; 4.740 sec/batch)
2018-05-03 17:22:48.414457: step 37430, loss = 18.25 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 17:23:38.008904: step 37440, loss = 18.36 (3.2 examples/sec; 4.937 sec/batch)
2018-05-03 17:24:26.955165: step 37450, loss = 18.34 (3.2 examples/sec; 5.052 sec/batch)
2018-05-03 17:25:14.841978: step 37460, loss = 18.38 (4.2 examples/sec; 3.772 sec/batch)
2018-05-03 17:26:01.469909: step 37470, loss = 18.02 (3.2 examples/sec; 5.058 sec/batch)
2018-05-03 17:26:50.608920: step 37480, loss = 18.80 (3.2 examples/sec; 4.998 sec/batch)
2018-05-03 17:27:39.224529: step 37490, loss = 18.11 (3.3 examples/sec; 4.860 sec/batch)
2018-05-03 17:28:29.459804: step 37500, loss = 18.19 (3.1 examples/sec; 5.235 sec/batch)
2018-05-03 17:29:22.722046: step 37510, loss = 18.58 (3.3 examples/sec; 4.860 sec/batch)
2018-05-03 17:30:12.064027: step 37520, loss = 19.00 (3.2 examples/sec; 4.999 sec/batch)
2018-05-03 17:31:01.504747: step 37530, loss = 18.14 (3.3 examples/sec; 4.820 sec/batch)
2018-05-03 17:31:50.852648: step 37540, loss = 18.53 (3.4 examples/sec; 4.705 sec/batch)
2018-05-03 17:32:39.694507: step 37550, loss = 18.33 (3.3 examples/sec; 4.882 sec/batch)
2018-05-03 17:33:28.613285: step 37560, loss = 19.06 (3.4 examples/sec; 4.751 sec/batch)
2018-05-03 17:34:18.103315: step 37570, loss = 19.10 (3.4 examples/sec; 4.763 sec/batch)
2018-05-03 17:35:07.201178: step 37580, loss = 17.92 (3.2 examples/sec; 4.959 sec/batch)
2018-05-03 17:35:53.537222: step 37590, loss = 18.62 (3.3 examples/sec; 4.916 sec/batch)
2018-05-03 17:36:43.391933: step 37600, loss = 18.56 (3.3 examples/sec; 4.874 sec/batch)
2018-05-03 17:37:36.797694: step 37610, loss = 18.36 (3.1 examples/sec; 5.090 sec/batch)
2018-05-03 17:38:25.924359: step 37620, loss = 18.72 (3.3 examples/sec; 4.883 sec/batch)
2018-05-03 17:39:14.574759: step 37630, loss = 18.55 (3.3 examples/sec; 4.818 sec/batch)
2018-05-03 17:40:03.486739: step 37640, loss = 18.19 (3.3 examples/sec; 4.912 sec/batch)
2018-05-03 17:40:52.120457: step 37650, loss = 18.47 (3.1 examples/sec; 5.103 sec/batch)
2018-05-03 17:41:41.546939: step 37660, loss = 18.54 (3.2 examples/sec; 5.013 sec/batch)
2018-05-03 17:42:31.265540: step 37670, loss = 18.23 (3.3 examples/sec; 4.882 sec/batch)
2018-05-03 17:43:20.272584: step 37680, loss = 18.55 (3.3 examples/sec; 4.807 sec/batch)
2018-05-03 17:44:09.653352: step 37690, loss = 18.62 (3.2 examples/sec; 4.981 sec/batch)
2018-05-03 17:44:58.973654: step 37700, loss = 18.67 (3.2 examples/sec; 4.957 sec/batch)
2018-05-03 17:45:49.517483: step 37710, loss = 18.54 (4.2 examples/sec; 3.796 sec/batch)
2018-05-03 17:46:38.772050: step 37720, loss = 18.48 (3.2 examples/sec; 5.025 sec/batch)
2018-05-03 17:47:28.039349: step 37730, loss = 18.37 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 17:48:17.732994: step 37740, loss = 18.17 (3.2 examples/sec; 4.978 sec/batch)
2018-05-03 17:49:07.153414: step 37750, loss = 18.91 (3.3 examples/sec; 4.844 sec/batch)
2018-05-03 17:49:56.429791: step 37760, loss = 18.53 (3.2 examples/sec; 4.957 sec/batch)
2018-05-03 17:50:46.290540: step 37770, loss = 18.46 (3.3 examples/sec; 4.832 sec/batch)
2018-05-03 17:51:35.570895: step 37780, loss = 18.57 (3.3 examples/sec; 4.822 sec/batch)
2018-05-03 17:52:25.225706: step 37790, loss = 18.04 (3.2 examples/sec; 5.017 sec/batch)
2018-05-03 17:53:14.245888: step 37800, loss = 18.39 (3.3 examples/sec; 4.906 sec/batch)
2018-05-03 17:54:06.887429: step 37810, loss = 17.96 (3.2 examples/sec; 4.937 sec/batch)
2018-05-03 17:54:56.053724: step 37820, loss = 18.36 (3.3 examples/sec; 4.830 sec/batch)
2018-05-03 17:55:45.216429: step 37830, loss = 18.13 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 17:56:32.143190: step 37840, loss = 18.32 (3.3 examples/sec; 4.891 sec/batch)
2018-05-03 17:57:21.662116: step 37850, loss = 18.32 (3.3 examples/sec; 4.894 sec/batch)
2018-05-03 17:58:10.933084: step 37860, loss = 18.49 (3.2 examples/sec; 5.064 sec/batch)
2018-05-03 17:58:59.744804: step 37870, loss = 18.34 (3.3 examples/sec; 4.847 sec/batch)
2018-05-03 17:59:49.365573: step 37880, loss = 18.43 (3.2 examples/sec; 5.073 sec/batch)
2018-05-03 18:00:39.348341: step 37890, loss = 18.08 (3.2 examples/sec; 4.963 sec/batch)
2018-05-03 18:01:28.227764: step 37900, loss = 18.27 (3.3 examples/sec; 4.833 sec/batch)
2018-05-03 18:02:21.061661: step 37910, loss = 18.87 (3.2 examples/sec; 4.927 sec/batch)
2018-05-03 18:03:10.176282: step 37920, loss = 18.20 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 18:03:59.701953: step 37930, loss = 18.47 (3.4 examples/sec; 4.738 sec/batch)
2018-05-03 18:04:49.167521: step 37940, loss = 18.72 (3.2 examples/sec; 4.990 sec/batch)
2018-05-03 18:05:38.961969: step 37950, loss = 18.37 (3.2 examples/sec; 5.010 sec/batch)
2018-05-03 18:06:24.976785: step 37960, loss = 18.00 (3.3 examples/sec; 4.815 sec/batch)
2018-05-03 18:07:14.777005: step 37970, loss = 18.24 (3.2 examples/sec; 4.993 sec/batch)
2018-05-03 18:08:04.003640: step 37980, loss = 18.19 (3.3 examples/sec; 4.902 sec/batch)
2018-05-03 18:08:53.155558: step 37990, loss = 18.40 (3.3 examples/sec; 4.815 sec/batch)
2018-05-03 18:09:42.058485: step 38000, loss = 18.51 (3.2 examples/sec; 4.964 sec/batch)
2018-05-03 18:10:35.711009: step 38010, loss = 18.20 (3.2 examples/sec; 5.031 sec/batch)
2018-05-03 18:11:25.528934: step 38020, loss = 18.39 (3.2 examples/sec; 4.997 sec/batch)
2018-05-03 18:12:14.514725: step 38030, loss = 18.51 (3.4 examples/sec; 4.756 sec/batch)
2018-05-03 18:13:03.470805: step 38040, loss = 18.35 (3.3 examples/sec; 4.907 sec/batch)
2018-05-03 18:13:52.773981: step 38050, loss = 18.17 (3.2 examples/sec; 4.928 sec/batch)
2018-05-03 18:14:42.214019: step 38060, loss = 18.01 (3.2 examples/sec; 4.925 sec/batch)
2018-05-03 18:15:31.392811: step 38070, loss = 18.18 (3.2 examples/sec; 4.937 sec/batch)
2018-05-03 18:16:20.723106: step 38080, loss = 18.27 (3.3 examples/sec; 4.848 sec/batch)
2018-05-03 18:17:06.351389: step 38090, loss = 18.41 (3.3 examples/sec; 4.816 sec/batch)
2018-05-03 18:17:55.981274: step 38100, loss = 19.37 (3.2 examples/sec; 5.024 sec/batch)
2018-05-03 18:18:48.558961: step 38110, loss = 18.73 (3.3 examples/sec; 4.778 sec/batch)
2018-05-03 18:19:38.663964: step 38120, loss = 18.88 (3.3 examples/sec; 4.860 sec/batch)
2018-05-03 18:20:28.172145: step 38130, loss = 17.94 (3.2 examples/sec; 5.072 sec/batch)
2018-05-03 18:21:17.881004: step 38140, loss = 18.06 (3.4 examples/sec; 4.733 sec/batch)
2018-05-03 18:22:06.863031: step 38150, loss = 18.48 (3.3 examples/sec; 4.897 sec/batch)
2018-05-03 18:22:56.013887: step 38160, loss = 18.32 (3.3 examples/sec; 4.814 sec/batch)
2018-05-03 18:23:44.786806: step 38170, loss = 18.27 (3.4 examples/sec; 4.770 sec/batch)
2018-05-03 18:24:34.003339: step 38180, loss = 19.00 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 18:25:23.821471: step 38190, loss = 18.07 (3.2 examples/sec; 4.927 sec/batch)
2018-05-03 18:26:13.372620: step 38200, loss = 18.43 (3.3 examples/sec; 4.908 sec/batch)
2018-05-03 18:27:03.120278: step 38210, loss = 18.59 (3.3 examples/sec; 4.878 sec/batch)
2018-05-03 18:27:52.700688: step 38220, loss = 18.64 (3.2 examples/sec; 5.040 sec/batch)
2018-05-03 18:28:42.646697: step 38230, loss = 18.43 (3.1 examples/sec; 5.215 sec/batch)
2018-05-03 18:29:32.416703: step 38240, loss = 18.63 (3.3 examples/sec; 4.851 sec/batch)
2018-05-03 18:30:21.663271: step 38250, loss = 18.11 (3.2 examples/sec; 5.028 sec/batch)
2018-05-03 18:31:11.046206: step 38260, loss = 18.27 (3.3 examples/sec; 4.923 sec/batch)
2018-05-03 18:31:59.975973: step 38270, loss = 18.27 (3.4 examples/sec; 4.745 sec/batch)
2018-05-03 18:32:49.685775: step 38280, loss = 18.19 (3.1 examples/sec; 5.105 sec/batch)
2018-05-03 18:33:39.019208: step 38290, loss = 18.23 (3.3 examples/sec; 4.891 sec/batch)
2018-05-03 18:34:28.649380: step 38300, loss = 18.73 (3.2 examples/sec; 5.072 sec/batch)
2018-05-03 18:35:21.665385: step 38310, loss = 18.61 (3.2 examples/sec; 4.997 sec/batch)
2018-05-03 18:36:10.868607: step 38320, loss = 18.58 (3.2 examples/sec; 4.937 sec/batch)
2018-05-03 18:37:00.173050: step 38330, loss = 18.36 (3.2 examples/sec; 4.929 sec/batch)
2018-05-03 18:37:46.521947: step 38340, loss = 17.94 (3.1 examples/sec; 5.163 sec/batch)
2018-05-03 18:38:36.038277: step 38350, loss = 18.59 (3.2 examples/sec; 4.983 sec/batch)
2018-05-03 18:39:25.480394: step 38360, loss = 17.88 (3.2 examples/sec; 4.981 sec/batch)
2018-05-03 18:40:14.818370: step 38370, loss = 18.84 (3.2 examples/sec; 4.987 sec/batch)
2018-05-03 18:41:04.911643: step 38380, loss = 18.14 (3.2 examples/sec; 4.965 sec/batch)
2018-05-03 18:41:54.908821: step 38390, loss = 18.71 (3.3 examples/sec; 4.860 sec/batch)
2018-05-03 18:42:44.006976: step 38400, loss = 17.99 (3.3 examples/sec; 4.789 sec/batch)
2018-05-03 18:43:37.281267: step 38410, loss = 18.04 (3.2 examples/sec; 4.999 sec/batch)
2018-05-03 18:44:26.620081: step 38420, loss = 18.32 (3.3 examples/sec; 4.895 sec/batch)
2018-05-03 18:45:15.389897: step 38430, loss = 18.33 (3.4 examples/sec; 4.751 sec/batch)
2018-05-03 18:46:04.833684: step 38440, loss = 18.27 (3.2 examples/sec; 4.998 sec/batch)
2018-05-03 18:46:54.319294: step 38450, loss = 18.72 (3.2 examples/sec; 4.966 sec/batch)
2018-05-03 18:47:41.134940: step 38460, loss = 18.06 (3.2 examples/sec; 4.925 sec/batch)
2018-05-03 18:48:30.550356: step 38470, loss = 18.75 (3.2 examples/sec; 5.073 sec/batch)
2018-05-03 18:49:19.910986: step 38480, loss = 18.20 (3.3 examples/sec; 4.883 sec/batch)
2018-05-03 18:50:09.076034: step 38490, loss = 18.51 (3.3 examples/sec; 4.909 sec/batch)
2018-05-03 18:50:58.549089: step 38500, loss = 18.24 (3.3 examples/sec; 4.922 sec/batch)
2018-05-03 18:51:50.939469: step 38510, loss = 18.20 (3.3 examples/sec; 4.909 sec/batch)
2018-05-03 18:52:40.403050: step 38520, loss = 18.46 (3.3 examples/sec; 4.824 sec/batch)
2018-05-03 18:53:29.945957: step 38530, loss = 19.21 (3.3 examples/sec; 4.868 sec/batch)
2018-05-03 18:54:19.815873: step 38540, loss = 18.82 (3.2 examples/sec; 4.966 sec/batch)
2018-05-03 18:55:09.269883: step 38550, loss = 18.37 (3.2 examples/sec; 4.940 sec/batch)
2018-05-03 18:55:58.737437: step 38560, loss = 18.24 (3.3 examples/sec; 4.865 sec/batch)
2018-05-03 18:56:48.653872: step 38570, loss = 18.54 (3.2 examples/sec; 5.053 sec/batch)
2018-05-03 18:57:37.615933: step 38580, loss = 18.54 (4.2 examples/sec; 3.793 sec/batch)
2018-05-03 18:58:25.349472: step 38590, loss = 18.44 (3.1 examples/sec; 5.104 sec/batch)
2018-05-03 18:59:15.105464: step 38600, loss = 18.36 (3.2 examples/sec; 5.010 sec/batch)
2018-05-03 19:00:08.334477: step 38610, loss = 18.40 (3.2 examples/sec; 5.042 sec/batch)
2018-05-03 19:00:58.340441: step 38620, loss = 18.36 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 19:01:47.947150: step 38630, loss = 18.11 (3.2 examples/sec; 4.979 sec/batch)
2018-05-03 19:02:36.850016: step 38640, loss = 18.74 (3.4 examples/sec; 4.754 sec/batch)
2018-05-03 19:03:26.611103: step 38650, loss = 18.42 (3.1 examples/sec; 5.093 sec/batch)
2018-05-03 19:04:16.283381: step 38660, loss = 18.69 (3.2 examples/sec; 5.074 sec/batch)
2018-05-03 19:05:05.732867: step 38670, loss = 18.31 (3.4 examples/sec; 4.744 sec/batch)
2018-05-03 19:05:55.627645: step 38680, loss = 18.23 (3.3 examples/sec; 4.909 sec/batch)
2018-05-03 19:06:45.101783: step 38690, loss = 18.48 (3.3 examples/sec; 4.820 sec/batch)
2018-05-03 19:07:34.794023: step 38700, loss = 18.66 (3.3 examples/sec; 4.861 sec/batch)
2018-05-03 19:08:24.535144: step 38710, loss = 17.98 (3.2 examples/sec; 4.933 sec/batch)
2018-05-03 19:09:14.278320: step 38720, loss = 18.34 (3.2 examples/sec; 5.074 sec/batch)
2018-05-03 19:10:03.563777: step 38730, loss = 18.35 (3.2 examples/sec; 4.942 sec/batch)
2018-05-03 19:10:53.203659: step 38740, loss = 18.31 (3.3 examples/sec; 4.827 sec/batch)
2018-05-03 19:11:42.763217: step 38750, loss = 18.92 (3.1 examples/sec; 5.132 sec/batch)
2018-05-03 19:12:32.471713: step 38760, loss = 18.56 (3.2 examples/sec; 5.002 sec/batch)
2018-05-03 19:13:21.848536: step 38770, loss = 17.97 (3.2 examples/sec; 4.983 sec/batch)
2018-05-03 19:14:12.137173: step 38780, loss = 18.58 (3.2 examples/sec; 4.950 sec/batch)
2018-05-03 19:15:01.772763: step 38790, loss = 17.98 (3.3 examples/sec; 4.811 sec/batch)
2018-05-03 19:15:51.105955: step 38800, loss = 18.50 (3.2 examples/sec; 4.980 sec/batch)
2018-05-03 19:16:44.020777: step 38810, loss = 18.75 (3.2 examples/sec; 4.925 sec/batch)
2018-05-03 19:17:33.114597: step 38820, loss = 17.87 (3.2 examples/sec; 4.929 sec/batch)
2018-05-03 19:18:18.678520: step 38830, loss = 18.03 (4.1 examples/sec; 3.883 sec/batch)
2018-05-03 19:19:07.830510: step 38840, loss = 19.07 (3.3 examples/sec; 4.907 sec/batch)
2018-05-03 19:19:57.604667: step 38850, loss = 18.07 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 19:20:47.314822: step 38860, loss = 18.36 (3.2 examples/sec; 4.983 sec/batch)
2018-05-03 19:21:37.110652: step 38870, loss = 18.91 (3.2 examples/sec; 5.038 sec/batch)
2018-05-03 19:22:26.573595: step 38880, loss = 18.71 (3.2 examples/sec; 4.981 sec/batch)
2018-05-03 19:23:15.524231: step 38890, loss = 18.52 (3.3 examples/sec; 4.884 sec/batch)
2018-05-03 19:24:05.702301: step 38900, loss = 18.56 (3.1 examples/sec; 5.209 sec/batch)
2018-05-03 19:24:59.022098: step 38910, loss = 18.13 (3.2 examples/sec; 4.980 sec/batch)
2018-05-03 19:25:48.551808: step 38920, loss = 18.10 (3.2 examples/sec; 5.019 sec/batch)
2018-05-03 19:26:38.132974: step 38930, loss = 17.90 (3.3 examples/sec; 4.857 sec/batch)
2018-05-03 19:27:27.356992: step 38940, loss = 18.65 (3.1 examples/sec; 5.090 sec/batch)
2018-05-03 19:28:16.579295: step 38950, loss = 18.10 (3.5 examples/sec; 4.558 sec/batch)
2018-05-03 19:29:02.636307: step 38960, loss = 18.58 (3.1 examples/sec; 5.146 sec/batch)
2018-05-03 19:29:52.761399: step 38970, loss = 18.44 (3.2 examples/sec; 4.935 sec/batch)
2018-05-03 19:30:41.916706: step 38980, loss = 18.05 (3.2 examples/sec; 4.936 sec/batch)
2018-05-03 19:31:31.804894: step 38990, loss = 18.89 (3.2 examples/sec; 5.046 sec/batch)
2018-05-03 19:32:21.412178: step 39000, loss = 18.17 (3.2 examples/sec; 5.010 sec/batch)
2018-05-03 19:33:14.645201: step 39010, loss = 18.28 (3.2 examples/sec; 5.032 sec/batch)
2018-05-03 19:34:04.619128: step 39020, loss = 18.57 (3.3 examples/sec; 4.827 sec/batch)
2018-05-03 19:34:53.975633: step 39030, loss = 18.08 (3.1 examples/sec; 5.099 sec/batch)
2018-05-03 19:35:44.131989: step 39040, loss = 18.76 (3.3 examples/sec; 4.783 sec/batch)
2018-05-03 19:36:33.406235: step 39050, loss = 18.26 (3.2 examples/sec; 5.063 sec/batch)
2018-05-03 19:37:23.219225: step 39060, loss = 18.12 (3.1 examples/sec; 5.136 sec/batch)
2018-05-03 19:38:12.688112: step 39070, loss = 18.36 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 19:38:59.433880: step 39080, loss = 18.02 (3.2 examples/sec; 5.063 sec/batch)
2018-05-03 19:39:49.455959: step 39090, loss = 18.71 (3.2 examples/sec; 4.974 sec/batch)
2018-05-03 19:40:39.275682: step 39100, loss = 18.17 (3.2 examples/sec; 5.057 sec/batch)
2018-05-03 19:41:32.253420: step 39110, loss = 18.25 (3.2 examples/sec; 4.952 sec/batch)
2018-05-03 19:42:21.522395: step 39120, loss = 18.75 (3.4 examples/sec; 4.673 sec/batch)
2018-05-03 19:43:11.324799: step 39130, loss = 18.37 (3.2 examples/sec; 4.977 sec/batch)
2018-05-03 19:44:01.328824: step 39140, loss = 17.98 (3.1 examples/sec; 5.100 sec/batch)
2018-05-03 19:44:50.674909: step 39150, loss = 19.00 (3.2 examples/sec; 5.079 sec/batch)
2018-05-03 19:45:40.076739: step 39160, loss = 18.60 (3.3 examples/sec; 4.880 sec/batch)
2018-05-03 19:46:29.632475: step 39170, loss = 18.61 (3.3 examples/sec; 4.913 sec/batch)
2018-05-03 19:47:19.573538: step 39180, loss = 18.69 (3.3 examples/sec; 4.898 sec/batch)
2018-05-03 19:48:08.873367: step 39190, loss = 18.56 (3.3 examples/sec; 4.806 sec/batch)
2018-05-03 19:48:58.420608: step 39200, loss = 18.23 (3.1 examples/sec; 5.135 sec/batch)
2018-05-03 19:49:48.489139: step 39210, loss = 18.27 (3.4 examples/sec; 4.689 sec/batch)
2018-05-03 19:50:37.040959: step 39220, loss = 18.27 (3.2 examples/sec; 5.036 sec/batch)
2018-05-03 19:51:27.071567: step 39230, loss = 18.12 (3.3 examples/sec; 4.831 sec/batch)
2018-05-03 19:52:16.227816: step 39240, loss = 18.41 (3.3 examples/sec; 4.894 sec/batch)
2018-05-03 19:53:06.189790: step 39250, loss = 17.88 (3.2 examples/sec; 4.979 sec/batch)
2018-05-03 19:53:55.892361: step 39260, loss = 18.15 (3.1 examples/sec; 5.154 sec/batch)
2018-05-03 19:54:45.534992: step 39270, loss = 18.00 (3.2 examples/sec; 5.044 sec/batch)
2018-05-03 19:55:35.385202: step 39280, loss = 18.13 (3.1 examples/sec; 5.121 sec/batch)
2018-05-03 19:56:24.413000: step 39290, loss = 18.38 (3.4 examples/sec; 4.759 sec/batch)
2018-05-03 19:57:13.617698: step 39300, loss = 17.78 (3.3 examples/sec; 4.829 sec/batch)
2018-05-03 19:58:06.573472: step 39310, loss = 18.37 (3.2 examples/sec; 5.061 sec/batch)
2018-05-03 19:58:55.973090: step 39320, loss = 18.17 (3.2 examples/sec; 5.025 sec/batch)
2018-05-03 19:59:42.467040: step 39330, loss = 18.66 (3.3 examples/sec; 4.908 sec/batch)
2018-05-03 20:00:32.289187: step 39340, loss = 18.23 (3.2 examples/sec; 4.941 sec/batch)
2018-05-03 20:01:21.635444: step 39350, loss = 18.28 (3.3 examples/sec; 4.899 sec/batch)
2018-05-03 20:02:11.028211: step 39360, loss = 18.94 (3.1 examples/sec; 5.216 sec/batch)
2018-05-03 20:03:00.457697: step 39370, loss = 18.96 (3.3 examples/sec; 4.786 sec/batch)
2018-05-03 20:03:50.609937: step 39380, loss = 18.03 (3.1 examples/sec; 5.097 sec/batch)
2018-05-03 20:04:40.401259: step 39390, loss = 17.89 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 20:05:29.559198: step 39400, loss = 18.25 (3.3 examples/sec; 4.868 sec/batch)
2018-05-03 20:06:22.699881: step 39410, loss = 18.02 (3.2 examples/sec; 5.022 sec/batch)
2018-05-03 20:07:12.199381: step 39420, loss = 18.28 (3.2 examples/sec; 5.010 sec/batch)
2018-05-03 20:08:02.653285: step 39430, loss = 18.51 (3.2 examples/sec; 5.059 sec/batch)
2018-05-03 20:08:52.399667: step 39440, loss = 18.10 (3.2 examples/sec; 4.955 sec/batch)
2018-05-03 20:09:39.788210: step 39450, loss = 18.83 (4.2 examples/sec; 3.823 sec/batch)
2018-05-03 20:10:29.202961: step 39460, loss = 18.20 (3.2 examples/sec; 4.942 sec/batch)
2018-05-03 20:11:19.229897: step 39470, loss = 18.73 (3.3 examples/sec; 4.854 sec/batch)
2018-05-03 20:12:08.574719: step 39480, loss = 19.18 (3.3 examples/sec; 4.822 sec/batch)
2018-05-03 20:12:57.727037: step 39490, loss = 19.38 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 20:13:47.113446: step 39500, loss = 18.68 (3.2 examples/sec; 4.930 sec/batch)
2018-05-03 20:14:40.224089: step 39510, loss = 18.56 (3.3 examples/sec; 4.826 sec/batch)
2018-05-03 20:15:30.261949: step 39520, loss = 18.19 (3.1 examples/sec; 5.097 sec/batch)
2018-05-03 20:16:19.688311: step 39530, loss = 18.13 (3.2 examples/sec; 5.019 sec/batch)
2018-05-03 20:17:09.808654: step 39540, loss = 18.87 (3.1 examples/sec; 5.117 sec/batch)
2018-05-03 20:17:58.759989: step 39550, loss = 18.33 (3.2 examples/sec; 4.960 sec/batch)
2018-05-03 20:18:48.621329: step 39560, loss = 17.99 (3.3 examples/sec; 4.915 sec/batch)
2018-05-03 20:19:38.296474: step 39570, loss = 18.54 (3.4 examples/sec; 4.730 sec/batch)
2018-05-03 20:20:24.854184: step 39580, loss = 17.99 (3.2 examples/sec; 5.040 sec/batch)
2018-05-03 20:21:14.823851: step 39590, loss = 18.30 (3.3 examples/sec; 4.837 sec/batch)
2018-05-03 20:22:04.330777: step 39600, loss = 18.43 (3.3 examples/sec; 4.895 sec/batch)
2018-05-03 20:22:56.803280: step 39610, loss = 18.15 (3.3 examples/sec; 4.871 sec/batch)
2018-05-03 20:23:45.839165: step 39620, loss = 18.35 (3.5 examples/sec; 4.586 sec/batch)
2018-05-03 20:24:35.487054: step 39630, loss = 18.32 (3.2 examples/sec; 4.931 sec/batch)
2018-05-03 20:25:25.126347: step 39640, loss = 18.58 (3.3 examples/sec; 4.913 sec/batch)
2018-05-03 20:26:14.209867: step 39650, loss = 18.63 (3.2 examples/sec; 5.030 sec/batch)
2018-05-03 20:27:03.900161: step 39660, loss = 18.36 (3.1 examples/sec; 5.151 sec/batch)
2018-05-03 20:27:53.132350: step 39670, loss = 18.15 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 20:28:43.038307: step 39680, loss = 18.27 (3.2 examples/sec; 5.012 sec/batch)
2018-05-03 20:29:32.314543: step 39690, loss = 18.39 (3.3 examples/sec; 4.818 sec/batch)
2018-05-03 20:30:19.416370: step 39700, loss = 18.06 (4.2 examples/sec; 3.824 sec/batch)
2018-05-03 20:31:11.762036: step 39710, loss = 17.95 (3.3 examples/sec; 4.907 sec/batch)
2018-05-03 20:32:01.493703: step 39720, loss = 18.38 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 20:32:50.980919: step 39730, loss = 18.39 (3.2 examples/sec; 4.947 sec/batch)
2018-05-03 20:33:40.524916: step 39740, loss = 18.53 (3.2 examples/sec; 5.002 sec/batch)
2018-05-03 20:34:30.413207: step 39750, loss = 19.04 (3.2 examples/sec; 4.958 sec/batch)
2018-05-03 20:35:19.710294: step 39760, loss = 18.14 (3.3 examples/sec; 4.903 sec/batch)
2018-05-03 20:36:09.152911: step 39770, loss = 18.54 (3.3 examples/sec; 4.897 sec/batch)
2018-05-03 20:36:58.293767: step 39780, loss = 18.33 (3.2 examples/sec; 5.004 sec/batch)
2018-05-03 20:37:46.896982: step 39790, loss = 17.91 (3.2 examples/sec; 5.029 sec/batch)
2018-05-03 20:38:36.331224: step 39800, loss = 18.22 (3.2 examples/sec; 4.967 sec/batch)
2018-05-03 20:39:29.453771: step 39810, loss = 18.01 (3.2 examples/sec; 4.943 sec/batch)
2018-05-03 20:40:18.658718: step 39820, loss = 18.52 (3.3 examples/sec; 4.834 sec/batch)
2018-05-03 20:41:04.773932: step 39830, loss = 18.50 (3.2 examples/sec; 4.929 sec/batch)
2018-05-03 20:41:53.837225: step 39840, loss = 18.15 (3.2 examples/sec; 5.049 sec/batch)
2018-05-03 20:42:43.299060: step 39850, loss = 18.26 (3.0 examples/sec; 5.276 sec/batch)
2018-05-03 20:43:32.792770: step 39860, loss = 18.78 (3.2 examples/sec; 5.001 sec/batch)
2018-05-03 20:44:22.422813: step 39870, loss = 18.92 (3.0 examples/sec; 5.249 sec/batch)
2018-05-03 20:45:11.901841: step 39880, loss = 18.15 (3.3 examples/sec; 4.777 sec/batch)
2018-05-03 20:46:01.313887: step 39890, loss = 18.22 (3.1 examples/sec; 5.132 sec/batch)
2018-05-03 20:46:50.237529: step 39900, loss = 18.45 (3.3 examples/sec; 4.790 sec/batch)
2018-05-03 20:47:42.722159: step 39910, loss = 18.91 (3.4 examples/sec; 4.735 sec/batch)
2018-05-03 20:48:31.973002: step 39920, loss = 18.49 (3.3 examples/sec; 4.886 sec/batch)
2018-05-03 20:49:21.236821: step 39930, loss = 18.01 (3.3 examples/sec; 4.849 sec/batch)
2018-05-03 20:50:10.918969: step 39940, loss = 18.14 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 20:50:59.254992: step 39950, loss = 18.14 (4.2 examples/sec; 3.854 sec/batch)
2018-05-03 20:51:46.347475: step 39960, loss = 18.66 (3.3 examples/sec; 4.845 sec/batch)
2018-05-03 20:52:35.969147: step 39970, loss = 18.27 (3.3 examples/sec; 4.914 sec/batch)
2018-05-03 20:53:25.850460: step 39980, loss = 18.14 (3.1 examples/sec; 5.124 sec/batch)
2018-05-03 20:54:15.261564: step 39990, loss = 19.07 (3.2 examples/sec; 4.947 sec/batch)
2018-05-03 20:55:05.028598: step 40000, loss = 18.46 (3.3 examples/sec; 4.850 sec/batch)
2018-05-03 20:55:57.506241: step 40010, loss = 18.12 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 20:56:47.337679: step 40020, loss = 18.56 (3.2 examples/sec; 5.024 sec/batch)
2018-05-03 20:57:36.578126: step 40030, loss = 17.94 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 20:58:25.683436: step 40040, loss = 17.93 (3.2 examples/sec; 4.979 sec/batch)
2018-05-03 20:59:15.014280: step 40050, loss = 18.21 (3.2 examples/sec; 5.014 sec/batch)
2018-05-03 21:00:04.680687: step 40060, loss = 18.89 (3.2 examples/sec; 4.983 sec/batch)
2018-05-03 21:00:53.739522: step 40070, loss = 17.94 (3.3 examples/sec; 4.865 sec/batch)
2018-05-03 21:01:39.445832: step 40080, loss = 18.56 (3.3 examples/sec; 4.894 sec/batch)
2018-05-03 21:02:28.926227: step 40090, loss = 18.50 (3.1 examples/sec; 5.102 sec/batch)
2018-05-03 21:03:18.052331: step 40100, loss = 18.32 (3.3 examples/sec; 4.882 sec/batch)
2018-05-03 21:04:11.380257: step 40110, loss = 18.38 (3.2 examples/sec; 5.051 sec/batch)
2018-05-03 21:05:01.057114: step 40120, loss = 18.73 (3.2 examples/sec; 4.924 sec/batch)
2018-05-03 21:05:51.321707: step 40130, loss = 18.56 (3.2 examples/sec; 4.989 sec/batch)
2018-05-03 21:06:41.344105: step 40140, loss = 17.91 (3.3 examples/sec; 4.904 sec/batch)
2018-05-03 21:07:30.820676: step 40150, loss = 18.34 (3.2 examples/sec; 4.976 sec/batch)
2018-05-03 21:08:20.496072: step 40160, loss = 18.03 (3.2 examples/sec; 5.057 sec/batch)
2018-05-03 21:09:10.563799: step 40170, loss = 18.07 (3.2 examples/sec; 5.018 sec/batch)
2018-05-03 21:10:00.265074: step 40180, loss = 17.98 (3.3 examples/sec; 4.886 sec/batch)
2018-05-03 21:10:49.309963: step 40190, loss = 18.25 (3.4 examples/sec; 4.693 sec/batch)
2018-05-03 21:11:37.159639: step 40200, loss = 18.06 (4.1 examples/sec; 3.884 sec/batch)
2018-05-03 21:12:28.240998: step 40210, loss = 18.11 (3.3 examples/sec; 4.856 sec/batch)
2018-05-03 21:13:16.906040: step 40220, loss = 18.46 (3.3 examples/sec; 4.796 sec/batch)
2018-05-03 21:14:06.064862: step 40230, loss = 18.05 (3.3 examples/sec; 4.879 sec/batch)
2018-05-03 21:14:54.975085: step 40240, loss = 17.96 (3.2 examples/sec; 4.939 sec/batch)
2018-05-03 21:15:44.396126: step 40250, loss = 18.10 (3.2 examples/sec; 5.009 sec/batch)
2018-05-03 21:16:34.009020: step 40260, loss = 18.16 (3.2 examples/sec; 4.962 sec/batch)
2018-05-03 21:17:23.313819: step 40270, loss = 18.74 (3.2 examples/sec; 5.001 sec/batch)
2018-05-03 21:18:12.105890: step 40280, loss = 18.46 (3.2 examples/sec; 4.941 sec/batch)
2018-05-03 21:19:01.338999: step 40290, loss = 18.20 (3.3 examples/sec; 4.852 sec/batch)
2018-05-03 21:19:50.662485: step 40300, loss = 18.79 (3.2 examples/sec; 4.939 sec/batch)
2018-05-03 21:20:43.390189: step 40310, loss = 18.46 (3.3 examples/sec; 4.778 sec/batch)
2018-05-03 21:21:32.939872: step 40320, loss = 18.53 (3.2 examples/sec; 4.946 sec/batch)
2018-05-03 21:22:18.903228: step 40330, loss = 19.34 (3.3 examples/sec; 4.897 sec/batch)
2018-05-03 21:23:08.387968: step 40340, loss = 18.16 (3.2 examples/sec; 4.950 sec/batch)
2018-05-03 21:23:57.569331: step 40350, loss = 19.08 (3.3 examples/sec; 4.877 sec/batch)
2018-05-03 21:24:47.073991: step 40360, loss = 18.84 (3.3 examples/sec; 4.921 sec/batch)
2018-05-03 21:25:35.715268: step 40370, loss = 18.32 (3.3 examples/sec; 4.866 sec/batch)
2018-05-03 21:26:24.174598: step 40380, loss = 18.74 (3.3 examples/sec; 4.848 sec/batch)
2018-05-03 21:27:13.334497: step 40390, loss = 18.55 (3.3 examples/sec; 4.891 sec/batch)
2018-05-03 21:28:02.568067: step 40400, loss = 18.09 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 21:28:55.805933: step 40410, loss = 18.10 (3.2 examples/sec; 4.965 sec/batch)
2018-05-03 21:29:45.060645: step 40420, loss = 18.51 (3.2 examples/sec; 5.040 sec/batch)
2018-05-03 21:30:33.870175: step 40430, loss = 19.20 (3.3 examples/sec; 4.830 sec/batch)
2018-05-03 21:31:24.066853: step 40440, loss = 18.33 (3.2 examples/sec; 5.003 sec/batch)
2018-05-03 21:32:13.356530: step 40450, loss = 19.17 (3.3 examples/sec; 4.802 sec/batch)
2018-05-03 21:32:59.648611: step 40460, loss = 18.35 (3.2 examples/sec; 4.925 sec/batch)
2018-05-03 21:33:49.399017: step 40470, loss = 18.18 (3.2 examples/sec; 4.946 sec/batch)
2018-05-03 21:34:38.931749: step 40480, loss = 18.04 (3.3 examples/sec; 4.902 sec/batch)
2018-05-03 21:35:28.744072: step 40490, loss = 18.24 (3.2 examples/sec; 5.031 sec/batch)
2018-05-03 21:36:18.833949: step 40500, loss = 18.30 (3.1 examples/sec; 5.090 sec/batch)
2018-05-03 21:37:11.599412: step 40510, loss = 18.88 (3.2 examples/sec; 4.988 sec/batch)
2018-05-03 21:38:01.392997: step 40520, loss = 18.93 (3.3 examples/sec; 4.920 sec/batch)
2018-05-03 21:38:50.453940: step 40530, loss = 18.11 (3.3 examples/sec; 4.876 sec/batch)
2018-05-03 21:39:40.067105: step 40540, loss = 18.08 (3.4 examples/sec; 4.718 sec/batch)
2018-05-03 21:40:29.495685: step 40550, loss = 18.43 (3.3 examples/sec; 4.917 sec/batch)
2018-05-03 21:41:19.108486: step 40560, loss = 18.12 (3.2 examples/sec; 5.055 sec/batch)
2018-05-03 21:42:08.566261: step 40570, loss = 18.20 (3.2 examples/sec; 4.983 sec/batch)
2018-05-03 21:42:54.412549: step 40580, loss = 18.22 (3.3 examples/sec; 4.820 sec/batch)
2018-05-03 21:43:44.322901: step 40590, loss = 18.24 (3.2 examples/sec; 4.924 sec/batch)
2018-05-03 21:44:33.857088: step 40600, loss = 18.11 (3.3 examples/sec; 4.920 sec/batch)
2018-05-03 21:45:26.476147: step 40610, loss = 17.92 (3.2 examples/sec; 4.940 sec/batch)
2018-05-03 21:46:15.537650: step 40620, loss = 18.63 (3.3 examples/sec; 4.893 sec/batch)
2018-05-03 21:47:04.865831: step 40630, loss = 18.51 (3.2 examples/sec; 4.926 sec/batch)
2018-05-03 21:47:54.265165: step 40640, loss = 18.38 (3.2 examples/sec; 4.958 sec/batch)
2018-05-03 21:48:43.677012: step 40650, loss = 18.61 (3.2 examples/sec; 4.930 sec/batch)
2018-05-03 21:49:32.482340: step 40660, loss = 18.43 (3.3 examples/sec; 4.908 sec/batch)
2018-05-03 21:50:21.793331: step 40670, loss = 18.72 (3.1 examples/sec; 5.106 sec/batch)
2018-05-03 21:51:11.223219: step 40680, loss = 18.31 (3.3 examples/sec; 4.920 sec/batch)
2018-05-03 21:52:00.701758: step 40690, loss = 18.54 (3.2 examples/sec; 5.028 sec/batch)
2018-05-03 21:52:48.691091: step 40700, loss = 18.14 (4.2 examples/sec; 3.831 sec/batch)
2018-05-03 21:53:39.806348: step 40710, loss = 18.34 (3.3 examples/sec; 4.828 sec/batch)
2018-05-03 21:54:29.238106: step 40720, loss = 18.32 (3.2 examples/sec; 5.025 sec/batch)
2018-05-03 21:55:18.878790: step 40730, loss = 18.40 (3.3 examples/sec; 4.835 sec/batch)
2018-05-03 21:56:07.717409: step 40740, loss = 18.30 (3.2 examples/sec; 4.934 sec/batch)
2018-05-03 21:56:56.601466: step 40750, loss = 18.41 (3.3 examples/sec; 4.864 sec/batch)
2018-05-03 21:57:46.492653: step 40760, loss = 18.07 (3.0 examples/sec; 5.246 sec/batch)
2018-05-03 21:58:35.887683: step 40770, loss = 18.21 (3.2 examples/sec; 4.940 sec/batch)
2018-05-03 21:59:24.936924: step 40780, loss = 18.10 (3.2 examples/sec; 4.938 sec/batch)
2018-05-03 22:00:14.652871: step 40790, loss = 18.29 (3.3 examples/sec; 4.873 sec/batch)
2018-05-03 22:01:04.313397: step 40800, loss = 18.88 (3.3 examples/sec; 4.910 sec/batch)
2018-05-03 22:01:56.501438: step 40810, loss = 18.36 (3.3 examples/sec; 4.840 sec/batch)
2018-05-03 22:02:46.466891: step 40820, loss = 18.26 (3.1 examples/sec; 5.100 sec/batch)
2018-05-03 22:03:33.134156: step 40830, loss = 18.44 (3.2 examples/sec; 5.001 sec/batch)
2018-05-03 22:04:22.599175: step 40840, loss = 18.42 (3.3 examples/sec; 4.867 sec/batch)
2018-05-03 22:05:11.865443: step 40850, loss = 18.00 (3.3 examples/sec; 4.918 sec/batch)
2018-05-03 22:06:01.273423: step 40860, loss = 18.88 (3.2 examples/sec; 4.951 sec/batch)
2018-05-03 22:06:51.140306: step 40870, loss = 18.12 (3.2 examples/sec; 5.073 sec/batch)
2018-05-03 22:07:41.030274: step 40880, loss = 18.05 (3.2 examples/sec; 4.927 sec/batch)
2018-05-03 22:08:29.776464: step 40890, loss = 18.92 (3.3 examples/sec; 4.868 sec/batch)
2018-05-03 22:09:19.227440: step 40900, loss = 18.40 (3.3 examples/sec; 4.882 sec/batch)
2018-05-03 22:10:11.851930: step 40910, loss = 17.83 (3.2 examples/sec; 4.943 sec/batch)
2018-05-03 22:11:00.895523: step 40920, loss = 18.47 (3.2 examples/sec; 4.989 sec/batch)
2018-05-03 22:11:49.997075: step 40930, loss = 18.69 (3.3 examples/sec; 4.891 sec/batch)
2018-05-03 22:12:38.803419: step 40940, loss = 18.44 (3.3 examples/sec; 4.783 sec/batch)
2018-05-03 22:13:27.031439: step 40950, loss = 18.68 (4.2 examples/sec; 3.785 sec/batch)
2018-05-03 22:14:15.448240: step 40960, loss = 18.24 (3.2 examples/sec; 4.941 sec/batch)
2018-05-03 22:15:04.396395: step 40970, loss = 18.85 (3.3 examples/sec; 4.871 sec/batch)
2018-05-03 22:15:53.848386: step 40980, loss = 18.26 (3.2 examples/sec; 4.970 sec/batch)
2018-05-03 22:16:43.153032: step 40990, loss = 18.48 (3.1 examples/sec; 5.095 sec/batch)
2018-05-03 22:17:32.034872: step 41000, loss = 18.22 (3.2 examples/sec; 4.946 sec/batch)
2018-05-03 22:18:24.460480: step 41010, loss = 18.43 (3.3 examples/sec; 4.858 sec/batch)
2018-05-03 22:19:13.634409: step 41020, loss = 18.10 (3.4 examples/sec; 4.710 sec/batch)
2018-05-03 22:20:02.813977: step 41030, loss = 18.12 (3.2 examples/sec; 4.933 sec/batch)
2018-05-03 22:20:52.226558: step 41040, loss = 17.92 (3.2 examples/sec; 4.982 sec/batch)
2018-05-03 22:21:41.916156: step 41050, loss = 18.43 (3.3 examples/sec; 4.827 sec/batch)
2018-05-03 22:22:31.471994: step 41060, loss = 18.28 (3.2 examples/sec; 4.938 sec/batch)
2018-05-03 22:23:20.960143: step 41070, loss = 18.01 (3.3 examples/sec; 4.915 sec/batch)
2018-05-03 22:24:07.314880: step 41080, loss = 18.09 (3.4 examples/sec; 4.756 sec/batch)
2018-05-03 22:24:57.343928: step 41090, loss = 18.80 (3.2 examples/sec; 4.964 sec/batch)
2018-05-03 22:25:47.413384: step 41100, loss = 18.43 (3.3 examples/sec; 4.920 sec/batch)
2018-05-03 22:26:40.352246: step 41110, loss = 18.42 (3.2 examples/sec; 4.947 sec/batch)
2018-05-03 22:27:29.857802: step 41120, loss = 17.96 (3.2 examples/sec; 4.999 sec/batch)
2018-05-03 22:28:19.456159: step 41130, loss = 18.19 (3.2 examples/sec; 4.969 sec/batch)
2018-05-03 22:29:08.932483: step 41140, loss = 18.22 (3.2 examples/sec; 5.021 sec/batch)
2018-05-03 22:29:58.813991: step 41150, loss = 18.75 (3.2 examples/sec; 5.015 sec/batch)
2018-05-03 22:30:47.764206: step 41160, loss = 18.10 (3.3 examples/sec; 4.822 sec/batch)
2018-05-03 22:31:37.229575: step 41170, loss = 18.43 (3.2 examples/sec; 5.021 sec/batch)
2018-05-03 22:32:26.557465: step 41180, loss = 18.20 (3.3 examples/sec; 4.842 sec/batch)
2018-05-03 22:33:16.134722: step 41190, loss = 18.73 (3.2 examples/sec; 4.964 sec/batch)
2018-05-03 22:34:04.027209: step 41200, loss = 17.93 (4.2 examples/sec; 3.804 sec/batch)
2018-05-03 22:34:56.591752: step 41210, loss = 18.42 (3.3 examples/sec; 4.795 sec/batch)
2018-05-03 22:35:45.334082: step 41220, loss = 17.98 (3.2 examples/sec; 4.969 sec/batch)
2018-05-03 22:36:34.853052: step 41230, loss = 18.59 (3.3 examples/sec; 4.912 sec/batch)
2018-05-03 22:37:23.806236: step 41240, loss = 20.04 (3.3 examples/sec; 4.909 sec/batch)
2018-05-03 22:38:13.487430: step 41250, loss = 18.17 (3.2 examples/sec; 5.022 sec/batch)
2018-05-03 22:39:02.490053: step 41260, loss = 18.04 (3.3 examples/sec; 4.859 sec/batch)
2018-05-03 22:39:51.525043: step 41270, loss = 18.04 (3.3 examples/sec; 4.892 sec/batch)
2018-05-03 22:40:40.444357: step 41280, loss = 18.35 (3.3 examples/sec; 4.879 sec/batch)
2018-05-03 22:41:29.778536: step 41290, loss = 18.46 (3.2 examples/sec; 4.993 sec/batch)
2018-05-03 22:42:18.935676: step 41300, loss = 18.43 (3.3 examples/sec; 4.921 sec/batch)
2018-05-03 22:43:12.128116: step 41310, loss = 18.19 (3.1 examples/sec; 5.113 sec/batch)
2018-05-03 22:44:01.195118: step 41320, loss = 17.90 (3.3 examples/sec; 4.866 sec/batch)
2018-05-03 22:44:47.940404: step 41330, loss = 18.47 (3.1 examples/sec; 5.141 sec/batch)
2018-05-03 22:45:37.616352: step 41340, loss = 18.37 (3.2 examples/sec; 4.973 sec/batch)
2018-05-03 22:46:26.825281: step 41350, loss = 17.99 (3.4 examples/sec; 4.759 sec/batch)
2018-05-03 22:47:15.615480: step 41360, loss = 18.14 (3.3 examples/sec; 4.838 sec/batch)
2018-05-03 22:48:05.174848: step 41370, loss = 18.57 (3.2 examples/sec; 5.060 sec/batch)
2018-05-03 22:48:54.901617: step 41380, loss = 18.15 (3.3 examples/sec; 4.859 sec/batch)
2018-05-03 22:49:44.752400: step 41390, loss = 18.19 (3.1 examples/sec; 5.112 sec/batch)
2018-05-03 22:50:33.911888: step 41400, loss = 18.48 (3.2 examples/sec; 4.948 sec/batch)
2018-05-03 22:51:26.753114: step 41410, loss = 18.38 (3.3 examples/sec; 4.896 sec/batch)
2018-05-03 22:52:15.635250: step 41420, loss = 18.85 (3.3 examples/sec; 4.852 sec/batch)
2018-05-03 22:53:04.853054: step 41430, loss = 18.46 (3.3 examples/sec; 4.896 sec/batch)
2018-05-03 22:53:54.980491: step 41440, loss = 18.17 (3.3 examples/sec; 4.870 sec/batch)
2018-05-03 22:54:42.435595: step 41450, loss = 18.27 (4.4 examples/sec; 3.673 sec/batch)
2018-05-03 22:55:31.128518: step 41460, loss = 18.91 (3.1 examples/sec; 5.123 sec/batch)
2018-05-03 22:56:20.680750: step 41470, loss = 18.11 (3.2 examples/sec; 5.017 sec/batch)
2018-05-03 22:57:10.268252: step 41480, loss = 18.16 (3.3 examples/sec; 4.781 sec/batch)
2018-05-03 22:57:59.671613: step 41490, loss = 18.23 (3.3 examples/sec; 4.893 sec/batch)
2018-05-03 22:58:48.679526: step 41500, loss = 17.96 (3.2 examples/sec; 4.993 sec/batch)
2018-05-03 22:59:41.609450: step 41510, loss = 18.07 (3.2 examples/sec; 4.997 sec/batch)
2018-05-03 23:00:30.847455: step 41520, loss = 18.62 (3.2 examples/sec; 4.963 sec/batch)
2018-05-03 23:01:19.852442: step 41530, loss = 17.95 (3.4 examples/sec; 4.755 sec/batch)
2018-05-03 23:02:09.500207: step 41540, loss = 18.61 (3.2 examples/sec; 5.022 sec/batch)
2018-05-03 23:02:59.483498: step 41550, loss = 18.90 (3.1 examples/sec; 5.083 sec/batch)
2018-05-03 23:03:48.857259: step 41560, loss = 17.96 (3.3 examples/sec; 4.909 sec/batch)
2018-05-03 23:04:38.908556: step 41570, loss = 18.17 (3.0 examples/sec; 5.291 sec/batch)
2018-05-03 23:05:24.927641: step 41580, loss = 18.15 (3.2 examples/sec; 4.935 sec/batch)
2018-05-03 23:06:14.962570: step 41590, loss = 18.46 (3.2 examples/sec; 4.930 sec/batch)
2018-05-03 23:07:04.817542: step 41600, loss = 18.50 (3.2 examples/sec; 4.934 sec/batch)
2018-05-03 23:07:58.622851: step 41610, loss = 18.07 (3.0 examples/sec; 5.403 sec/batch)
2018-05-03 23:08:47.757330: step 41620, loss = 17.92 (3.3 examples/sec; 4.908 sec/batch)
2018-05-03 23:09:36.910104: step 41630, loss = 18.51 (3.3 examples/sec; 4.829 sec/batch)
2018-05-03 23:10:26.484342: step 41640, loss = 19.18 (3.3 examples/sec; 4.862 sec/batch)
2018-05-03 23:11:15.790793: step 41650, loss = 18.72 (3.2 examples/sec; 4.926 sec/batch)
2018-05-03 23:12:05.569947: step 41660, loss = 19.22 (3.2 examples/sec; 5.009 sec/batch)
2018-05-03 23:12:54.622146: step 41670, loss = 18.26 (3.2 examples/sec; 5.024 sec/batch)
2018-05-03 23:13:44.825814: step 41680, loss = 18.39 (3.1 examples/sec; 5.137 sec/batch)
2018-05-03 23:14:33.874164: step 41690, loss = 17.92 (3.4 examples/sec; 4.715 sec/batch)
2018-05-03 23:15:20.755731: step 41700, loss = 18.40 (4.2 examples/sec; 3.816 sec/batch)
2018-05-03 23:16:13.900313: step 41710, loss = 18.11 (3.3 examples/sec; 4.881 sec/batch)
2018-05-03 23:17:02.720698: step 41720, loss = 18.78 (3.2 examples/sec; 5.036 sec/batch)
2018-05-03 23:17:52.259550: step 41730, loss = 18.21 (3.3 examples/sec; 4.874 sec/batch)
2018-05-03 23:18:41.771768: step 41740, loss = 18.29 (3.2 examples/sec; 5.054 sec/batch)
2018-05-03 23:19:31.499673: step 41750, loss = 18.34 (3.2 examples/sec; 4.959 sec/batch)
2018-05-03 23:20:21.316972: step 41760, loss = 18.05 (3.3 examples/sec; 4.875 sec/batch)
2018-05-03 23:21:11.021241: step 41770, loss = 18.59 (3.3 examples/sec; 4.852 sec/batch)
2018-05-03 23:22:00.805031: step 41780, loss = 18.58 (3.2 examples/sec; 5.053 sec/batch)
2018-05-03 23:22:49.809161: step 41790, loss = 18.34 (3.3 examples/sec; 4.895 sec/batch)
2018-05-03 23:23:40.250019: step 41800, loss = 18.30 (3.2 examples/sec; 4.988 sec/batch)
2018-05-03 23:24:32.964412: step 41810, loss = 18.23 (3.3 examples/sec; 4.904 sec/batch)
2018-05-03 23:25:22.609610: step 41820, loss = 18.41 (3.2 examples/sec; 5.005 sec/batch)
2018-05-03 23:26:09.458349: step 41830, loss = 18.85 (3.2 examples/sec; 5.042 sec/batch)
2018-05-03 23:26:58.982832: step 41840, loss = 17.89 (3.2 examples/sec; 4.960 sec/batch)
2018-05-03 23:27:48.495693: step 41850, loss = 18.14 (3.3 examples/sec; 4.892 sec/batch)
2018-05-03 23:28:38.828447: step 41860, loss = 18.34 (3.1 examples/sec; 5.172 sec/batch)
2018-05-03 23:29:28.360880: step 41870, loss = 18.11 (3.3 examples/sec; 4.876 sec/batch)
2018-05-03 23:30:17.439627: step 41880, loss = 18.27 (3.4 examples/sec; 4.774 sec/batch)
2018-05-03 23:31:06.113812: step 41890, loss = 17.99 (3.4 examples/sec; 4.772 sec/batch)
2018-05-03 23:31:56.380200: step 41900, loss = 18.28 (3.2 examples/sec; 5.024 sec/batch)
2018-05-03 23:32:49.722602: step 41910, loss = 18.24 (3.3 examples/sec; 4.855 sec/batch)
2018-05-03 23:33:39.290060: step 41920, loss = 17.97 (3.2 examples/sec; 4.988 sec/batch)
2018-05-03 23:34:28.454302: step 41930, loss = 18.39 (3.2 examples/sec; 5.011 sec/batch)
2018-05-03 23:35:17.520994: step 41940, loss = 18.01 (3.4 examples/sec; 4.761 sec/batch)
2018-05-03 23:36:04.233375: step 41950, loss = 19.15 (3.2 examples/sec; 5.056 sec/batch)
2018-05-03 23:36:54.104806: step 41960, loss = 17.95 (3.2 examples/sec; 4.988 sec/batch)
2018-05-03 23:37:43.638041: step 41970, loss = 18.41 (3.2 examples/sec; 4.946 sec/batch)
2018-05-03 23:38:32.662264: step 41980, loss = 18.00 (3.2 examples/sec; 4.991 sec/batch)
2018-05-03 23:39:21.307444: step 41990, loss = 18.39 (3.3 examples/sec; 4.806 sec/batch)
2018-05-03 23:40:10.448366: step 42000, loss = 19.15 (3.3 examples/sec; 4.815 sec/batch)
2018-05-03 23:41:03.299400: step 42010, loss = 18.22 (3.3 examples/sec; 4.892 sec/batch)
2018-05-03 23:41:52.505673: step 42020, loss = 18.65 (3.2 examples/sec; 4.940 sec/batch)
2018-05-03 23:42:42.249693: step 42030, loss = 18.60 (3.2 examples/sec; 5.007 sec/batch)
2018-05-03 23:43:31.754553: step 42040, loss = 18.30 (3.3 examples/sec; 4.878 sec/batch)
2018-05-03 23:44:21.168363: step 42050, loss = 18.36 (3.2 examples/sec; 5.050 sec/batch)
2018-05-03 23:45:11.053581: step 42060, loss = 17.96 (3.3 examples/sec; 4.820 sec/batch)
2018-05-03 23:45:59.192278: step 42070, loss = 18.23 (4.3 examples/sec; 3.699 sec/batch)
2018-05-03 23:46:47.151626: step 42080, loss = 18.10 (3.2 examples/sec; 5.012 sec/batch)
2018-05-03 23:47:36.530998: step 42090, loss = 18.15 (3.1 examples/sec; 5.158 sec/batch)
2018-05-03 23:48:25.684143: step 42100, loss = 18.30 (3.3 examples/sec; 4.778 sec/batch)
2018-05-03 23:49:18.309074: step 42110, loss = 18.03 (3.2 examples/sec; 5.017 sec/batch)
2018-05-03 23:50:07.790032: step 42120, loss = 18.30 (3.1 examples/sec; 5.108 sec/batch)
2018-05-03 23:50:56.270878: step 42130, loss = 18.16 (3.3 examples/sec; 4.849 sec/batch)
2018-05-03 23:51:46.212698: step 42140, loss = 18.11 (3.3 examples/sec; 4.880 sec/batch)
2018-05-03 23:52:35.648519: step 42150, loss = 17.98 (3.2 examples/sec; 5.028 sec/batch)
2018-05-03 23:53:24.732864: step 42160, loss = 18.24 (3.2 examples/sec; 4.930 sec/batch)
2018-05-03 23:54:13.933593: step 42170, loss = 18.02 (3.2 examples/sec; 5.051 sec/batch)
2018-05-03 23:55:03.430838: step 42180, loss = 18.20 (3.4 examples/sec; 4.766 sec/batch)
2018-05-03 23:55:52.291792: step 42190, loss = 19.09 (3.2 examples/sec; 4.988 sec/batch)
2018-05-03 23:56:38.079942: step 42200, loss = 19.11 (3.3 examples/sec; 4.905 sec/batch)
2018-05-03 23:57:30.952920: step 42210, loss = 19.13 (3.3 examples/sec; 4.870 sec/batch)
2018-05-03 23:58:19.580073: step 42220, loss = 18.41 (3.2 examples/sec; 4.982 sec/batch)
2018-05-03 23:59:08.501115: step 42230, loss = 18.83 (3.2 examples/sec; 4.934 sec/batch)
2018-05-03 23:59:57.532321: step 42240, loss = 18.37 (3.3 examples/sec; 4.866 sec/batch)
2018-05-04 00:00:46.843888: step 42250, loss = 18.66 (3.2 examples/sec; 4.974 sec/batch)
2018-05-04 00:01:35.969575: step 42260, loss = 18.06 (3.2 examples/sec; 4.947 sec/batch)
2018-05-04 00:02:25.155573: step 42270, loss = 18.17 (3.3 examples/sec; 4.782 sec/batch)
2018-05-04 00:03:14.483516: step 42280, loss = 19.59 (3.2 examples/sec; 5.032 sec/batch)
2018-05-04 00:04:03.976415: step 42290, loss = 18.44 (3.3 examples/sec; 4.895 sec/batch)
2018-05-04 00:04:53.969649: step 42300, loss = 18.16 (3.2 examples/sec; 5.019 sec/batch)
2018-05-04 00:05:47.071195: step 42310, loss = 18.08 (3.2 examples/sec; 4.946 sec/batch)
2018-05-04 00:06:34.309915: step 42320, loss = 18.45 (4.2 examples/sec; 3.785 sec/batch)
2018-05-04 00:07:22.298362: step 42330, loss = 18.53 (3.2 examples/sec; 4.937 sec/batch)
2018-05-04 00:08:12.614243: step 42340, loss = 18.25 (3.3 examples/sec; 4.825 sec/batch)
2018-05-04 00:09:03.278208: step 42350, loss = 18.48 (3.2 examples/sec; 4.971 sec/batch)
2018-05-04 00:09:52.616659: step 42360, loss = 18.67 (3.3 examples/sec; 4.851 sec/batch)
2018-05-04 00:10:42.080278: step 42370, loss = 18.68 (3.2 examples/sec; 5.008 sec/batch)
2018-05-04 00:11:31.820399: step 42380, loss = 18.12 (3.2 examples/sec; 4.924 sec/batch)
2018-05-04 00:12:21.386787: step 42390, loss = 18.80 (3.3 examples/sec; 4.898 sec/batch)
2018-05-04 00:13:11.613802: step 42400, loss = 18.36 (3.2 examples/sec; 5.046 sec/batch)
2018-05-04 00:14:04.805587: step 42410, loss = 18.10 (3.3 examples/sec; 4.847 sec/batch)
2018-05-04 00:14:54.153479: step 42420, loss = 18.05 (3.1 examples/sec; 5.118 sec/batch)
2018-05-04 00:15:44.292734: step 42430, loss = 18.16 (3.2 examples/sec; 5.060 sec/batch)
2018-05-04 00:16:34.125450: step 42440, loss = 18.09 (3.1 examples/sec; 5.130 sec/batch)
2018-05-04 00:17:20.562769: step 42450, loss = 17.95 (3.2 examples/sec; 4.975 sec/batch)
2018-05-04 00:18:10.248998: step 42460, loss = 18.27 (3.2 examples/sec; 4.964 sec/batch)
2018-05-04 00:18:59.483146: step 42470, loss = 18.35 (3.1 examples/sec; 5.101 sec/batch)
2018-05-04 00:19:48.863564: step 42480, loss = 18.07 (3.3 examples/sec; 4.901 sec/batch)
2018-05-04 00:20:38.702445: step 42490, loss = 18.06 (3.3 examples/sec; 4.780 sec/batch)
2018-05-04 00:21:27.483382: step 42500, loss = 18.74 (3.3 examples/sec; 4.818 sec/batch)
2018-05-04 00:22:20.317566: step 42510, loss = 18.84 (3.4 examples/sec; 4.766 sec/batch)
2018-05-04 00:23:09.764363: step 42520, loss = 17.96 (3.2 examples/sec; 5.047 sec/batch)
2018-05-04 00:23:58.491475: step 42530, loss = 18.27 (3.2 examples/sec; 5.056 sec/batch)
2018-05-04 00:24:48.176027: step 42540, loss = 18.23 (3.1 examples/sec; 5.142 sec/batch)
2018-05-04 00:25:37.759042: step 42550, loss = 18.97 (3.2 examples/sec; 5.030 sec/batch)
2018-05-04 00:26:27.252067: step 42560, loss = 17.98 (3.2 examples/sec; 4.945 sec/batch)
2018-05-04 00:27:14.213060: step 42570, loss = 18.74 (3.2 examples/sec; 4.997 sec/batch)
2018-05-04 00:28:04.048348: step 42580, loss = 18.39 (3.3 examples/sec; 4.784 sec/batch)
2018-05-04 00:28:54.124069: step 42590, loss = 18.31 (3.3 examples/sec; 4.894 sec/batch)
2018-05-04 00:29:44.517161: step 42600, loss = 17.90 (3.1 examples/sec; 5.122 sec/batch)
2018-05-04 00:30:37.335854: step 42610, loss = 18.48 (3.3 examples/sec; 4.804 sec/batch)
2018-05-04 00:31:27.133342: step 42620, loss = 17.77 (3.2 examples/sec; 4.964 sec/batch)
2018-05-04 00:32:16.537860: step 42630, loss = 18.76 (3.3 examples/sec; 4.819 sec/batch)
2018-05-04 00:33:05.785617: step 42640, loss = 18.31 (3.2 examples/sec; 5.055 sec/batch)
2018-05-04 00:33:54.907237: step 42650, loss = 18.31 (3.3 examples/sec; 4.818 sec/batch)
2018-05-04 00:34:44.388992: step 42660, loss = 18.36 (3.2 examples/sec; 5.043 sec/batch)
2018-05-04 00:35:34.127884: step 42670, loss = 18.27 (3.2 examples/sec; 4.931 sec/batch)
2018-05-04 00:36:23.525894: step 42680, loss = 18.48 (3.3 examples/sec; 4.874 sec/batch)
2018-05-04 00:37:12.032628: step 42690, loss = 19.13 (4.2 examples/sec; 3.771 sec/batch)
2018-05-04 00:37:59.117782: step 42700, loss = 18.47 (3.3 examples/sec; 4.843 sec/batch)
2018-05-04 00:38:52.145649: step 42710, loss = 18.46 (3.1 examples/sec; 5.138 sec/batch)
2018-05-04 00:39:41.437523: step 42720, loss = 18.47 (3.2 examples/sec; 4.975 sec/batch)
2018-05-04 00:40:30.952189: step 42730, loss = 18.29 (3.3 examples/sec; 4.889 sec/batch)
2018-05-04 00:41:20.275851: step 42740, loss = 18.10 (3.2 examples/sec; 4.955 sec/batch)
2018-05-04 00:42:10.014557: step 42750, loss = 18.10 (3.2 examples/sec; 4.991 sec/batch)
2018-05-04 00:42:59.473728: step 42760, loss = 17.94 (3.1 examples/sec; 5.087 sec/batch)
2018-05-04 00:43:48.541442: step 42770, loss = 18.99 (3.2 examples/sec; 4.953 sec/batch)
2018-05-04 00:44:38.280186: step 42780, loss = 18.28 (3.2 examples/sec; 5.023 sec/batch)
2018-05-04 00:45:27.603744: step 42790, loss = 17.96 (3.3 examples/sec; 4.839 sec/batch)
2018-05-04 00:46:17.058780: step 42800, loss = 18.48 (3.3 examples/sec; 4.830 sec/batch)
2018-05-04 00:47:09.691099: step 42810, loss = 18.00 (3.3 examples/sec; 4.813 sec/batch)
2018-05-04 00:47:56.834746: step 42820, loss = 18.45 (3.1 examples/sec; 5.093 sec/batch)
2018-05-04 00:48:46.107609: step 42830, loss = 18.51 (3.2 examples/sec; 5.041 sec/batch)
2018-05-04 00:49:35.908976: step 42840, loss = 17.97 (3.2 examples/sec; 4.952 sec/batch)
2018-05-04 00:50:25.775701: step 42850, loss = 18.32 (3.3 examples/sec; 4.908 sec/batch)
2018-05-04 00:51:15.309020: step 42860, loss = 18.25 (3.3 examples/sec; 4.852 sec/batch)
2018-05-04 00:52:04.631999: step 42870, loss = 18.06 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 00:52:53.569571: step 42880, loss = 18.01 (3.3 examples/sec; 4.784 sec/batch)
2018-05-04 00:53:42.795216: step 42890, loss = 18.29 (3.2 examples/sec; 5.013 sec/batch)
2018-05-04 00:54:31.447820: step 42900, loss = 18.77 (3.3 examples/sec; 4.811 sec/batch)
2018-05-04 00:55:24.711064: step 42910, loss = 18.02 (3.1 examples/sec; 5.153 sec/batch)
2018-05-04 00:56:13.563196: step 42920, loss = 17.86 (3.4 examples/sec; 4.694 sec/batch)
2018-05-04 00:57:02.462893: step 42930, loss = 18.49 (3.4 examples/sec; 4.695 sec/batch)
2018-05-04 00:57:48.480535: step 42940, loss = 18.05 (3.7 examples/sec; 4.312 sec/batch)
2018-05-04 00:58:38.509692: step 42950, loss = 18.14 (3.2 examples/sec; 5.065 sec/batch)
2018-05-04 00:59:27.977718: step 42960, loss = 18.43 (3.2 examples/sec; 4.995 sec/batch)
2018-05-04 01:00:17.049349: step 42970, loss = 18.37 (3.3 examples/sec; 4.795 sec/batch)
2018-05-04 01:01:06.092905: step 42980, loss = 18.57 (3.3 examples/sec; 4.918 sec/batch)
2018-05-04 01:01:54.832317: step 42990, loss = 18.09 (3.5 examples/sec; 4.601 sec/batch)
2018-05-04 01:02:44.272075: step 43000, loss = 17.88 (3.2 examples/sec; 4.929 sec/batch)
2018-05-04 01:03:36.709069: step 43010, loss = 18.35 (3.3 examples/sec; 4.846 sec/batch)
2018-05-04 01:04:26.261242: step 43020, loss = 18.11 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 01:05:16.153205: step 43030, loss = 17.97 (3.2 examples/sec; 5.030 sec/batch)
2018-05-04 01:06:05.183828: step 43040, loss = 18.53 (3.4 examples/sec; 4.747 sec/batch)
2018-05-04 01:06:54.725553: step 43050, loss = 18.16 (3.3 examples/sec; 4.876 sec/batch)
2018-05-04 01:07:43.821405: step 43060, loss = 18.38 (3.3 examples/sec; 4.880 sec/batch)
2018-05-04 01:08:30.634010: step 43070, loss = 18.30 (3.2 examples/sec; 5.053 sec/batch)
2018-05-04 01:09:19.968915: step 43080, loss = 18.26 (3.2 examples/sec; 4.983 sec/batch)
2018-05-04 01:10:08.968827: step 43090, loss = 18.15 (3.2 examples/sec; 4.996 sec/batch)
2018-05-04 01:10:58.346543: step 43100, loss = 18.33 (3.2 examples/sec; 4.964 sec/batch)
2018-05-04 01:11:51.522791: step 43110, loss = 18.32 (3.2 examples/sec; 5.056 sec/batch)
2018-05-04 01:12:40.461300: step 43120, loss = 17.89 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 01:13:29.506511: step 43130, loss = 17.90 (3.2 examples/sec; 5.011 sec/batch)
2018-05-04 01:14:18.464816: step 43140, loss = 17.97 (3.3 examples/sec; 4.854 sec/batch)
2018-05-04 01:15:07.902899: step 43150, loss = 18.20 (3.2 examples/sec; 4.974 sec/batch)
2018-05-04 01:15:57.236169: step 43160, loss = 18.01 (3.2 examples/sec; 5.020 sec/batch)
2018-05-04 01:16:47.032825: step 43170, loss = 18.49 (3.3 examples/sec; 4.783 sec/batch)
2018-05-04 01:17:36.623449: step 43180, loss = 17.93 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 01:18:22.406738: step 43190, loss = 18.10 (4.2 examples/sec; 3.852 sec/batch)
2018-05-04 01:19:11.911349: step 43200, loss = 18.74 (3.3 examples/sec; 4.886 sec/batch)
2018-05-04 01:20:04.574127: step 43210, loss = 18.72 (3.3 examples/sec; 4.886 sec/batch)
2018-05-04 01:20:53.839839: step 43220, loss = 18.80 (3.3 examples/sec; 4.879 sec/batch)
2018-05-04 01:21:43.402889: step 43230, loss = 18.18 (3.1 examples/sec; 5.156 sec/batch)
2018-05-04 01:22:32.697239: step 43240, loss = 18.05 (3.2 examples/sec; 4.998 sec/batch)
2018-05-04 01:23:22.364447: step 43250, loss = 18.51 (3.2 examples/sec; 4.938 sec/batch)
2018-05-04 01:24:11.245894: step 43260, loss = 18.44 (3.2 examples/sec; 4.997 sec/batch)
2018-05-04 01:25:00.792538: step 43270, loss = 18.32 (3.2 examples/sec; 4.927 sec/batch)
2018-05-04 01:25:51.095138: step 43280, loss = 18.47 (3.2 examples/sec; 5.040 sec/batch)
2018-05-04 01:26:40.324842: step 43290, loss = 17.95 (3.3 examples/sec; 4.893 sec/batch)
2018-05-04 01:27:30.171296: step 43300, loss = 18.20 (3.1 examples/sec; 5.082 sec/batch)
2018-05-04 01:28:23.415832: step 43310, loss = 18.30 (3.2 examples/sec; 4.930 sec/batch)
2018-05-04 01:29:09.795927: step 43320, loss = 18.73 (3.2 examples/sec; 4.925 sec/batch)
2018-05-04 01:29:59.134731: step 43330, loss = 18.21 (3.2 examples/sec; 4.980 sec/batch)
2018-05-04 01:30:48.051811: step 43340, loss = 18.02 (3.3 examples/sec; 4.856 sec/batch)
2018-05-04 01:31:37.835959: step 43350, loss = 18.56 (3.2 examples/sec; 5.035 sec/batch)
2018-05-04 01:32:27.187752: step 43360, loss = 18.14 (3.2 examples/sec; 5.000 sec/batch)
2018-05-04 01:33:16.473076: step 43370, loss = 18.70 (3.2 examples/sec; 4.972 sec/batch)
2018-05-04 01:34:05.364254: step 43380, loss = 18.18 (3.2 examples/sec; 5.007 sec/batch)
2018-05-04 01:34:54.531708: step 43390, loss = 18.35 (3.2 examples/sec; 5.022 sec/batch)
2018-05-04 01:35:43.660677: step 43400, loss = 18.42 (3.3 examples/sec; 4.810 sec/batch)
2018-05-04 01:36:36.587148: step 43410, loss = 18.05 (3.2 examples/sec; 4.973 sec/batch)
2018-05-04 01:37:25.593936: step 43420, loss = 18.02 (3.2 examples/sec; 5.010 sec/batch)
2018-05-04 01:38:14.636615: step 43430, loss = 18.91 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 01:39:01.293507: step 43440, loss = 18.23 (3.6 examples/sec; 4.456 sec/batch)
2018-05-04 01:39:51.075912: step 43450, loss = 17.92 (3.3 examples/sec; 4.813 sec/batch)
2018-05-04 01:40:41.103597: step 43460, loss = 17.79 (3.2 examples/sec; 5.017 sec/batch)
2018-05-04 01:41:30.294988: step 43470, loss = 19.12 (3.3 examples/sec; 4.833 sec/batch)
2018-05-04 01:42:19.497265: step 43480, loss = 18.51 (3.2 examples/sec; 5.010 sec/batch)
2018-05-04 01:43:08.621842: step 43490, loss = 18.52 (3.3 examples/sec; 4.914 sec/batch)
2018-05-04 01:43:58.299891: step 43500, loss = 17.93 (3.3 examples/sec; 4.913 sec/batch)
2018-05-04 01:44:51.483159: step 43510, loss = 17.92 (3.2 examples/sec; 4.960 sec/batch)
2018-05-04 01:45:40.868139: step 43520, loss = 18.17 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 01:46:30.091023: step 43530, loss = 17.98 (3.4 examples/sec; 4.775 sec/batch)
2018-05-04 01:47:19.010758: step 43540, loss = 18.18 (3.4 examples/sec; 4.747 sec/batch)
2018-05-04 01:48:08.633961: step 43550, loss = 18.32 (3.2 examples/sec; 5.017 sec/batch)
2018-05-04 01:48:58.436835: step 43560, loss = 18.11 (3.1 examples/sec; 5.202 sec/batch)
2018-05-04 01:49:44.836280: step 43570, loss = 18.19 (3.2 examples/sec; 4.968 sec/batch)
2018-05-04 01:50:33.714871: step 43580, loss = 18.28 (3.3 examples/sec; 4.904 sec/batch)
2018-05-04 01:51:22.962767: step 43590, loss = 18.39 (3.3 examples/sec; 4.868 sec/batch)
2018-05-04 01:52:12.603140: step 43600, loss = 18.26 (3.3 examples/sec; 4.912 sec/batch)
2018-05-04 01:53:05.235606: step 43610, loss = 18.76 (3.3 examples/sec; 4.898 sec/batch)
2018-05-04 01:53:54.206974: step 43620, loss = 18.15 (3.2 examples/sec; 4.972 sec/batch)
2018-05-04 01:54:43.276389: step 43630, loss = 17.99 (3.4 examples/sec; 4.758 sec/batch)
2018-05-04 01:55:32.587363: step 43640, loss = 17.97 (3.2 examples/sec; 5.007 sec/batch)
2018-05-04 01:56:21.857577: step 43650, loss = 18.75 (3.2 examples/sec; 5.011 sec/batch)
2018-05-04 01:57:10.738417: step 43660, loss = 18.08 (3.3 examples/sec; 4.899 sec/batch)
2018-05-04 01:58:00.027750: step 43670, loss = 18.22 (3.3 examples/sec; 4.780 sec/batch)
2018-05-04 01:58:48.186030: step 43680, loss = 18.11 (3.3 examples/sec; 4.815 sec/batch)
2018-05-04 01:59:35.907242: step 43690, loss = 18.16 (4.2 examples/sec; 3.783 sec/batch)
2018-05-04 02:00:23.819453: step 43700, loss = 17.91 (3.0 examples/sec; 5.410 sec/batch)
2018-05-04 02:01:16.760722: step 43710, loss = 17.81 (3.2 examples/sec; 4.956 sec/batch)
2018-05-04 02:02:05.653059: step 43720, loss = 18.79 (3.3 examples/sec; 4.878 sec/batch)
2018-05-04 02:02:55.145506: step 43730, loss = 18.15 (3.3 examples/sec; 4.851 sec/batch)
2018-05-04 02:03:44.299568: step 43740, loss = 18.34 (3.3 examples/sec; 4.812 sec/batch)
2018-05-04 02:04:33.540400: step 43750, loss = 17.95 (3.2 examples/sec; 4.924 sec/batch)
2018-05-04 02:05:22.889919: step 43760, loss = 18.21 (3.3 examples/sec; 4.847 sec/batch)
2018-05-04 02:06:12.189116: step 43770, loss = 18.59 (3.3 examples/sec; 4.899 sec/batch)
2018-05-04 02:07:01.515100: step 43780, loss = 18.65 (3.3 examples/sec; 4.880 sec/batch)
2018-05-04 02:07:50.848288: step 43790, loss = 18.37 (3.2 examples/sec; 4.933 sec/batch)
2018-05-04 02:08:39.268339: step 43800, loss = 17.80 (3.2 examples/sec; 4.958 sec/batch)
2018-05-04 02:09:31.495400: step 43810, loss = 18.50 (3.2 examples/sec; 4.958 sec/batch)
2018-05-04 02:10:18.207310: step 43820, loss = 18.14 (3.1 examples/sec; 5.154 sec/batch)
2018-05-04 02:11:07.868179: step 43830, loss = 19.02 (3.2 examples/sec; 4.975 sec/batch)
2018-05-04 02:11:57.284665: step 43840, loss = 19.16 (3.2 examples/sec; 4.989 sec/batch)
2018-05-04 02:12:46.482429: step 43850, loss = 18.69 (3.3 examples/sec; 4.860 sec/batch)
2018-05-04 02:13:35.756691: step 43860, loss = 18.10 (3.3 examples/sec; 4.848 sec/batch)
2018-05-04 02:14:25.836332: step 43870, loss = 18.33 (3.2 examples/sec; 4.971 sec/batch)
2018-05-04 02:15:15.440445: step 43880, loss = 18.33 (3.3 examples/sec; 4.843 sec/batch)
2018-05-04 02:16:05.332221: step 43890, loss = 18.46 (3.2 examples/sec; 4.983 sec/batch)
2018-05-04 02:16:55.164071: step 43900, loss = 17.99 (3.3 examples/sec; 4.836 sec/batch)
2018-05-04 02:17:48.682038: step 43910, loss = 18.74 (3.3 examples/sec; 4.909 sec/batch)
2018-05-04 02:18:37.880642: step 43920, loss = 18.12 (3.3 examples/sec; 4.880 sec/batch)
2018-05-04 02:19:27.127456: step 43930, loss = 18.02 (3.3 examples/sec; 4.811 sec/batch)
2018-05-04 02:20:14.632110: step 43940, loss = 18.21 (4.2 examples/sec; 3.782 sec/batch)
2018-05-04 02:21:02.779634: step 43950, loss = 18.15 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 02:21:51.732191: step 43960, loss = 18.08 (3.2 examples/sec; 4.926 sec/batch)
2018-05-04 02:22:41.122234: step 43970, loss = 18.02 (3.2 examples/sec; 5.075 sec/batch)
2018-05-04 02:23:31.029689: step 43980, loss = 17.89 (3.1 examples/sec; 5.082 sec/batch)
2018-05-04 02:24:20.853812: step 43990, loss = 18.00 (3.2 examples/sec; 5.076 sec/batch)
2018-05-04 02:25:10.017286: step 44000, loss = 18.46 (3.2 examples/sec; 4.947 sec/batch)
2018-05-04 02:26:03.199170: step 44010, loss = 18.26 (3.2 examples/sec; 4.966 sec/batch)
2018-05-04 02:26:52.799068: step 44020, loss = 18.53 (3.2 examples/sec; 5.001 sec/batch)
2018-05-04 02:27:42.890825: step 44030, loss = 18.50 (3.3 examples/sec; 4.879 sec/batch)
2018-05-04 02:28:32.176400: step 44040, loss = 18.57 (3.2 examples/sec; 4.939 sec/batch)
2018-05-04 02:29:21.327856: step 44050, loss = 18.39 (3.3 examples/sec; 4.839 sec/batch)
2018-05-04 02:30:10.493994: step 44060, loss = 18.37 (3.4 examples/sec; 4.772 sec/batch)
2018-05-04 02:30:56.108178: step 44070, loss = 18.07 (3.3 examples/sec; 4.815 sec/batch)
2018-05-04 02:31:45.910325: step 44080, loss = 18.44 (3.0 examples/sec; 5.348 sec/batch)
2018-05-04 02:32:35.572517: step 44090, loss = 18.08 (3.2 examples/sec; 5.018 sec/batch)
2018-05-04 02:33:25.296631: step 44100, loss = 19.13 (3.4 examples/sec; 4.700 sec/batch)
2018-05-04 02:34:18.028431: step 44110, loss = 18.36 (3.2 examples/sec; 5.030 sec/batch)
2018-05-04 02:35:07.200919: step 44120, loss = 17.93 (3.3 examples/sec; 4.835 sec/batch)
2018-05-04 02:35:56.081973: step 44130, loss = 18.46 (3.3 examples/sec; 4.919 sec/batch)
2018-05-04 02:36:45.813177: step 44140, loss = 18.14 (3.2 examples/sec; 4.935 sec/batch)
2018-05-04 02:37:35.553450: step 44150, loss = 18.43 (3.3 examples/sec; 4.813 sec/batch)
2018-05-04 02:38:25.035791: step 44160, loss = 17.85 (3.3 examples/sec; 4.906 sec/batch)
2018-05-04 02:39:14.665741: step 44170, loss = 18.16 (3.2 examples/sec; 4.991 sec/batch)
2018-05-04 02:40:04.180109: step 44180, loss = 17.99 (3.2 examples/sec; 5.033 sec/batch)
2018-05-04 02:40:50.771407: step 44190, loss = 17.88 (4.2 examples/sec; 3.769 sec/batch)
2018-05-04 02:41:38.644915: step 44200, loss = 18.32 (3.2 examples/sec; 4.969 sec/batch)
2018-05-04 02:42:31.487381: step 44210, loss = 18.84 (3.3 examples/sec; 4.797 sec/batch)
2018-05-04 02:43:20.173491: step 44220, loss = 18.28 (3.4 examples/sec; 4.660 sec/batch)
2018-05-04 02:44:09.632834: step 44230, loss = 18.35 (3.3 examples/sec; 4.807 sec/batch)
2018-05-04 02:44:59.380544: step 44240, loss = 18.12 (3.3 examples/sec; 4.817 sec/batch)
2018-05-04 02:45:48.593157: step 44250, loss = 17.97 (3.4 examples/sec; 4.775 sec/batch)
2018-05-04 02:46:37.832449: step 44260, loss = 17.98 (3.3 examples/sec; 4.845 sec/batch)
2018-05-04 02:47:27.479947: step 44270, loss = 18.19 (3.2 examples/sec; 4.991 sec/batch)
2018-05-04 02:48:17.182225: step 44280, loss = 18.11 (3.3 examples/sec; 4.895 sec/batch)
2018-05-04 02:49:06.847351: step 44290, loss = 17.83 (3.2 examples/sec; 4.948 sec/batch)
2018-05-04 02:49:56.582359: step 44300, loss = 18.07 (3.3 examples/sec; 4.883 sec/batch)
2018-05-04 02:50:49.532544: step 44310, loss = 17.97 (3.3 examples/sec; 4.829 sec/batch)
2018-05-04 02:51:36.271519: step 44320, loss = 17.87 (3.1 examples/sec; 5.127 sec/batch)
2018-05-04 02:52:25.794036: step 44330, loss = 17.95 (3.3 examples/sec; 4.872 sec/batch)
2018-05-04 02:53:15.207367: step 44340, loss = 18.19 (3.3 examples/sec; 4.880 sec/batch)
2018-05-04 02:54:04.597529: step 44350, loss = 18.72 (3.1 examples/sec; 5.129 sec/batch)
2018-05-04 02:54:54.829396: step 44360, loss = 18.66 (3.2 examples/sec; 5.023 sec/batch)
2018-05-04 02:55:44.343586: step 44370, loss = 17.87 (3.2 examples/sec; 4.949 sec/batch)
2018-05-04 02:56:33.457195: step 44380, loss = 18.24 (3.2 examples/sec; 4.938 sec/batch)
2018-05-04 02:57:22.524860: step 44390, loss = 18.72 (3.2 examples/sec; 5.049 sec/batch)
2018-05-04 02:58:11.969483: step 44400, loss = 18.44 (3.2 examples/sec; 4.987 sec/batch)
2018-05-04 02:59:04.934430: step 44410, loss = 18.31 (3.2 examples/sec; 4.985 sec/batch)
2018-05-04 02:59:54.251211: step 44420, loss = 18.85 (3.1 examples/sec; 5.106 sec/batch)
2018-05-04 03:00:43.237207: step 44430, loss = 17.87 (3.3 examples/sec; 4.866 sec/batch)
2018-05-04 03:01:30.648280: step 44440, loss = 18.13 (3.4 examples/sec; 4.746 sec/batch)
2018-05-04 03:02:20.210205: step 44450, loss = 18.37 (3.2 examples/sec; 4.990 sec/batch)
2018-05-04 03:03:09.543846: step 44460, loss = 18.19 (3.3 examples/sec; 4.827 sec/batch)
2018-05-04 03:03:58.904805: step 44470, loss = 18.04 (3.3 examples/sec; 4.898 sec/batch)
2018-05-04 03:04:47.911245: step 44480, loss = 18.49 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 03:05:38.267237: step 44490, loss = 18.32 (3.3 examples/sec; 4.896 sec/batch)
2018-05-04 03:06:27.197937: step 44500, loss = 17.96 (3.2 examples/sec; 4.934 sec/batch)
2018-05-04 03:07:20.296417: step 44510, loss = 18.27 (3.1 examples/sec; 5.200 sec/batch)
2018-05-04 03:08:09.638915: step 44520, loss = 17.82 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 03:08:59.150974: step 44530, loss = 18.49 (3.1 examples/sec; 5.097 sec/batch)
2018-05-04 03:09:48.984223: step 44540, loss = 18.32 (3.3 examples/sec; 4.901 sec/batch)
2018-05-04 03:10:38.634214: step 44550, loss = 18.55 (3.2 examples/sec; 5.021 sec/batch)
2018-05-04 03:11:28.141565: step 44560, loss = 18.51 (3.3 examples/sec; 4.878 sec/batch)
2018-05-04 03:12:14.466914: step 44570, loss = 18.00 (3.1 examples/sec; 5.112 sec/batch)
2018-05-04 03:13:04.240864: step 44580, loss = 18.25 (3.3 examples/sec; 4.837 sec/batch)
2018-05-04 03:13:53.476532: step 44590, loss = 18.45 (3.3 examples/sec; 4.788 sec/batch)
2018-05-04 03:14:42.787860: step 44600, loss = 18.09 (3.2 examples/sec; 5.065 sec/batch)
2018-05-04 03:15:35.359002: step 44610, loss = 18.95 (3.2 examples/sec; 4.993 sec/batch)
2018-05-04 03:16:24.452078: step 44620, loss = 18.41 (3.2 examples/sec; 4.971 sec/batch)
2018-05-04 03:17:13.469206: step 44630, loss = 18.30 (3.2 examples/sec; 5.046 sec/batch)
2018-05-04 03:18:02.437008: step 44640, loss = 18.38 (3.3 examples/sec; 4.905 sec/batch)
2018-05-04 03:18:51.962662: step 44650, loss = 18.34 (3.2 examples/sec; 4.998 sec/batch)
2018-05-04 03:19:41.710225: step 44660, loss = 18.80 (3.2 examples/sec; 4.925 sec/batch)
2018-05-04 03:20:30.698224: step 44670, loss = 18.13 (3.3 examples/sec; 4.899 sec/batch)
2018-05-04 03:21:19.716989: step 44680, loss = 18.11 (3.3 examples/sec; 4.921 sec/batch)
2018-05-04 03:22:05.749442: step 44690, loss = 18.40 (3.3 examples/sec; 4.867 sec/batch)
2018-05-04 03:22:55.221325: step 44700, loss = 18.18 (3.2 examples/sec; 4.954 sec/batch)
2018-05-04 03:23:47.981642: step 44710, loss = 17.90 (3.2 examples/sec; 4.955 sec/batch)
2018-05-04 03:24:37.030181: step 44720, loss = 18.16 (3.2 examples/sec; 4.983 sec/batch)
2018-05-04 03:25:26.083868: step 44730, loss = 18.00 (3.3 examples/sec; 4.866 sec/batch)
2018-05-04 03:26:16.258451: step 44740, loss = 18.04 (3.2 examples/sec; 4.981 sec/batch)
2018-05-04 03:27:05.515094: step 44750, loss = 18.41 (3.2 examples/sec; 5.000 sec/batch)
2018-05-04 03:27:55.277948: step 44760, loss = 18.11 (3.2 examples/sec; 5.047 sec/batch)
2018-05-04 03:28:44.935611: step 44770, loss = 17.84 (3.2 examples/sec; 4.957 sec/batch)
2018-05-04 03:29:34.062691: step 44780, loss = 17.83 (3.3 examples/sec; 4.795 sec/batch)
2018-05-04 03:30:22.831382: step 44790, loss = 17.92 (3.2 examples/sec; 4.973 sec/batch)
2018-05-04 03:31:11.763910: step 44800, loss = 18.03 (3.2 examples/sec; 5.021 sec/batch)
2018-05-04 03:32:05.225921: step 44810, loss = 17.93 (3.1 examples/sec; 5.100 sec/batch)
2018-05-04 03:32:51.372215: step 44820, loss = 18.43 (3.3 examples/sec; 4.814 sec/batch)
2018-05-04 03:33:41.012147: step 44830, loss = 18.81 (3.2 examples/sec; 4.998 sec/batch)
2018-05-04 03:34:30.397745: step 44840, loss = 18.62 (3.3 examples/sec; 4.843 sec/batch)
2018-05-04 03:35:20.122255: step 44850, loss = 18.78 (3.3 examples/sec; 4.921 sec/batch)
2018-05-04 03:36:10.237882: step 44860, loss = 17.99 (3.3 examples/sec; 4.921 sec/batch)
2018-05-04 03:36:59.553155: step 44870, loss = 18.34 (3.2 examples/sec; 4.963 sec/batch)
2018-05-04 03:37:49.008138: step 44880, loss = 18.48 (3.1 examples/sec; 5.102 sec/batch)
2018-05-04 03:38:38.628804: step 44890, loss = 18.96 (3.3 examples/sec; 4.816 sec/batch)
2018-05-04 03:39:28.463965: step 44900, loss = 18.01 (3.2 examples/sec; 4.954 sec/batch)
2018-05-04 03:40:21.434576: step 44910, loss = 18.04 (3.4 examples/sec; 4.744 sec/batch)
2018-05-04 03:41:11.152407: step 44920, loss = 18.59 (3.2 examples/sec; 5.045 sec/batch)
2018-05-04 03:42:00.323366: step 44930, loss = 18.32 (3.3 examples/sec; 4.922 sec/batch)
2018-05-04 03:42:45.997047: step 44940, loss = 18.86 (3.2 examples/sec; 4.953 sec/batch)
2018-05-04 03:43:34.638124: step 44950, loss = 18.38 (3.3 examples/sec; 4.920 sec/batch)
2018-05-04 03:44:23.747607: step 44960, loss = 18.49 (3.4 examples/sec; 4.739 sec/batch)
2018-05-04 03:45:12.385920: step 44970, loss = 18.00 (3.3 examples/sec; 4.862 sec/batch)
2018-05-04 03:46:01.083641: step 44980, loss = 18.32 (3.4 examples/sec; 4.746 sec/batch)
2018-05-04 03:46:50.019716: step 44990, loss = 18.13 (3.3 examples/sec; 4.847 sec/batch)
2018-05-04 03:47:39.193284: step 45000, loss = 18.32 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 03:48:31.563513: step 45010, loss = 18.41 (3.2 examples/sec; 5.051 sec/batch)
2018-05-04 03:49:20.523944: step 45020, loss = 18.34 (3.2 examples/sec; 4.962 sec/batch)
2018-05-04 03:50:09.504992: step 45030, loss = 18.47 (3.3 examples/sec; 4.792 sec/batch)
2018-05-04 03:50:59.211691: step 45040, loss = 17.96 (3.2 examples/sec; 4.976 sec/batch)
2018-05-04 03:51:48.030815: step 45050, loss = 18.44 (3.2 examples/sec; 4.988 sec/batch)
2018-05-04 03:52:36.706610: step 45060, loss = 18.32 (3.2 examples/sec; 4.975 sec/batch)
2018-05-04 03:53:22.391103: step 45070, loss = 18.06 (3.2 examples/sec; 4.957 sec/batch)
2018-05-04 03:54:12.264844: step 45080, loss = 18.32 (3.2 examples/sec; 5.013 sec/batch)
2018-05-04 03:55:01.520643: step 45090, loss = 17.99 (3.3 examples/sec; 4.848 sec/batch)
2018-05-04 03:55:50.924638: step 45100, loss = 19.20 (3.2 examples/sec; 5.042 sec/batch)
2018-05-04 03:56:43.991653: step 45110, loss = 18.33 (3.2 examples/sec; 4.992 sec/batch)
2018-05-04 03:57:33.076156: step 45120, loss = 18.06 (3.3 examples/sec; 4.859 sec/batch)
2018-05-04 03:58:22.213189: step 45130, loss = 18.23 (3.2 examples/sec; 5.007 sec/batch)
2018-05-04 03:59:10.951233: step 45140, loss = 18.01 (3.4 examples/sec; 4.760 sec/batch)
2018-05-04 03:59:59.860056: step 45150, loss = 18.30 (3.4 examples/sec; 4.754 sec/batch)
2018-05-04 04:00:49.083320: step 45160, loss = 18.03 (3.3 examples/sec; 4.812 sec/batch)
2018-05-04 04:01:38.449554: step 45170, loss = 17.87 (3.2 examples/sec; 5.079 sec/batch)
2018-05-04 04:02:28.446167: step 45180, loss = 17.97 (3.3 examples/sec; 4.908 sec/batch)
2018-05-04 04:03:14.665130: step 45190, loss = 18.19 (4.1 examples/sec; 3.940 sec/batch)
2018-05-04 04:04:03.877501: step 45200, loss = 18.07 (3.2 examples/sec; 5.071 sec/batch)
2018-05-04 04:04:57.247621: step 45210, loss = 18.94 (3.3 examples/sec; 4.853 sec/batch)
2018-05-04 04:05:46.258775: step 45220, loss = 18.31 (3.2 examples/sec; 4.961 sec/batch)
2018-05-04 04:06:35.647052: step 45230, loss = 17.89 (3.2 examples/sec; 5.036 sec/batch)
2018-05-04 04:07:25.133569: step 45240, loss = 18.37 (3.3 examples/sec; 4.784 sec/batch)
2018-05-04 04:08:13.653190: step 45250, loss = 18.21 (3.3 examples/sec; 4.832 sec/batch)
2018-05-04 04:09:02.615993: step 45260, loss = 17.77 (3.4 examples/sec; 4.655 sec/batch)
2018-05-04 04:09:51.774332: step 45270, loss = 17.95 (3.2 examples/sec; 4.955 sec/batch)
2018-05-04 04:10:41.560196: step 45280, loss = 18.31 (3.1 examples/sec; 5.082 sec/batch)
2018-05-04 04:11:30.755564: step 45290, loss = 18.33 (3.3 examples/sec; 4.836 sec/batch)
2018-05-04 04:12:20.298959: step 45300, loss = 18.25 (3.2 examples/sec; 5.058 sec/batch)
2018-05-04 04:13:12.975993: step 45310, loss = 18.25 (3.4 examples/sec; 4.710 sec/batch)
2018-05-04 04:14:00.048788: step 45320, loss = 18.69 (3.2 examples/sec; 4.985 sec/batch)
2018-05-04 04:14:49.239471: step 45330, loss = 18.00 (3.3 examples/sec; 4.915 sec/batch)
2018-05-04 04:15:39.196427: step 45340, loss = 18.51 (3.2 examples/sec; 4.981 sec/batch)
2018-05-04 04:16:28.184562: step 45350, loss = 18.33 (3.4 examples/sec; 4.640 sec/batch)
2018-05-04 04:17:17.528789: step 45360, loss = 18.19 (3.2 examples/sec; 5.029 sec/batch)
2018-05-04 04:18:07.206596: step 45370, loss = 17.92 (3.2 examples/sec; 5.033 sec/batch)
2018-05-04 04:18:56.817618: step 45380, loss = 17.99 (3.3 examples/sec; 4.840 sec/batch)
2018-05-04 04:19:46.289209: step 45390, loss = 18.32 (3.1 examples/sec; 5.082 sec/batch)
2018-05-04 04:20:35.954927: step 45400, loss = 18.22 (3.2 examples/sec; 5.053 sec/batch)
2018-05-04 04:21:29.416698: step 45410, loss = 18.27 (3.3 examples/sec; 4.918 sec/batch)
2018-05-04 04:22:18.755009: step 45420, loss = 18.04 (3.3 examples/sec; 4.908 sec/batch)
2018-05-04 04:23:08.368677: step 45430, loss = 18.58 (3.3 examples/sec; 4.886 sec/batch)
2018-05-04 04:23:54.724721: step 45440, loss = 18.77 (3.3 examples/sec; 4.888 sec/batch)
2018-05-04 04:24:44.651082: step 45450, loss = 18.11 (3.2 examples/sec; 4.968 sec/batch)
2018-05-04 04:25:33.810460: step 45460, loss = 18.16 (3.4 examples/sec; 4.753 sec/batch)
2018-05-04 04:26:23.315636: step 45470, loss = 18.02 (3.2 examples/sec; 4.939 sec/batch)
2018-05-04 04:27:12.490250: step 45480, loss = 18.40 (3.3 examples/sec; 4.869 sec/batch)
2018-05-04 04:28:01.764002: step 45490, loss = 17.94 (3.3 examples/sec; 4.896 sec/batch)
2018-05-04 04:28:51.034218: step 45500, loss = 18.12 (3.2 examples/sec; 5.002 sec/batch)
2018-05-04 04:29:45.012060: step 45510, loss = 17.76 (3.2 examples/sec; 5.078 sec/batch)
2018-05-04 04:30:35.053875: step 45520, loss = 18.32 (3.2 examples/sec; 4.995 sec/batch)
2018-05-04 04:31:24.601893: step 45530, loss = 18.34 (3.3 examples/sec; 4.834 sec/batch)
2018-05-04 04:32:13.227951: step 45540, loss = 18.15 (3.3 examples/sec; 4.796 sec/batch)
2018-05-04 04:33:02.509423: step 45550, loss = 18.70 (3.2 examples/sec; 5.001 sec/batch)
2018-05-04 04:33:51.860015: step 45560, loss = 18.11 (3.3 examples/sec; 4.904 sec/batch)
2018-05-04 04:34:37.313569: step 45570, loss = 17.81 (3.2 examples/sec; 4.971 sec/batch)
2018-05-04 04:35:26.917156: step 45580, loss = 18.55 (3.2 examples/sec; 4.939 sec/batch)
2018-05-04 04:36:16.770309: step 45590, loss = 18.33 (3.1 examples/sec; 5.089 sec/batch)
2018-05-04 04:37:06.847476: step 45600, loss = 18.06 (3.2 examples/sec; 5.000 sec/batch)
2018-05-04 04:38:00.348054: step 45610, loss = 17.84 (3.1 examples/sec; 5.121 sec/batch)
2018-05-04 04:38:49.965555: step 45620, loss = 18.72 (3.3 examples/sec; 4.857 sec/batch)
2018-05-04 04:39:39.318006: step 45630, loss = 17.85 (3.3 examples/sec; 4.824 sec/batch)
2018-05-04 04:40:28.059270: step 45640, loss = 17.99 (3.3 examples/sec; 4.894 sec/batch)
2018-05-04 04:41:17.178581: step 45650, loss = 17.98 (3.3 examples/sec; 4.917 sec/batch)
2018-05-04 04:42:06.369865: step 45660, loss = 18.81 (3.3 examples/sec; 4.878 sec/batch)
2018-05-04 04:42:55.228422: step 45670, loss = 18.16 (3.3 examples/sec; 4.819 sec/batch)
2018-05-04 04:43:44.376301: step 45680, loss = 18.95 (3.3 examples/sec; 4.844 sec/batch)
2018-05-04 04:44:30.065780: step 45690, loss = 18.01 (3.1 examples/sec; 5.126 sec/batch)
2018-05-04 04:45:19.137850: step 45700, loss = 18.05 (3.2 examples/sec; 5.046 sec/batch)
2018-05-04 04:46:12.368397: step 45710, loss = 18.11 (3.2 examples/sec; 5.043 sec/batch)
2018-05-04 04:47:02.844617: step 45720, loss = 18.68 (3.2 examples/sec; 4.972 sec/batch)
2018-05-04 04:47:52.383077: step 45730, loss = 17.95 (3.1 examples/sec; 5.132 sec/batch)
2018-05-04 04:48:42.321263: step 45740, loss = 18.49 (3.3 examples/sec; 4.908 sec/batch)
2018-05-04 04:49:32.445833: step 45750, loss = 18.27 (3.1 examples/sec; 5.150 sec/batch)
2018-05-04 04:50:21.549354: step 45760, loss = 18.00 (3.3 examples/sec; 4.835 sec/batch)
2018-05-04 04:51:11.213066: step 45770, loss = 17.96 (3.3 examples/sec; 4.839 sec/batch)
2018-05-04 04:52:01.286061: step 45780, loss = 18.22 (3.3 examples/sec; 4.860 sec/batch)
2018-05-04 04:52:50.936324: step 45790, loss = 18.28 (3.2 examples/sec; 5.032 sec/batch)
2018-05-04 04:53:40.003748: step 45800, loss = 18.40 (3.3 examples/sec; 4.830 sec/batch)
2018-05-04 04:54:30.616167: step 45810, loss = 18.27 (4.2 examples/sec; 3.798 sec/batch)
2018-05-04 04:55:19.976502: step 45820, loss = 18.82 (3.2 examples/sec; 4.942 sec/batch)
2018-05-04 04:56:09.675220: step 45830, loss = 18.39 (3.3 examples/sec; 4.914 sec/batch)
2018-05-04 04:57:00.093892: step 45840, loss = 18.06 (3.2 examples/sec; 4.968 sec/batch)
2018-05-04 04:57:49.809322: step 45850, loss = 18.28 (3.2 examples/sec; 4.975 sec/batch)
2018-05-04 04:58:39.467587: step 45860, loss = 17.98 (3.2 examples/sec; 4.985 sec/batch)
2018-05-04 04:59:29.009369: step 45870, loss = 18.01 (3.3 examples/sec; 4.910 sec/batch)
2018-05-04 05:00:18.449159: step 45880, loss = 18.05 (3.1 examples/sec; 5.113 sec/batch)
2018-05-04 05:01:07.935314: step 45890, loss = 18.63 (3.2 examples/sec; 5.026 sec/batch)
2018-05-04 05:01:57.448701: step 45900, loss = 17.94 (3.2 examples/sec; 5.042 sec/batch)
2018-05-04 05:02:50.597546: step 45910, loss = 18.79 (3.3 examples/sec; 4.866 sec/batch)
2018-05-04 05:03:40.147437: step 45920, loss = 18.01 (3.1 examples/sec; 5.084 sec/batch)
2018-05-04 05:04:29.210049: step 45930, loss = 18.00 (3.3 examples/sec; 4.811 sec/batch)
2018-05-04 05:05:15.163443: step 45940, loss = 18.30 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 05:06:04.396806: step 45950, loss = 18.19 (3.2 examples/sec; 5.027 sec/batch)
2018-05-04 05:06:54.319792: step 45960, loss = 18.09 (3.2 examples/sec; 4.953 sec/batch)
2018-05-04 05:07:43.648268: step 45970, loss = 18.65 (3.3 examples/sec; 4.904 sec/batch)
2018-05-04 05:08:33.194874: step 45980, loss = 18.20 (3.1 examples/sec; 5.123 sec/batch)
2018-05-04 05:09:22.285329: step 45990, loss = 18.32 (3.3 examples/sec; 4.912 sec/batch)
2018-05-04 05:10:11.863237: step 46000, loss = 18.21 (3.1 examples/sec; 5.237 sec/batch)
2018-05-04 05:11:04.217077: step 46010, loss = 18.27 (3.5 examples/sec; 4.606 sec/batch)
2018-05-04 05:11:54.308147: step 46020, loss = 18.13 (3.1 examples/sec; 5.112 sec/batch)
2018-05-04 05:12:43.530923: step 46030, loss = 18.32 (3.2 examples/sec; 4.991 sec/batch)
2018-05-04 05:13:32.492905: step 46040, loss = 18.03 (3.2 examples/sec; 4.992 sec/batch)
2018-05-04 05:14:21.995718: step 46050, loss = 18.01 (3.2 examples/sec; 5.006 sec/batch)
2018-05-04 05:15:08.640043: step 46060, loss = 18.38 (3.9 examples/sec; 4.140 sec/batch)
2018-05-04 05:15:58.179628: step 46070, loss = 17.76 (3.3 examples/sec; 4.854 sec/batch)
2018-05-04 05:16:47.442755: step 46080, loss = 17.96 (3.4 examples/sec; 4.662 sec/batch)
2018-05-04 05:17:36.512964: step 46090, loss = 18.15 (3.3 examples/sec; 4.846 sec/batch)
2018-05-04 05:18:26.370700: step 46100, loss = 18.11 (3.2 examples/sec; 4.959 sec/batch)
2018-05-04 05:19:19.302635: step 46110, loss = 18.93 (3.2 examples/sec; 5.041 sec/batch)
2018-05-04 05:20:08.632384: step 46120, loss = 18.26 (3.1 examples/sec; 5.151 sec/batch)
2018-05-04 05:20:58.461269: step 46130, loss = 18.60 (3.1 examples/sec; 5.093 sec/batch)
2018-05-04 05:21:46.690202: step 46140, loss = 18.28 (3.4 examples/sec; 4.737 sec/batch)
2018-05-04 05:22:35.820783: step 46150, loss = 18.38 (3.3 examples/sec; 4.868 sec/batch)
2018-05-04 05:23:25.286092: step 46160, loss = 18.88 (3.3 examples/sec; 4.837 sec/batch)
2018-05-04 05:24:15.002695: step 46170, loss = 18.47 (3.3 examples/sec; 4.828 sec/batch)
2018-05-04 05:25:03.847011: step 46180, loss = 18.20 (3.3 examples/sec; 4.889 sec/batch)
2018-05-04 05:25:50.370622: step 46190, loss = 18.45 (3.2 examples/sec; 5.041 sec/batch)
2018-05-04 05:26:39.697939: step 46200, loss = 17.80 (3.1 examples/sec; 5.110 sec/batch)
2018-05-04 05:27:33.094741: step 46210, loss = 18.16 (3.2 examples/sec; 4.942 sec/batch)
2018-05-04 05:28:22.014546: step 46220, loss = 18.59 (3.2 examples/sec; 4.959 sec/batch)
2018-05-04 05:29:11.362961: step 46230, loss = 18.42 (3.1 examples/sec; 5.081 sec/batch)
2018-05-04 05:30:00.926589: step 46240, loss = 18.48 (3.2 examples/sec; 5.002 sec/batch)
2018-05-04 05:30:50.300873: step 46250, loss = 18.70 (3.4 examples/sec; 4.710 sec/batch)
2018-05-04 05:31:40.491613: step 46260, loss = 18.45 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 05:32:30.007923: step 46270, loss = 17.89 (3.2 examples/sec; 5.065 sec/batch)
2018-05-04 05:33:19.567303: step 46280, loss = 17.84 (3.2 examples/sec; 5.006 sec/batch)
2018-05-04 05:34:08.852328: step 46290, loss = 18.07 (3.2 examples/sec; 4.929 sec/batch)
2018-05-04 05:34:58.126056: step 46300, loss = 18.02 (3.3 examples/sec; 4.913 sec/batch)
2018-05-04 05:35:47.578596: step 46310, loss = 18.82 (3.4 examples/sec; 4.743 sec/batch)
2018-05-04 05:36:36.234149: step 46320, loss = 17.94 (3.3 examples/sec; 4.831 sec/batch)
2018-05-04 05:37:25.516338: step 46330, loss = 18.13 (3.3 examples/sec; 4.900 sec/batch)
2018-05-04 05:38:15.451268: step 46340, loss = 17.96 (3.3 examples/sec; 4.898 sec/batch)
2018-05-04 05:39:04.658430: step 46350, loss = 18.59 (3.2 examples/sec; 5.002 sec/batch)
2018-05-04 05:39:53.654895: step 46360, loss = 18.02 (3.3 examples/sec; 4.910 sec/batch)
2018-05-04 05:40:42.942979: step 46370, loss = 18.30 (3.3 examples/sec; 4.782 sec/batch)
2018-05-04 05:41:32.297269: step 46380, loss = 18.03 (3.4 examples/sec; 4.717 sec/batch)
2018-05-04 05:42:21.653901: step 46390, loss = 18.69 (3.2 examples/sec; 4.990 sec/batch)
2018-05-04 05:43:11.268412: step 46400, loss = 17.81 (3.3 examples/sec; 4.867 sec/batch)
2018-05-04 05:44:04.951710: step 46410, loss = 18.34 (3.3 examples/sec; 4.895 sec/batch)
2018-05-04 05:44:54.372978: step 46420, loss = 18.31 (3.2 examples/sec; 5.015 sec/batch)
2018-05-04 05:45:42.899550: step 46430, loss = 18.19 (3.3 examples/sec; 4.832 sec/batch)
2018-05-04 05:46:29.505566: step 46440, loss = 18.05 (3.2 examples/sec; 5.043 sec/batch)
2018-05-04 05:47:19.061891: step 46450, loss = 18.27 (3.0 examples/sec; 5.251 sec/batch)
2018-05-04 05:48:08.369387: step 46460, loss = 18.28 (3.3 examples/sec; 4.893 sec/batch)
2018-05-04 05:48:57.607206: step 46470, loss = 18.48 (3.2 examples/sec; 4.931 sec/batch)
2018-05-04 05:49:47.395955: step 46480, loss = 18.63 (3.1 examples/sec; 5.102 sec/batch)
2018-05-04 05:50:36.293013: step 46490, loss = 17.83 (3.4 examples/sec; 4.721 sec/batch)
2018-05-04 05:51:25.141455: step 46500, loss = 18.05 (3.3 examples/sec; 4.822 sec/batch)
2018-05-04 05:52:18.171970: step 46510, loss = 17.89 (3.3 examples/sec; 4.842 sec/batch)
2018-05-04 05:53:07.518845: step 46520, loss = 18.13 (3.2 examples/sec; 5.060 sec/batch)
2018-05-04 05:53:56.843038: step 46530, loss = 18.05 (3.3 examples/sec; 4.897 sec/batch)
2018-05-04 05:54:45.609728: step 46540, loss = 17.91 (3.2 examples/sec; 4.954 sec/batch)
2018-05-04 05:55:34.154155: step 46550, loss = 17.91 (3.3 examples/sec; 4.870 sec/batch)
2018-05-04 05:56:19.880030: step 46560, loss = 17.96 (4.3 examples/sec; 3.741 sec/batch)
2018-05-04 05:57:08.993155: step 46570, loss = 18.00 (3.2 examples/sec; 4.956 sec/batch)
2018-05-04 05:57:58.209858: step 46580, loss = 17.97 (3.4 examples/sec; 4.730 sec/batch)
2018-05-04 05:58:47.489378: step 46590, loss = 18.40 (3.2 examples/sec; 5.044 sec/batch)
2018-05-04 05:59:36.652471: step 46600, loss = 18.71 (3.1 examples/sec; 5.083 sec/batch)
2018-05-04 06:00:29.376955: step 46610, loss = 18.27 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 06:01:18.298995: step 46620, loss = 18.38 (3.2 examples/sec; 4.973 sec/batch)
2018-05-04 06:02:07.561687: step 46630, loss = 17.93 (3.4 examples/sec; 4.758 sec/batch)
2018-05-04 06:02:56.964718: step 46640, loss = 17.84 (3.2 examples/sec; 5.041 sec/batch)
2018-05-04 06:03:46.692099: step 46650, loss = 18.21 (3.3 examples/sec; 4.876 sec/batch)
2018-05-04 06:04:36.216026: step 46660, loss = 17.93 (3.2 examples/sec; 5.019 sec/batch)
2018-05-04 06:05:25.633824: step 46670, loss = 17.98 (3.2 examples/sec; 4.984 sec/batch)
2018-05-04 06:06:15.952573: step 46680, loss = 18.36 (3.1 examples/sec; 5.083 sec/batch)
2018-05-04 06:07:02.770921: step 46690, loss = 18.10 (3.2 examples/sec; 4.936 sec/batch)
2018-05-04 06:07:51.899906: step 46700, loss = 17.99 (3.2 examples/sec; 4.982 sec/batch)
2018-05-04 06:08:44.683799: step 46710, loss = 18.36 (3.2 examples/sec; 4.969 sec/batch)
2018-05-04 06:09:34.192537: step 46720, loss = 18.21 (3.2 examples/sec; 5.023 sec/batch)
2018-05-04 06:10:23.901254: step 46730, loss = 18.46 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 06:11:13.532027: step 46740, loss = 17.77 (3.2 examples/sec; 5.033 sec/batch)
2018-05-04 06:12:03.219162: step 46750, loss = 17.93 (3.1 examples/sec; 5.127 sec/batch)
2018-05-04 06:12:52.549007: step 46760, loss = 18.18 (3.3 examples/sec; 4.778 sec/batch)
2018-05-04 06:13:41.674279: step 46770, loss = 18.56 (3.3 examples/sec; 4.813 sec/batch)
2018-05-04 06:14:31.314147: step 46780, loss = 18.18 (3.3 examples/sec; 4.863 sec/batch)
2018-05-04 06:15:21.039970: step 46790, loss = 18.03 (3.2 examples/sec; 5.039 sec/batch)
2018-05-04 06:16:10.575802: step 46800, loss = 17.96 (3.1 examples/sec; 5.109 sec/batch)
2018-05-04 06:16:59.991335: step 46810, loss = 18.05 (3.4 examples/sec; 4.741 sec/batch)
2018-05-04 06:17:49.129319: step 46820, loss = 18.63 (3.2 examples/sec; 5.002 sec/batch)
2018-05-04 06:18:38.488540: step 46830, loss = 18.31 (3.3 examples/sec; 4.857 sec/batch)
2018-05-04 06:19:27.949300: step 46840, loss = 18.50 (3.3 examples/sec; 4.906 sec/batch)
2018-05-04 06:20:17.608857: step 46850, loss = 18.14 (3.2 examples/sec; 4.958 sec/batch)
2018-05-04 06:21:07.542828: step 46860, loss = 18.60 (3.2 examples/sec; 4.981 sec/batch)
2018-05-04 06:21:57.048191: step 46870, loss = 18.86 (3.2 examples/sec; 5.051 sec/batch)
2018-05-04 06:22:46.276355: step 46880, loss = 17.86 (3.3 examples/sec; 4.874 sec/batch)
2018-05-04 06:23:35.931026: step 46890, loss = 17.87 (3.1 examples/sec; 5.092 sec/batch)
2018-05-04 06:24:25.565932: step 46900, loss = 18.40 (3.2 examples/sec; 5.072 sec/batch)
2018-05-04 06:25:18.264641: step 46910, loss = 18.81 (3.3 examples/sec; 4.811 sec/batch)
2018-05-04 06:26:07.821809: step 46920, loss = 18.50 (3.1 examples/sec; 5.087 sec/batch)
2018-05-04 06:26:57.617979: step 46930, loss = 17.88 (3.2 examples/sec; 4.935 sec/batch)
2018-05-04 06:27:43.926198: step 46940, loss = 18.09 (3.3 examples/sec; 4.921 sec/batch)
2018-05-04 06:28:33.168576: step 46950, loss = 17.93 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 06:29:22.981712: step 46960, loss = 18.47 (3.2 examples/sec; 5.049 sec/batch)
2018-05-04 06:30:12.961320: step 46970, loss = 18.26 (3.2 examples/sec; 5.050 sec/batch)
2018-05-04 06:31:02.429105: step 46980, loss = 18.00 (3.2 examples/sec; 4.966 sec/batch)
2018-05-04 06:31:51.813081: step 46990, loss = 18.19 (3.2 examples/sec; 4.930 sec/batch)
2018-05-04 06:32:41.600683: step 47000, loss = 18.08 (3.3 examples/sec; 4.821 sec/batch)
2018-05-04 06:33:34.351814: step 47010, loss = 18.05 (3.3 examples/sec; 4.913 sec/batch)
2018-05-04 06:34:23.708495: step 47020, loss = 18.26 (3.4 examples/sec; 4.726 sec/batch)
2018-05-04 06:35:12.936631: step 47030, loss = 18.72 (3.2 examples/sec; 4.972 sec/batch)
2018-05-04 06:36:03.410034: step 47040, loss = 18.66 (3.2 examples/sec; 4.981 sec/batch)
2018-05-04 06:36:52.890094: step 47050, loss = 18.64 (3.3 examples/sec; 4.846 sec/batch)
2018-05-04 06:37:39.016198: step 47060, loss = 17.88 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 06:38:28.135846: step 47070, loss = 17.77 (3.2 examples/sec; 4.938 sec/batch)
2018-05-04 06:39:16.852428: step 47080, loss = 18.05 (3.3 examples/sec; 4.867 sec/batch)
2018-05-04 06:40:06.404650: step 47090, loss = 18.12 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 06:40:55.460778: step 47100, loss = 17.69 (3.3 examples/sec; 4.778 sec/batch)
2018-05-04 06:41:51.116039: step 47110, loss = 18.56 (3.2 examples/sec; 4.973 sec/batch)
2018-05-04 06:42:39.959274: step 47120, loss = 18.21 (3.3 examples/sec; 4.867 sec/batch)
2018-05-04 06:43:29.536998: step 47130, loss = 18.03 (3.2 examples/sec; 4.969 sec/batch)
2018-05-04 06:44:19.131504: step 47140, loss = 18.41 (3.3 examples/sec; 4.862 sec/batch)
2018-05-04 06:45:08.659845: step 47150, loss = 18.27 (3.2 examples/sec; 4.964 sec/batch)
2018-05-04 06:45:57.984576: step 47160, loss = 17.89 (3.2 examples/sec; 5.022 sec/batch)
2018-05-04 06:46:47.215877: step 47170, loss = 17.96 (3.3 examples/sec; 4.920 sec/batch)
2018-05-04 06:47:36.305474: step 47180, loss = 17.97 (3.3 examples/sec; 4.848 sec/batch)
2018-05-04 06:48:22.486598: step 47190, loss = 18.24 (3.2 examples/sec; 5.031 sec/batch)
2018-05-04 06:49:11.839583: step 47200, loss = 17.91 (3.3 examples/sec; 4.919 sec/batch)
2018-05-04 06:50:04.294825: step 47210, loss = 18.14 (3.3 examples/sec; 4.802 sec/batch)
2018-05-04 06:50:53.313112: step 47220, loss = 18.53 (3.4 examples/sec; 4.746 sec/batch)
2018-05-04 06:51:42.220858: step 47230, loss = 17.79 (3.4 examples/sec; 4.704 sec/batch)
2018-05-04 06:52:31.332158: step 47240, loss = 17.79 (3.3 examples/sec; 4.919 sec/batch)
2018-05-04 06:53:20.771109: step 47250, loss = 18.61 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 06:54:09.764584: step 47260, loss = 18.12 (3.3 examples/sec; 4.861 sec/batch)
2018-05-04 06:54:58.722300: step 47270, loss = 18.10 (3.3 examples/sec; 4.832 sec/batch)
2018-05-04 06:55:48.541622: step 47280, loss = 18.44 (3.3 examples/sec; 4.862 sec/batch)
2018-05-04 06:56:38.468617: step 47290, loss = 17.85 (3.1 examples/sec; 5.118 sec/batch)
2018-05-04 06:57:27.894483: step 47300, loss = 18.05 (3.1 examples/sec; 5.081 sec/batch)
2018-05-04 06:58:18.128040: step 47310, loss = 18.56 (3.3 examples/sec; 4.876 sec/batch)
2018-05-04 06:59:07.041405: step 47320, loss = 18.36 (3.2 examples/sec; 4.992 sec/batch)
2018-05-04 06:59:57.002899: step 47330, loss = 18.22 (3.2 examples/sec; 5.004 sec/batch)
2018-05-04 07:00:45.876349: step 47340, loss = 17.78 (3.2 examples/sec; 4.945 sec/batch)
2018-05-04 07:01:34.968853: step 47350, loss = 18.26 (3.3 examples/sec; 4.805 sec/batch)
2018-05-04 07:02:24.008459: step 47360, loss = 18.11 (3.2 examples/sec; 4.934 sec/batch)
2018-05-04 07:03:13.021044: step 47370, loss = 18.22 (3.2 examples/sec; 4.946 sec/batch)
2018-05-04 07:04:02.687435: step 47380, loss = 18.11 (3.3 examples/sec; 4.785 sec/batch)
2018-05-04 07:04:51.939128: step 47390, loss = 17.92 (3.3 examples/sec; 4.904 sec/batch)
2018-05-04 07:05:41.845896: step 47400, loss = 18.30 (3.3 examples/sec; 4.889 sec/batch)
2018-05-04 07:06:34.922401: step 47410, loss = 18.04 (3.3 examples/sec; 4.891 sec/batch)
2018-05-04 07:07:23.859927: step 47420, loss = 18.76 (3.3 examples/sec; 4.806 sec/batch)
2018-05-04 07:08:13.286374: step 47430, loss = 17.98 (3.2 examples/sec; 5.077 sec/batch)
2018-05-04 07:09:00.214218: step 47440, loss = 19.00 (3.1 examples/sec; 5.113 sec/batch)
2018-05-04 07:09:49.634288: step 47450, loss = 17.95 (3.1 examples/sec; 5.080 sec/batch)
2018-05-04 07:10:39.397030: step 47460, loss = 17.86 (3.3 examples/sec; 4.868 sec/batch)
2018-05-04 07:11:28.978840: step 47470, loss = 18.39 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 07:12:18.413610: step 47480, loss = 18.48 (3.3 examples/sec; 4.920 sec/batch)
2018-05-04 07:13:07.696028: step 47490, loss = 18.20 (3.5 examples/sec; 4.613 sec/batch)
2018-05-04 07:13:56.447864: step 47500, loss = 18.55 (3.3 examples/sec; 4.879 sec/batch)
2018-05-04 07:14:49.382368: step 47510, loss = 18.46 (3.4 examples/sec; 4.723 sec/batch)
2018-05-04 07:15:39.404183: step 47520, loss = 18.24 (3.2 examples/sec; 4.982 sec/batch)
2018-05-04 07:16:28.513196: step 47530, loss = 18.49 (3.3 examples/sec; 4.874 sec/batch)
2018-05-04 07:17:18.009179: step 47540, loss = 18.60 (3.2 examples/sec; 5.077 sec/batch)
2018-05-04 07:18:07.397772: step 47550, loss = 18.03 (3.2 examples/sec; 4.950 sec/batch)
2018-05-04 07:18:53.640597: step 47560, loss = 18.08 (3.3 examples/sec; 4.810 sec/batch)
2018-05-04 07:19:42.543991: step 47570, loss = 18.32 (3.2 examples/sec; 4.950 sec/batch)
2018-05-04 07:20:32.399959: step 47580, loss = 17.92 (3.2 examples/sec; 5.079 sec/batch)
2018-05-04 07:21:21.980641: step 47590, loss = 18.51 (3.3 examples/sec; 4.822 sec/batch)
2018-05-04 07:22:11.480546: step 47600, loss = 18.30 (3.3 examples/sec; 4.886 sec/batch)
2018-05-04 07:23:04.658325: step 47610, loss = 18.24 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 07:23:55.147111: step 47620, loss = 17.82 (3.2 examples/sec; 4.929 sec/batch)
2018-05-04 07:24:44.401084: step 47630, loss = 18.19 (3.2 examples/sec; 5.023 sec/batch)
2018-05-04 07:25:33.288231: step 47640, loss = 18.13 (3.2 examples/sec; 5.032 sec/batch)
2018-05-04 07:26:22.621539: step 47650, loss = 18.34 (3.3 examples/sec; 4.908 sec/batch)
2018-05-04 07:27:11.550679: step 47660, loss = 18.04 (3.4 examples/sec; 4.680 sec/batch)
2018-05-04 07:28:00.944228: step 47670, loss = 18.16 (3.2 examples/sec; 4.983 sec/batch)
2018-05-04 07:28:50.644635: step 47680, loss = 18.65 (3.1 examples/sec; 5.156 sec/batch)
2018-05-04 07:29:37.340345: step 47690, loss = 18.06 (3.2 examples/sec; 4.943 sec/batch)
2018-05-04 07:30:27.242906: step 47700, loss = 17.88 (3.3 examples/sec; 4.884 sec/batch)
2018-05-04 07:31:21.229921: step 47710, loss = 18.18 (3.2 examples/sec; 5.003 sec/batch)
2018-05-04 07:32:10.838896: step 47720, loss = 18.19 (3.3 examples/sec; 4.840 sec/batch)
2018-05-04 07:33:00.986932: step 47730, loss = 18.44 (3.2 examples/sec; 5.043 sec/batch)
2018-05-04 07:33:50.316547: step 47740, loss = 18.11 (3.2 examples/sec; 5.049 sec/batch)
2018-05-04 07:34:40.430651: step 47750, loss = 18.74 (3.1 examples/sec; 5.088 sec/batch)
2018-05-04 07:35:30.477734: step 47760, loss = 18.20 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 07:36:20.126256: step 47770, loss = 18.11 (3.4 examples/sec; 4.667 sec/batch)
2018-05-04 07:37:10.071584: step 47780, loss = 18.25 (3.2 examples/sec; 5.032 sec/batch)
2018-05-04 07:37:59.159868: step 47790, loss = 18.77 (3.2 examples/sec; 5.052 sec/batch)
2018-05-04 07:38:48.444801: step 47800, loss = 18.01 (3.2 examples/sec; 5.012 sec/batch)
2018-05-04 07:39:39.164639: step 47810, loss = 17.80 (3.2 examples/sec; 4.998 sec/batch)
2018-05-04 07:40:27.996089: step 47820, loss = 17.94 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 07:41:17.799180: step 47830, loss = 18.07 (3.3 examples/sec; 4.880 sec/batch)
2018-05-04 07:42:07.138172: step 47840, loss = 18.27 (3.2 examples/sec; 5.068 sec/batch)
2018-05-04 07:42:57.324183: step 47850, loss = 18.20 (3.2 examples/sec; 5.006 sec/batch)
2018-05-04 07:43:46.700250: step 47860, loss = 18.84 (3.2 examples/sec; 4.927 sec/batch)
2018-05-04 07:44:36.498810: step 47870, loss = 18.14 (3.2 examples/sec; 4.944 sec/batch)
2018-05-04 07:45:25.459213: step 47880, loss = 18.53 (3.3 examples/sec; 4.810 sec/batch)
2018-05-04 07:46:14.248937: step 47890, loss = 18.38 (3.3 examples/sec; 4.896 sec/batch)
2018-05-04 07:47:04.120782: step 47900, loss = 17.94 (3.3 examples/sec; 4.898 sec/batch)
2018-05-04 07:47:57.418568: step 47910, loss = 18.34 (3.3 examples/sec; 4.905 sec/batch)
2018-05-04 07:48:46.389507: step 47920, loss = 17.77 (3.4 examples/sec; 4.733 sec/batch)
2018-05-04 07:49:33.039841: step 47930, loss = 18.71 (3.5 examples/sec; 4.623 sec/batch)
2018-05-04 07:50:22.566821: step 47940, loss = 18.63 (3.2 examples/sec; 4.955 sec/batch)
2018-05-04 07:51:12.138798: step 47950, loss = 18.10 (3.3 examples/sec; 4.801 sec/batch)
2018-05-04 07:52:01.424175: step 47960, loss = 17.94 (3.3 examples/sec; 4.921 sec/batch)
2018-05-04 07:52:50.515058: step 47970, loss = 18.00 (3.2 examples/sec; 4.974 sec/batch)
2018-05-04 07:53:40.526278: step 47980, loss = 18.22 (3.2 examples/sec; 4.950 sec/batch)
2018-05-04 07:54:30.183830: step 47990, loss = 18.43 (3.2 examples/sec; 4.944 sec/batch)
2018-05-04 07:55:19.341877: step 48000, loss = 18.26 (3.3 examples/sec; 4.908 sec/batch)
2018-05-04 07:56:11.829098: step 48010, loss = 18.71 (3.3 examples/sec; 4.850 sec/batch)
2018-05-04 07:57:01.225228: step 48020, loss = 17.89 (3.3 examples/sec; 4.880 sec/batch)
2018-05-04 07:57:50.928764: step 48030, loss = 17.91 (3.1 examples/sec; 5.191 sec/batch)
2018-05-04 07:58:39.915309: step 48040, loss = 18.20 (3.3 examples/sec; 4.915 sec/batch)
2018-05-04 07:59:29.318522: step 48050, loss = 18.00 (3.3 examples/sec; 4.791 sec/batch)
2018-05-04 08:00:15.952165: step 48060, loss = 18.00 (3.3 examples/sec; 4.862 sec/batch)
2018-05-04 08:01:05.852880: step 48070, loss = 17.79 (3.1 examples/sec; 5.174 sec/batch)
2018-05-04 08:01:54.787490: step 48080, loss = 17.75 (3.2 examples/sec; 4.954 sec/batch)
2018-05-04 08:02:44.308249: step 48090, loss = 18.23 (3.2 examples/sec; 5.003 sec/batch)
2018-05-04 08:03:34.242598: step 48100, loss = 18.60 (3.2 examples/sec; 4.926 sec/batch)
2018-05-04 08:04:27.525401: step 48110, loss = 17.85 (3.2 examples/sec; 5.008 sec/batch)
2018-05-04 08:05:16.777960: step 48120, loss = 18.00 (3.1 examples/sec; 5.093 sec/batch)
2018-05-04 08:06:06.374137: step 48130, loss = 18.02 (3.2 examples/sec; 4.978 sec/batch)
2018-05-04 08:06:56.359434: step 48140, loss = 18.37 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 08:07:46.058269: step 48150, loss = 18.28 (3.3 examples/sec; 4.819 sec/batch)
2018-05-04 08:08:35.167575: step 48160, loss = 18.05 (3.1 examples/sec; 5.119 sec/batch)
2018-05-04 08:09:23.853959: step 48170, loss = 18.54 (3.2 examples/sec; 4.948 sec/batch)
2018-05-04 08:10:10.214281: step 48180, loss = 18.22 (3.2 examples/sec; 4.951 sec/batch)
2018-05-04 08:11:00.231923: step 48190, loss = 18.16 (3.1 examples/sec; 5.117 sec/batch)
2018-05-04 08:11:49.359263: step 48200, loss = 18.53 (3.3 examples/sec; 4.859 sec/batch)
2018-05-04 08:12:42.300695: step 48210, loss = 17.83 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 08:13:31.735780: step 48220, loss = 18.12 (3.4 examples/sec; 4.737 sec/batch)
2018-05-04 08:14:22.243518: step 48230, loss = 18.60 (3.2 examples/sec; 4.948 sec/batch)
2018-05-04 08:15:12.010654: step 48240, loss = 18.72 (3.3 examples/sec; 4.824 sec/batch)
2018-05-04 08:16:01.101857: step 48250, loss = 18.18 (3.2 examples/sec; 4.925 sec/batch)
2018-05-04 08:16:50.990073: step 48260, loss = 18.69 (3.3 examples/sec; 4.831 sec/batch)
2018-05-04 08:17:41.000955: step 48270, loss = 18.19 (3.2 examples/sec; 4.943 sec/batch)
2018-05-04 08:18:30.986243: step 48280, loss = 18.51 (3.4 examples/sec; 4.743 sec/batch)
2018-05-04 08:19:20.295308: step 48290, loss = 18.36 (3.2 examples/sec; 5.048 sec/batch)
2018-05-04 08:20:08.859592: step 48300, loss = 18.17 (3.9 examples/sec; 4.069 sec/batch)
2018-05-04 08:21:00.401017: step 48310, loss = 18.02 (3.2 examples/sec; 5.064 sec/batch)
2018-05-04 08:21:49.508914: step 48320, loss = 18.01 (3.3 examples/sec; 4.901 sec/batch)
2018-05-04 08:22:39.227539: step 48330, loss = 18.31 (3.1 examples/sec; 5.131 sec/batch)
2018-05-04 08:23:28.394592: step 48340, loss = 18.22 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 08:24:18.000997: step 48350, loss = 18.55 (3.2 examples/sec; 5.030 sec/batch)
2018-05-04 08:25:07.359329: step 48360, loss = 18.15 (3.3 examples/sec; 4.915 sec/batch)
2018-05-04 08:25:57.456422: step 48370, loss = 17.84 (3.3 examples/sec; 4.895 sec/batch)
2018-05-04 08:26:47.167474: step 48380, loss = 18.49 (3.1 examples/sec; 5.206 sec/batch)
2018-05-04 08:27:35.724419: step 48390, loss = 18.31 (3.3 examples/sec; 4.814 sec/batch)
2018-05-04 08:28:25.225421: step 48400, loss = 18.02 (3.2 examples/sec; 4.924 sec/batch)
2018-05-04 08:29:17.680488: step 48410, loss = 18.15 (3.4 examples/sec; 4.716 sec/batch)
2018-05-04 08:30:07.283176: step 48420, loss = 18.19 (3.2 examples/sec; 4.978 sec/batch)
2018-05-04 08:30:53.441165: step 48430, loss = 18.14 (3.2 examples/sec; 4.933 sec/batch)
2018-05-04 08:31:42.860900: step 48440, loss = 18.79 (3.2 examples/sec; 4.985 sec/batch)
2018-05-04 08:32:32.555557: step 48450, loss = 18.52 (3.2 examples/sec; 4.956 sec/batch)
2018-05-04 08:33:21.760931: step 48460, loss = 17.82 (3.3 examples/sec; 4.787 sec/batch)
2018-05-04 08:34:11.861053: step 48470, loss = 18.36 (3.0 examples/sec; 5.246 sec/batch)
2018-05-04 08:35:01.308735: step 48480, loss = 18.03 (3.3 examples/sec; 4.870 sec/batch)
2018-05-04 08:35:50.583985: step 48490, loss = 18.35 (3.2 examples/sec; 5.002 sec/batch)
2018-05-04 08:36:39.833678: step 48500, loss = 17.90 (3.3 examples/sec; 4.892 sec/batch)
2018-05-04 08:37:32.749344: step 48510, loss = 18.23 (3.3 examples/sec; 4.779 sec/batch)
2018-05-04 08:38:22.023850: step 48520, loss = 18.69 (3.4 examples/sec; 4.739 sec/batch)
2018-05-04 08:39:12.002617: step 48530, loss = 18.59 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 08:40:01.391473: step 48540, loss = 17.81 (3.3 examples/sec; 4.904 sec/batch)
2018-05-04 08:40:47.605548: step 48550, loss = 18.33 (4.2 examples/sec; 3.775 sec/batch)
2018-05-04 08:41:36.940072: step 48560, loss = 18.60 (3.2 examples/sec; 4.961 sec/batch)
2018-05-04 08:42:25.690431: step 48570, loss = 18.35 (3.3 examples/sec; 4.870 sec/batch)
2018-05-04 08:43:15.288727: step 48580, loss = 18.15 (3.2 examples/sec; 5.041 sec/batch)
2018-05-04 08:44:04.855805: step 48590, loss = 18.28 (3.3 examples/sec; 4.782 sec/batch)
2018-05-04 08:44:53.674760: step 48600, loss = 18.19 (3.3 examples/sec; 4.840 sec/batch)
2018-05-04 08:45:46.322238: step 48610, loss = 18.32 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 08:46:35.284581: step 48620, loss = 18.20 (3.2 examples/sec; 4.964 sec/batch)
2018-05-04 08:47:24.772133: step 48630, loss = 18.52 (3.3 examples/sec; 4.874 sec/batch)
2018-05-04 08:48:14.081958: step 48640, loss = 17.91 (3.2 examples/sec; 4.976 sec/batch)
2018-05-04 08:49:04.332845: step 48650, loss = 18.47 (3.1 examples/sec; 5.098 sec/batch)
2018-05-04 08:49:53.904244: step 48660, loss = 18.25 (3.2 examples/sec; 5.005 sec/batch)
2018-05-04 08:50:43.169900: step 48670, loss = 18.48 (3.4 examples/sec; 4.736 sec/batch)
2018-05-04 08:51:29.750146: step 48680, loss = 18.04 (3.2 examples/sec; 4.978 sec/batch)
2018-05-04 08:52:19.719449: step 48690, loss = 17.92 (3.2 examples/sec; 5.007 sec/batch)
2018-05-04 08:53:11.723737: step 48700, loss = 18.34 (3.2 examples/sec; 4.950 sec/batch)
2018-05-04 08:54:05.365998: step 48710, loss = 18.16 (3.1 examples/sec; 5.115 sec/batch)
2018-05-04 08:54:55.255656: step 48720, loss = 17.74 (3.2 examples/sec; 5.075 sec/batch)
2018-05-04 08:55:44.952151: step 48730, loss = 17.99 (3.4 examples/sec; 4.766 sec/batch)
2018-05-04 08:56:35.002404: step 48740, loss = 18.36 (3.3 examples/sec; 4.876 sec/batch)
2018-05-04 08:57:24.026955: step 48750, loss = 18.15 (3.2 examples/sec; 4.960 sec/batch)
2018-05-04 08:58:13.845340: step 48760, loss = 17.89 (3.2 examples/sec; 4.957 sec/batch)
2018-05-04 08:59:03.383039: step 48770, loss = 18.02 (3.2 examples/sec; 5.026 sec/batch)
2018-05-04 08:59:53.445151: step 48780, loss = 19.17 (3.3 examples/sec; 4.885 sec/batch)
2018-05-04 09:00:42.237086: step 48790, loss = 18.38 (3.3 examples/sec; 4.909 sec/batch)
2018-05-04 09:01:28.837834: step 48800, loss = 18.77 (3.2 examples/sec; 4.933 sec/batch)
2018-05-04 09:02:21.590762: step 48810, loss = 18.06 (3.2 examples/sec; 4.957 sec/batch)
2018-05-04 09:03:11.355141: step 48820, loss = 18.19 (3.3 examples/sec; 4.860 sec/batch)
2018-05-04 09:04:00.568303: step 48830, loss = 18.12 (3.3 examples/sec; 4.839 sec/batch)
2018-05-04 09:04:49.882717: step 48840, loss = 18.11 (3.2 examples/sec; 4.996 sec/batch)
2018-05-04 09:05:39.465000: step 48850, loss = 18.60 (3.2 examples/sec; 4.955 sec/batch)
2018-05-04 09:06:28.755182: step 48860, loss = 18.22 (3.2 examples/sec; 5.008 sec/batch)
2018-05-04 09:07:17.851386: step 48870, loss = 17.79 (3.4 examples/sec; 4.755 sec/batch)
2018-05-04 09:08:06.722303: step 48880, loss = 18.39 (3.2 examples/sec; 5.005 sec/batch)
2018-05-04 09:08:56.072513: step 48890, loss = 18.15 (3.2 examples/sec; 5.052 sec/batch)
2018-05-04 09:09:46.282242: step 48900, loss = 17.93 (3.3 examples/sec; 4.869 sec/batch)
2018-05-04 09:10:39.055388: step 48910, loss = 18.38 (3.3 examples/sec; 4.838 sec/batch)
2018-05-04 09:11:28.852926: step 48920, loss = 18.00 (3.2 examples/sec; 4.985 sec/batch)
2018-05-04 09:12:14.884298: step 48930, loss = 17.89 (3.3 examples/sec; 4.868 sec/batch)
2018-05-04 09:13:04.615874: step 48940, loss = 18.16 (3.2 examples/sec; 4.934 sec/batch)
2018-05-04 09:13:53.740180: step 48950, loss = 18.15 (3.3 examples/sec; 4.891 sec/batch)
2018-05-04 09:14:43.148555: step 48960, loss = 18.31 (3.2 examples/sec; 5.046 sec/batch)
2018-05-04 09:15:32.317187: step 48970, loss = 17.89 (3.3 examples/sec; 4.917 sec/batch)
2018-05-04 09:16:21.805364: step 48980, loss = 18.66 (3.2 examples/sec; 5.012 sec/batch)
2018-05-04 09:17:11.306766: step 48990, loss = 18.10 (3.2 examples/sec; 4.971 sec/batch)
2018-05-04 09:18:00.725639: step 49000, loss = 17.94 (3.3 examples/sec; 4.850 sec/batch)
2018-05-04 09:18:53.788171: step 49010, loss = 18.31 (3.3 examples/sec; 4.817 sec/batch)
2018-05-04 09:19:43.058796: step 49020, loss = 18.26 (3.2 examples/sec; 5.036 sec/batch)
2018-05-04 09:20:32.867637: step 49030, loss = 18.11 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 09:21:22.754758: step 49040, loss = 18.70 (3.1 examples/sec; 5.105 sec/batch)
2018-05-04 09:22:09.633518: step 49050, loss = 18.14 (3.2 examples/sec; 5.078 sec/batch)
2018-05-04 09:22:59.244133: step 49060, loss = 17.98 (3.2 examples/sec; 4.936 sec/batch)
2018-05-04 09:23:48.777666: step 49070, loss = 18.75 (3.3 examples/sec; 4.841 sec/batch)
2018-05-04 09:24:37.907390: step 49080, loss = 18.09 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 09:25:26.922890: step 49090, loss = 18.61 (3.3 examples/sec; 4.875 sec/batch)
2018-05-04 09:26:15.965078: step 49100, loss = 18.57 (3.4 examples/sec; 4.756 sec/batch)
2018-05-04 09:27:08.848602: step 49110, loss = 17.81 (3.2 examples/sec; 4.970 sec/batch)
2018-05-04 09:27:58.271613: step 49120, loss = 18.22 (3.2 examples/sec; 4.963 sec/batch)
2018-05-04 09:28:48.556101: step 49130, loss = 18.39 (3.2 examples/sec; 4.981 sec/batch)
2018-05-04 09:29:37.965367: step 49140, loss = 18.14 (3.2 examples/sec; 4.926 sec/batch)
2018-05-04 09:30:27.470327: step 49150, loss = 18.45 (3.1 examples/sec; 5.151 sec/batch)
2018-05-04 09:31:16.617077: step 49160, loss = 17.77 (3.4 examples/sec; 4.755 sec/batch)
2018-05-04 09:32:06.073408: step 49170, loss = 18.22 (3.4 examples/sec; 4.747 sec/batch)
2018-05-04 09:32:53.783630: step 49180, loss = 17.73 (3.1 examples/sec; 5.096 sec/batch)
2018-05-04 09:33:43.425027: step 49190, loss = 18.54 (3.3 examples/sec; 4.778 sec/batch)
2018-05-04 09:34:33.290481: step 49200, loss = 18.36 (3.2 examples/sec; 4.991 sec/batch)
2018-05-04 09:35:26.236038: step 49210, loss = 18.21 (3.3 examples/sec; 4.834 sec/batch)
2018-05-04 09:36:15.748243: step 49220, loss = 18.38 (3.2 examples/sec; 5.044 sec/batch)
2018-05-04 09:37:05.237263: step 49230, loss = 18.47 (3.2 examples/sec; 4.982 sec/batch)
2018-05-04 09:37:54.516669: step 49240, loss = 18.62 (3.2 examples/sec; 4.999 sec/batch)
2018-05-04 09:38:43.754241: step 49250, loss = 17.87 (3.3 examples/sec; 4.900 sec/batch)
2018-05-04 09:39:33.918095: step 49260, loss = 17.86 (3.3 examples/sec; 4.822 sec/batch)
2018-05-04 09:40:23.087333: step 49270, loss = 18.03 (3.2 examples/sec; 4.975 sec/batch)
2018-05-04 09:41:12.506315: step 49280, loss = 18.85 (3.4 examples/sec; 4.751 sec/batch)
2018-05-04 09:42:02.045444: step 49290, loss = 18.12 (3.2 examples/sec; 4.960 sec/batch)
2018-05-04 09:42:47.722311: step 49300, loss = 18.22 (3.3 examples/sec; 4.855 sec/batch)
2018-05-04 09:43:40.509512: step 49310, loss = 18.17 (3.2 examples/sec; 4.989 sec/batch)
2018-05-04 09:44:30.562553: step 49320, loss = 18.47 (3.2 examples/sec; 4.924 sec/batch)
2018-05-04 09:45:20.251298: step 49330, loss = 17.71 (3.3 examples/sec; 4.862 sec/batch)
2018-05-04 09:46:09.645303: step 49340, loss = 18.47 (3.3 examples/sec; 4.898 sec/batch)
2018-05-04 09:46:58.667629: step 49350, loss = 17.82 (3.2 examples/sec; 4.934 sec/batch)
2018-05-04 09:47:48.139267: step 49360, loss = 17.93 (3.2 examples/sec; 4.934 sec/batch)
2018-05-04 09:48:37.886991: step 49370, loss = 18.17 (3.2 examples/sec; 4.953 sec/batch)
2018-05-04 09:49:26.698459: step 49380, loss = 18.20 (3.3 examples/sec; 4.856 sec/batch)
2018-05-04 09:50:16.183073: step 49390, loss = 18.37 (3.2 examples/sec; 5.005 sec/batch)
2018-05-04 09:51:05.945694: step 49400, loss = 18.61 (3.2 examples/sec; 4.977 sec/batch)
2018-05-04 09:51:59.290757: step 49410, loss = 18.01 (3.2 examples/sec; 5.071 sec/batch)
2018-05-04 09:52:45.348095: step 49420, loss = 17.95 (4.3 examples/sec; 3.691 sec/batch)
2018-05-04 09:53:33.925748: step 49430, loss = 18.65 (3.3 examples/sec; 4.842 sec/batch)
2018-05-04 09:54:23.079255: step 49440, loss = 18.37 (3.4 examples/sec; 4.760 sec/batch)
2018-05-04 09:55:12.023916: step 49450, loss = 17.95 (3.3 examples/sec; 4.858 sec/batch)
2018-05-04 09:56:01.271190: step 49460, loss = 18.06 (3.3 examples/sec; 4.913 sec/batch)
2018-05-04 09:56:50.289345: step 49470, loss = 18.27 (3.2 examples/sec; 5.056 sec/batch)
2018-05-04 09:57:39.778894: step 49480, loss = 18.11 (3.2 examples/sec; 5.027 sec/batch)
2018-05-04 09:58:28.809790: step 49490, loss = 17.84 (3.3 examples/sec; 4.901 sec/batch)
2018-05-04 09:59:18.059344: step 49500, loss = 18.30 (3.2 examples/sec; 5.059 sec/batch)
2018-05-04 10:00:10.723625: step 49510, loss = 17.86 (3.2 examples/sec; 5.018 sec/batch)
2018-05-04 10:00:59.894819: step 49520, loss = 18.50 (3.4 examples/sec; 4.718 sec/batch)
2018-05-04 10:01:49.330112: step 49530, loss = 18.34 (3.2 examples/sec; 4.946 sec/batch)
2018-05-04 10:02:38.900854: step 49540, loss = 17.73 (3.3 examples/sec; 4.858 sec/batch)
2018-05-04 10:03:24.552076: step 49550, loss = 18.19 (3.3 examples/sec; 4.881 sec/batch)
2018-05-04 10:04:13.018676: step 49560, loss = 18.09 (3.4 examples/sec; 4.704 sec/batch)
2018-05-04 10:05:01.970275: step 49570, loss = 17.80 (3.2 examples/sec; 4.989 sec/batch)
2018-05-04 10:05:51.175639: step 49580, loss = 18.11 (3.3 examples/sec; 4.875 sec/batch)
2018-05-04 10:06:40.328758: step 49590, loss = 18.26 (3.2 examples/sec; 4.936 sec/batch)
2018-05-04 10:07:29.483715: step 49600, loss = 18.10 (3.2 examples/sec; 5.013 sec/batch)
2018-05-04 10:08:22.182628: step 49610, loss = 17.87 (3.3 examples/sec; 4.833 sec/batch)
2018-05-04 10:09:11.253019: step 49620, loss = 18.20 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 10:10:00.346667: step 49630, loss = 17.77 (3.2 examples/sec; 4.937 sec/batch)
2018-05-04 10:10:50.462896: step 49640, loss = 18.41 (3.2 examples/sec; 5.067 sec/batch)
2018-05-04 10:11:40.295383: step 49650, loss = 18.11 (3.3 examples/sec; 4.914 sec/batch)
2018-05-04 10:12:29.985273: step 49660, loss = 17.78 (3.2 examples/sec; 4.946 sec/batch)
2018-05-04 10:13:19.325736: step 49670, loss = 18.50 (3.2 examples/sec; 4.978 sec/batch)
2018-05-04 10:14:04.928819: step 49680, loss = 18.37 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 10:14:53.814949: step 49690, loss = 18.33 (3.2 examples/sec; 5.056 sec/batch)
2018-05-04 10:15:43.779992: step 49700, loss = 18.58 (3.2 examples/sec; 5.025 sec/batch)
2018-05-04 10:16:37.012106: step 49710, loss = 18.51 (3.1 examples/sec; 5.084 sec/batch)
2018-05-04 10:17:26.242875: step 49720, loss = 17.89 (3.1 examples/sec; 5.131 sec/batch)
2018-05-04 10:18:15.705698: step 49730, loss = 18.09 (3.2 examples/sec; 4.957 sec/batch)
2018-05-04 10:19:04.896436: step 49740, loss = 18.29 (3.1 examples/sec; 5.104 sec/batch)
2018-05-04 10:19:54.792730: step 49750, loss = 18.36 (3.2 examples/sec; 4.952 sec/batch)
2018-05-04 10:20:44.004450: step 49760, loss = 18.83 (3.2 examples/sec; 5.022 sec/batch)
2018-05-04 10:21:33.221866: step 49770, loss = 18.02 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 10:22:22.854709: step 49780, loss = 17.83 (3.2 examples/sec; 4.969 sec/batch)
2018-05-04 10:23:12.062322: step 49790, loss = 18.68 (3.2 examples/sec; 4.935 sec/batch)
2018-05-04 10:23:58.035300: step 49800, loss = 18.59 (3.2 examples/sec; 5.070 sec/batch)
2018-05-04 10:24:51.095375: step 49810, loss = 18.34 (3.0 examples/sec; 5.413 sec/batch)
2018-05-04 10:25:40.423376: step 49820, loss = 18.02 (3.2 examples/sec; 4.945 sec/batch)
2018-05-04 10:26:29.931201: step 49830, loss = 18.09 (3.2 examples/sec; 4.948 sec/batch)
2018-05-04 10:27:19.567600: step 49840, loss = 18.30 (3.2 examples/sec; 4.956 sec/batch)
2018-05-04 10:28:08.252317: step 49850, loss = 18.77 (3.3 examples/sec; 4.789 sec/batch)
2018-05-04 10:28:57.537233: step 49860, loss = 18.00 (3.2 examples/sec; 4.991 sec/batch)
2018-05-04 10:29:47.217949: step 49870, loss = 18.14 (3.2 examples/sec; 5.008 sec/batch)
2018-05-04 10:30:36.636416: step 49880, loss = 18.31 (3.2 examples/sec; 5.018 sec/batch)
2018-05-04 10:31:25.626719: step 49890, loss = 18.13 (3.2 examples/sec; 4.954 sec/batch)
2018-05-04 10:32:14.690400: step 49900, loss = 17.74 (3.2 examples/sec; 4.949 sec/batch)
2018-05-04 10:33:07.800984: step 49910, loss = 18.24 (3.2 examples/sec; 5.010 sec/batch)
2018-05-04 10:33:57.254693: step 49920, loss = 18.67 (3.3 examples/sec; 4.810 sec/batch)
2018-05-04 10:34:43.133684: step 49930, loss = 17.85 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 10:35:31.874607: step 49940, loss = 18.30 (3.3 examples/sec; 4.841 sec/batch)
2018-05-04 10:36:21.736855: step 49950, loss = 18.00 (3.3 examples/sec; 4.913 sec/batch)
2018-05-04 10:37:10.769710: step 49960, loss = 18.49 (3.2 examples/sec; 4.961 sec/batch)
2018-05-04 10:37:59.800235: step 49970, loss = 18.09 (3.3 examples/sec; 4.851 sec/batch)
2018-05-04 10:38:48.985962: step 49980, loss = 18.26 (3.2 examples/sec; 5.059 sec/batch)
2018-05-04 10:39:38.665263: step 49990, loss = 18.64 (3.2 examples/sec; 4.949 sec/batch)
2018-05-04 10:40:27.650837: step 50000, loss = 18.61 (3.3 examples/sec; 4.834 sec/batch)
2018-05-04 10:41:20.811136: step 50010, loss = 17.93 (3.2 examples/sec; 4.978 sec/batch)
2018-05-04 10:42:09.901736: step 50020, loss = 18.39 (3.2 examples/sec; 5.011 sec/batch)
2018-05-04 10:42:59.472386: step 50030, loss = 18.00 (3.2 examples/sec; 4.936 sec/batch)
2018-05-04 10:43:49.613617: step 50040, loss = 17.96 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 10:44:35.707133: step 50050, loss = 18.27 (3.3 examples/sec; 4.836 sec/batch)
2018-05-04 10:45:24.977820: step 50060, loss = 18.15 (3.2 examples/sec; 4.927 sec/batch)
2018-05-04 10:46:14.717365: step 50070, loss = 18.23 (3.2 examples/sec; 4.993 sec/batch)
2018-05-04 10:47:04.565550: step 50080, loss = 18.71 (3.1 examples/sec; 5.108 sec/batch)
2018-05-04 10:47:53.861224: step 50090, loss = 17.91 (3.3 examples/sec; 4.832 sec/batch)
2018-05-04 10:48:43.403904: step 50100, loss = 18.19 (3.2 examples/sec; 4.979 sec/batch)
2018-05-04 10:49:36.669062: step 50110, loss = 18.37 (3.2 examples/sec; 4.953 sec/batch)
2018-05-04 10:50:26.784858: step 50120, loss = 17.96 (3.2 examples/sec; 5.015 sec/batch)
2018-05-04 10:51:16.020307: step 50130, loss = 18.19 (3.5 examples/sec; 4.584 sec/batch)
2018-05-04 10:52:05.217584: step 50140, loss = 18.65 (3.2 examples/sec; 4.974 sec/batch)
2018-05-04 10:52:55.190036: step 50150, loss = 18.00 (3.3 examples/sec; 4.820 sec/batch)
2018-05-04 10:53:44.726675: step 50160, loss = 18.15 (3.2 examples/sec; 5.022 sec/batch)
2018-05-04 10:54:32.690194: step 50170, loss = 17.77 (4.1 examples/sec; 3.869 sec/batch)
2018-05-04 10:55:21.139334: step 50180, loss = 18.04 (3.2 examples/sec; 4.995 sec/batch)
2018-05-04 10:56:10.414305: step 50190, loss = 18.17 (3.3 examples/sec; 4.836 sec/batch)
2018-05-04 10:57:00.436176: step 50200, loss = 18.48 (3.1 examples/sec; 5.109 sec/batch)
2018-05-04 10:57:53.920680: step 50210, loss = 18.38 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 10:58:43.611682: step 50220, loss = 17.79 (3.2 examples/sec; 4.989 sec/batch)
2018-05-04 10:59:33.409412: step 50230, loss = 18.36 (3.3 examples/sec; 4.827 sec/batch)
2018-05-04 11:00:22.826294: step 50240, loss = 18.32 (3.2 examples/sec; 4.999 sec/batch)
2018-05-04 11:01:12.106750: step 50250, loss = 18.31 (3.3 examples/sec; 4.813 sec/batch)
2018-05-04 11:02:02.259164: step 50260, loss = 18.11 (3.2 examples/sec; 5.041 sec/batch)
2018-05-04 11:02:52.094562: step 50270, loss = 17.84 (3.2 examples/sec; 5.070 sec/batch)
2018-05-04 11:03:41.592556: step 50280, loss = 18.02 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 11:04:31.036945: step 50290, loss = 17.85 (3.3 examples/sec; 4.915 sec/batch)
2018-05-04 11:05:17.234959: step 50300, loss = 18.14 (3.2 examples/sec; 4.927 sec/batch)
2018-05-04 11:06:10.171284: step 50310, loss = 18.24 (3.2 examples/sec; 4.969 sec/batch)
2018-05-04 11:07:00.055185: step 50320, loss = 17.84 (3.2 examples/sec; 4.994 sec/batch)
2018-05-04 11:07:48.871227: step 50330, loss = 18.02 (3.3 examples/sec; 4.777 sec/batch)
2018-05-04 11:08:38.624811: step 50340, loss = 18.31 (3.1 examples/sec; 5.087 sec/batch)
2018-05-04 11:09:27.898640: step 50350, loss = 18.16 (3.2 examples/sec; 4.969 sec/batch)
2018-05-04 11:10:17.116083: step 50360, loss = 18.06 (3.3 examples/sec; 4.848 sec/batch)
2018-05-04 11:11:07.196800: step 50370, loss = 18.33 (3.3 examples/sec; 4.878 sec/batch)
2018-05-04 11:11:56.325960: step 50380, loss = 18.26 (3.2 examples/sec; 4.969 sec/batch)
2018-05-04 11:12:45.199912: step 50390, loss = 18.09 (3.2 examples/sec; 4.946 sec/batch)
2018-05-04 11:13:34.815719: step 50400, loss = 18.46 (3.2 examples/sec; 4.927 sec/batch)
2018-05-04 11:14:27.143619: step 50410, loss = 18.07 (3.3 examples/sec; 4.852 sec/batch)
2018-05-04 11:15:13.216968: step 50420, loss = 18.09 (3.3 examples/sec; 4.863 sec/batch)
2018-05-04 11:16:02.073540: step 50430, loss = 18.24 (3.4 examples/sec; 4.718 sec/batch)
2018-05-04 11:16:50.853425: step 50440, loss = 17.91 (3.3 examples/sec; 4.782 sec/batch)
2018-05-04 11:17:40.467149: step 50450, loss = 17.79 (3.3 examples/sec; 4.849 sec/batch)
2018-05-04 11:18:29.751545: step 50460, loss = 17.73 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 11:19:18.920313: step 50470, loss = 17.91 (3.3 examples/sec; 4.876 sec/batch)
2018-05-04 11:20:08.379027: step 50480, loss = 17.83 (3.2 examples/sec; 5.071 sec/batch)
2018-05-04 11:20:57.088423: step 50490, loss = 18.05 (3.3 examples/sec; 4.855 sec/batch)
2018-05-04 11:21:45.571544: step 50500, loss = 18.04 (3.3 examples/sec; 4.783 sec/batch)
2018-05-04 11:22:38.317091: step 50510, loss = 17.98 (3.2 examples/sec; 4.933 sec/batch)
2018-05-04 11:23:27.501999: step 50520, loss = 18.23 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 11:24:16.292806: step 50530, loss = 18.51 (3.2 examples/sec; 4.954 sec/batch)
2018-05-04 11:25:05.396737: step 50540, loss = 17.84 (3.2 examples/sec; 5.070 sec/batch)
2018-05-04 11:25:51.913003: step 50550, loss = 18.07 (3.2 examples/sec; 4.927 sec/batch)
2018-05-04 11:26:41.544934: step 50560, loss = 18.37 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 11:27:30.946508: step 50570, loss = 18.21 (3.3 examples/sec; 4.834 sec/batch)
2018-05-04 11:28:20.141577: step 50580, loss = 18.17 (3.4 examples/sec; 4.761 sec/batch)
2018-05-04 11:29:09.355275: step 50590, loss = 17.93 (3.1 examples/sec; 5.081 sec/batch)
2018-05-04 11:29:58.467807: step 50600, loss = 17.71 (3.3 examples/sec; 4.891 sec/batch)
2018-05-04 11:30:50.703248: step 50610, loss = 18.12 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 11:31:40.106881: step 50620, loss = 18.54 (3.3 examples/sec; 4.885 sec/batch)
2018-05-04 11:32:29.792113: step 50630, loss = 18.74 (3.3 examples/sec; 4.874 sec/batch)
2018-05-04 11:33:19.355578: step 50640, loss = 18.33 (3.2 examples/sec; 4.993 sec/batch)
2018-05-04 11:34:08.923494: step 50650, loss = 18.09 (3.3 examples/sec; 4.879 sec/batch)
2018-05-04 11:34:58.176355: step 50660, loss = 18.10 (3.3 examples/sec; 4.870 sec/batch)
2018-05-04 11:35:44.446430: step 50670, loss = 17.99 (3.3 examples/sec; 4.865 sec/batch)
2018-05-04 11:36:33.687757: step 50680, loss = 18.10 (3.3 examples/sec; 4.912 sec/batch)
2018-05-04 11:37:22.954852: step 50690, loss = 18.35 (3.2 examples/sec; 4.998 sec/batch)
2018-05-04 11:38:11.910228: step 50700, loss = 18.01 (3.3 examples/sec; 4.885 sec/batch)
2018-05-04 11:39:05.357882: step 50710, loss = 18.29 (3.3 examples/sec; 4.824 sec/batch)
2018-05-04 11:39:55.209010: step 50720, loss = 18.09 (3.2 examples/sec; 4.956 sec/batch)
2018-05-04 11:40:44.282800: step 50730, loss = 17.79 (3.3 examples/sec; 4.822 sec/batch)
2018-05-04 11:41:33.714219: step 50740, loss = 18.37 (3.3 examples/sec; 4.840 sec/batch)
2018-05-04 11:42:22.425598: step 50750, loss = 18.51 (3.3 examples/sec; 4.871 sec/batch)
2018-05-04 11:43:12.596557: step 50760, loss = 18.11 (3.3 examples/sec; 4.848 sec/batch)
2018-05-04 11:44:02.188993: step 50770, loss = 17.88 (3.3 examples/sec; 4.836 sec/batch)
2018-05-04 11:44:51.223055: step 50780, loss = 18.09 (3.3 examples/sec; 4.922 sec/batch)
2018-05-04 11:45:40.248152: step 50790, loss = 18.04 (3.2 examples/sec; 5.069 sec/batch)
2018-05-04 11:46:26.092693: step 50800, loss = 18.17 (3.1 examples/sec; 5.116 sec/batch)
2018-05-04 11:47:19.555701: step 50810, loss = 17.98 (3.3 examples/sec; 4.884 sec/batch)
2018-05-04 11:48:08.701169: step 50820, loss = 17.98 (3.3 examples/sec; 4.847 sec/batch)
2018-05-04 11:48:58.087064: step 50830, loss = 17.86 (3.3 examples/sec; 4.901 sec/batch)
2018-05-04 11:49:47.138817: step 50840, loss = 18.70 (3.3 examples/sec; 4.874 sec/batch)
2018-05-04 11:50:36.157020: step 50850, loss = 18.15 (3.2 examples/sec; 5.047 sec/batch)
2018-05-04 11:51:25.183322: step 50860, loss = 18.29 (3.3 examples/sec; 4.898 sec/batch)
2018-05-04 11:52:14.044897: step 50870, loss = 17.88 (3.3 examples/sec; 4.895 sec/batch)
2018-05-04 11:53:02.855011: step 50880, loss = 18.26 (3.3 examples/sec; 4.782 sec/batch)
2018-05-04 11:53:52.089334: step 50890, loss = 18.23 (3.4 examples/sec; 4.748 sec/batch)
2018-05-04 11:54:41.630573: step 50900, loss = 18.05 (3.2 examples/sec; 4.938 sec/batch)
2018-05-04 11:55:34.061223: step 50910, loss = 18.50 (3.3 examples/sec; 4.843 sec/batch)
2018-05-04 11:56:19.685402: step 50920, loss = 18.28 (3.2 examples/sec; 4.958 sec/batch)
2018-05-04 11:57:09.315874: step 50930, loss = 17.78 (3.3 examples/sec; 4.909 sec/batch)
2018-05-04 11:57:58.551357: step 50940, loss = 17.88 (3.2 examples/sec; 4.997 sec/batch)
2018-05-04 11:58:47.840531: step 50950, loss = 18.45 (3.2 examples/sec; 4.924 sec/batch)
2018-05-04 11:59:36.786401: step 50960, loss = 18.08 (3.3 examples/sec; 4.875 sec/batch)
2018-05-04 12:00:26.291191: step 50970, loss = 18.36 (3.2 examples/sec; 4.998 sec/batch)
2018-05-04 12:01:15.441740: step 50980, loss = 18.27 (3.2 examples/sec; 5.031 sec/batch)
2018-05-04 12:02:04.968883: step 50990, loss = 18.43 (3.3 examples/sec; 4.897 sec/batch)
2018-05-04 12:02:54.607232: step 51000, loss = 18.33 (3.3 examples/sec; 4.901 sec/batch)
2018-05-04 12:03:47.056356: step 51010, loss = 17.77 (3.4 examples/sec; 4.722 sec/batch)
2018-05-04 12:04:35.767177: step 51020, loss = 18.56 (3.3 examples/sec; 4.882 sec/batch)
2018-05-04 12:05:25.303860: step 51030, loss = 17.90 (3.1 examples/sec; 5.131 sec/batch)
2018-05-04 12:06:15.117679: step 51040, loss = 18.59 (3.2 examples/sec; 4.950 sec/batch)
2018-05-04 12:07:01.245925: step 51050, loss = 18.12 (3.3 examples/sec; 4.889 sec/batch)
2018-05-04 12:07:50.374081: step 51060, loss = 18.03 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 12:08:39.330170: step 51070, loss = 18.73 (3.2 examples/sec; 4.973 sec/batch)
2018-05-04 12:09:27.745005: step 51080, loss = 18.22 (3.5 examples/sec; 4.635 sec/batch)
2018-05-04 12:10:17.342003: step 51090, loss = 18.64 (3.0 examples/sec; 5.311 sec/batch)
2018-05-04 12:11:06.899192: step 51100, loss = 18.75 (3.1 examples/sec; 5.158 sec/batch)
2018-05-04 12:11:59.827802: step 51110, loss = 18.07 (3.2 examples/sec; 4.971 sec/batch)
2018-05-04 12:12:49.942808: step 51120, loss = 18.26 (3.1 examples/sec; 5.097 sec/batch)
2018-05-04 12:13:39.352884: step 51130, loss = 18.16 (3.2 examples/sec; 5.026 sec/batch)
2018-05-04 12:14:28.895544: step 51140, loss = 18.25 (3.2 examples/sec; 4.971 sec/batch)
2018-05-04 12:15:18.451995: step 51150, loss = 18.27 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 12:16:07.124090: step 51160, loss = 18.22 (3.4 examples/sec; 4.686 sec/batch)
2018-05-04 12:16:53.018587: step 51170, loss = 18.01 (3.4 examples/sec; 4.747 sec/batch)
2018-05-04 12:17:42.919587: step 51180, loss = 19.20 (3.2 examples/sec; 5.050 sec/batch)
2018-05-04 12:18:32.635619: step 51190, loss = 18.68 (3.2 examples/sec; 4.925 sec/batch)
2018-05-04 12:19:21.594706: step 51200, loss = 18.42 (3.3 examples/sec; 4.913 sec/batch)
2018-05-04 12:20:14.581298: step 51210, loss = 17.97 (3.4 examples/sec; 4.754 sec/batch)
2018-05-04 12:21:04.227919: step 51220, loss = 17.82 (3.1 examples/sec; 5.115 sec/batch)
2018-05-04 12:21:53.278447: step 51230, loss = 18.14 (3.2 examples/sec; 4.949 sec/batch)
2018-05-04 12:22:43.220876: step 51240, loss = 18.18 (3.2 examples/sec; 5.077 sec/batch)
2018-05-04 12:23:32.941671: step 51250, loss = 17.85 (3.2 examples/sec; 4.993 sec/batch)
2018-05-04 12:24:22.288626: step 51260, loss = 18.12 (3.2 examples/sec; 4.951 sec/batch)
2018-05-04 12:25:11.241314: step 51270, loss = 18.12 (3.3 examples/sec; 4.877 sec/batch)
2018-05-04 12:26:00.565349: step 51280, loss = 18.41 (3.2 examples/sec; 4.959 sec/batch)
2018-05-04 12:26:49.962636: step 51290, loss = 17.93 (3.3 examples/sec; 4.857 sec/batch)
2018-05-04 12:27:35.984109: step 51300, loss = 18.33 (3.4 examples/sec; 4.775 sec/batch)
2018-05-04 12:28:29.451182: step 51310, loss = 18.43 (3.3 examples/sec; 4.874 sec/batch)
2018-05-04 12:29:18.686883: step 51320, loss = 18.35 (3.3 examples/sec; 4.877 sec/batch)
2018-05-04 12:30:08.274780: step 51330, loss = 18.14 (3.3 examples/sec; 4.831 sec/batch)
2018-05-04 12:30:57.632385: step 51340, loss = 18.15 (3.3 examples/sec; 4.900 sec/batch)
2018-05-04 12:31:46.714158: step 51350, loss = 18.36 (3.3 examples/sec; 4.897 sec/batch)
2018-05-04 12:32:35.903735: step 51360, loss = 18.12 (3.3 examples/sec; 4.795 sec/batch)
2018-05-04 12:33:25.217520: step 51370, loss = 18.70 (3.3 examples/sec; 4.803 sec/batch)
2018-05-04 12:34:14.404756: step 51380, loss = 17.98 (3.2 examples/sec; 5.077 sec/batch)
2018-05-04 12:35:03.991116: step 51390, loss = 18.06 (3.3 examples/sec; 4.846 sec/batch)
2018-05-04 12:35:53.528903: step 51400, loss = 17.91 (3.4 examples/sec; 4.701 sec/batch)
2018-05-04 12:36:46.252180: step 51410, loss = 18.61 (3.2 examples/sec; 4.933 sec/batch)
2018-05-04 12:37:32.314403: step 51420, loss = 17.72 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 12:38:21.506254: step 51430, loss = 18.37 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 12:39:10.582602: step 51440, loss = 18.18 (3.4 examples/sec; 4.746 sec/batch)
2018-05-04 12:40:00.636538: step 51450, loss = 18.34 (3.3 examples/sec; 4.879 sec/batch)
2018-05-04 12:40:49.722982: step 51460, loss = 17.98 (3.3 examples/sec; 4.917 sec/batch)
2018-05-04 12:41:38.845584: step 51470, loss = 18.04 (3.3 examples/sec; 4.887 sec/batch)
2018-05-04 12:42:28.844321: step 51480, loss = 17.81 (3.2 examples/sec; 5.010 sec/batch)
2018-05-04 12:43:18.129287: step 51490, loss = 18.03 (3.2 examples/sec; 5.015 sec/batch)
2018-05-04 12:44:06.993406: step 51500, loss = 18.73 (3.2 examples/sec; 5.028 sec/batch)
2018-05-04 12:44:59.655066: step 51510, loss = 18.12 (3.3 examples/sec; 4.878 sec/batch)
2018-05-04 12:45:48.682143: step 51520, loss = 18.27 (3.4 examples/sec; 4.755 sec/batch)
2018-05-04 12:46:38.373581: step 51530, loss = 18.08 (3.1 examples/sec; 5.087 sec/batch)
2018-05-04 12:47:27.414800: step 51540, loss = 17.82 (4.0 examples/sec; 3.998 sec/batch)
2018-05-04 12:48:14.536311: step 51550, loss = 18.83 (3.3 examples/sec; 4.816 sec/batch)
2018-05-04 12:49:04.403333: step 51560, loss = 18.17 (3.2 examples/sec; 5.002 sec/batch)
2018-05-04 12:49:53.720820: step 51570, loss = 18.27 (3.4 examples/sec; 4.749 sec/batch)
2018-05-04 12:50:43.111301: step 51580, loss = 17.93 (3.2 examples/sec; 5.058 sec/batch)
2018-05-04 12:51:32.481124: step 51590, loss = 17.83 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 12:52:21.791425: step 51600, loss = 18.17 (3.3 examples/sec; 4.812 sec/batch)
2018-05-04 12:53:15.201617: step 51610, loss = 17.74 (3.2 examples/sec; 4.986 sec/batch)
2018-05-04 12:54:04.888781: step 51620, loss = 18.73 (3.3 examples/sec; 4.831 sec/batch)
2018-05-04 12:54:54.651016: step 51630, loss = 17.93 (3.3 examples/sec; 4.880 sec/batch)
2018-05-04 12:55:45.010178: step 51640, loss = 18.49 (3.0 examples/sec; 5.369 sec/batch)
2018-05-04 12:56:34.984749: step 51650, loss = 18.16 (3.2 examples/sec; 5.037 sec/batch)
2018-05-04 12:57:24.536211: step 51660, loss = 18.42 (3.2 examples/sec; 4.995 sec/batch)
2018-05-04 12:58:11.150696: step 51670, loss = 18.21 (3.2 examples/sec; 5.070 sec/batch)
2018-05-04 12:59:01.400463: step 51680, loss = 18.24 (3.2 examples/sec; 5.052 sec/batch)
2018-05-04 12:59:50.771330: step 51690, loss = 18.04 (3.3 examples/sec; 4.834 sec/batch)
2018-05-04 13:00:40.174182: step 51700, loss = 18.18 (3.2 examples/sec; 4.985 sec/batch)
2018-05-04 13:01:33.636199: step 51710, loss = 17.96 (3.2 examples/sec; 4.990 sec/batch)
2018-05-04 13:02:22.874628: step 51720, loss = 18.28 (3.3 examples/sec; 4.896 sec/batch)
2018-05-04 13:03:13.114513: step 51730, loss = 17.97 (3.2 examples/sec; 4.980 sec/batch)
2018-05-04 13:04:02.635632: step 51740, loss = 17.85 (3.2 examples/sec; 4.958 sec/batch)
2018-05-04 13:04:51.985075: step 51750, loss = 17.96 (3.2 examples/sec; 5.066 sec/batch)
2018-05-04 13:05:41.105537: step 51760, loss = 18.49 (3.2 examples/sec; 4.965 sec/batch)
2018-05-04 13:06:29.932324: step 51770, loss = 18.19 (3.2 examples/sec; 4.961 sec/batch)
2018-05-04 13:07:19.138423: step 51780, loss = 18.07 (3.3 examples/sec; 4.787 sec/batch)
2018-05-04 13:08:06.244744: step 51790, loss = 18.23 (4.1 examples/sec; 3.877 sec/batch)
2018-05-04 13:08:55.386967: step 51800, loss = 18.02 (3.2 examples/sec; 5.046 sec/batch)
2018-05-04 13:09:48.012509: step 51810, loss = 17.87 (3.3 examples/sec; 4.874 sec/batch)
2018-05-04 13:10:37.216895: step 51820, loss = 18.27 (3.2 examples/sec; 5.034 sec/batch)
2018-05-04 13:11:26.316444: step 51830, loss = 17.79 (3.2 examples/sec; 4.941 sec/batch)
2018-05-04 13:12:15.572905: step 51840, loss = 18.79 (3.3 examples/sec; 4.815 sec/batch)
2018-05-04 13:13:04.827455: step 51850, loss = 17.76 (3.3 examples/sec; 4.790 sec/batch)
2018-05-04 13:13:54.240285: step 51860, loss = 17.90 (3.2 examples/sec; 4.978 sec/batch)
2018-05-04 13:14:43.065308: step 51870, loss = 18.28 (3.2 examples/sec; 4.989 sec/batch)
2018-05-04 13:15:32.340621: step 51880, loss = 17.97 (3.2 examples/sec; 4.931 sec/batch)
2018-05-04 13:16:21.161423: step 51890, loss = 18.28 (3.3 examples/sec; 4.848 sec/batch)
2018-05-04 13:17:10.289067: step 51900, loss = 18.83 (3.3 examples/sec; 4.914 sec/batch)
2018-05-04 13:18:02.660132: step 51910, loss = 17.61 (3.2 examples/sec; 4.945 sec/batch)
2018-05-04 13:18:48.874475: step 51920, loss = 18.84 (3.1 examples/sec; 5.148 sec/batch)
2018-05-04 13:19:38.376043: step 51930, loss = 18.12 (3.3 examples/sec; 4.866 sec/batch)
2018-05-04 13:20:28.365157: step 51940, loss = 18.24 (3.2 examples/sec; 5.011 sec/batch)
2018-05-04 13:21:17.293003: step 51950, loss = 17.77 (3.2 examples/sec; 4.974 sec/batch)
2018-05-04 13:22:07.112460: step 51960, loss = 18.82 (3.1 examples/sec; 5.101 sec/batch)
2018-05-04 13:22:56.598427: step 51970, loss = 18.22 (3.2 examples/sec; 5.015 sec/batch)
2018-05-04 13:23:46.185989: step 51980, loss = 18.73 (3.1 examples/sec; 5.135 sec/batch)
2018-05-04 13:24:35.554873: step 51990, loss = 18.14 (3.3 examples/sec; 4.918 sec/batch)
2018-05-04 13:25:24.849445: step 52000, loss = 18.85 (3.2 examples/sec; 5.033 sec/batch)
2018-05-04 13:26:17.832087: step 52010, loss = 18.34 (3.3 examples/sec; 4.882 sec/batch)
2018-05-04 13:27:07.263521: step 52020, loss = 18.27 (3.3 examples/sec; 4.798 sec/batch)
2018-05-04 13:27:56.810397: step 52030, loss = 17.92 (3.1 examples/sec; 5.087 sec/batch)
2018-05-04 13:28:43.230002: step 52040, loss = 17.97 (3.9 examples/sec; 4.094 sec/batch)
2018-05-04 13:29:32.652560: step 52050, loss = 17.82 (3.2 examples/sec; 4.927 sec/batch)
2018-05-04 13:30:22.192671: step 52060, loss = 17.90 (3.1 examples/sec; 5.174 sec/batch)
2018-05-04 13:31:11.721858: step 52070, loss = 17.87 (3.2 examples/sec; 4.978 sec/batch)
2018-05-04 13:32:00.923385: step 52080, loss = 17.86 (3.1 examples/sec; 5.138 sec/batch)
2018-05-04 13:32:50.875259: step 52090, loss = 17.65 (3.2 examples/sec; 4.926 sec/batch)
2018-05-04 13:33:40.198007: step 52100, loss = 18.24 (3.3 examples/sec; 4.889 sec/batch)
2018-05-04 13:34:33.479822: step 52110, loss = 18.35 (3.3 examples/sec; 4.836 sec/batch)
2018-05-04 13:35:22.926370: step 52120, loss = 18.27 (3.4 examples/sec; 4.719 sec/batch)
2018-05-04 13:36:11.851871: step 52130, loss = 18.06 (3.3 examples/sec; 4.807 sec/batch)
2018-05-04 13:37:01.305742: step 52140, loss = 18.18 (3.3 examples/sec; 4.886 sec/batch)
2018-05-04 13:37:51.247275: step 52150, loss = 17.88 (3.2 examples/sec; 4.979 sec/batch)
2018-05-04 13:38:41.166833: step 52160, loss = 18.90 (3.3 examples/sec; 4.851 sec/batch)
2018-05-04 13:39:27.188294: step 52170, loss = 17.83 (3.2 examples/sec; 4.965 sec/batch)
2018-05-04 13:40:17.071069: step 52180, loss = 17.93 (3.2 examples/sec; 5.074 sec/batch)
2018-05-04 13:41:06.713732: step 52190, loss = 18.09 (3.2 examples/sec; 4.961 sec/batch)
2018-05-04 13:41:56.299611: step 52200, loss = 17.89 (3.2 examples/sec; 5.009 sec/batch)
2018-05-04 13:42:48.998523: step 52210, loss = 18.56 (3.4 examples/sec; 4.697 sec/batch)
2018-05-04 13:43:37.854264: step 52220, loss = 18.01 (3.3 examples/sec; 4.878 sec/batch)
2018-05-04 13:44:28.055759: step 52230, loss = 17.79 (3.2 examples/sec; 4.942 sec/batch)
2018-05-04 13:45:17.408318: step 52240, loss = 17.91 (3.3 examples/sec; 4.851 sec/batch)
2018-05-04 13:46:06.876479: step 52250, loss = 17.89 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 13:46:55.708212: step 52260, loss = 18.58 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 13:47:45.324584: step 52270, loss = 17.93 (3.2 examples/sec; 5.052 sec/batch)
2018-05-04 13:48:35.081686: step 52280, loss = 18.06 (3.1 examples/sec; 5.111 sec/batch)
2018-05-04 13:49:21.402993: step 52290, loss = 17.95 (3.4 examples/sec; 4.721 sec/batch)
2018-05-04 13:50:10.153262: step 52300, loss = 18.61 (3.3 examples/sec; 4.890 sec/batch)
2018-05-04 13:51:02.922946: step 52310, loss = 17.77 (3.2 examples/sec; 4.960 sec/batch)
2018-05-04 13:51:52.649567: step 52320, loss = 18.17 (3.2 examples/sec; 5.014 sec/batch)
2018-05-04 13:52:42.509700: step 52330, loss = 18.18 (3.2 examples/sec; 5.000 sec/batch)
2018-05-04 13:53:32.450688: step 52340, loss = 18.12 (3.2 examples/sec; 5.035 sec/batch)
2018-05-04 13:54:21.838619: step 52350, loss = 18.15 (3.5 examples/sec; 4.601 sec/batch)
2018-05-04 13:55:12.123161: step 52360, loss = 18.39 (3.2 examples/sec; 4.956 sec/batch)
2018-05-04 13:56:01.639882: step 52370, loss = 18.05 (3.2 examples/sec; 4.946 sec/batch)
2018-05-04 13:56:51.572677: step 52380, loss = 18.14 (3.1 examples/sec; 5.159 sec/batch)
2018-05-04 13:57:39.968149: step 52390, loss = 18.10 (3.3 examples/sec; 4.897 sec/batch)
2018-05-04 13:58:29.707406: step 52400, loss = 18.48 (3.3 examples/sec; 4.907 sec/batch)
2018-05-04 13:59:20.341198: step 52410, loss = 18.15 (4.2 examples/sec; 3.812 sec/batch)
2018-05-04 14:00:08.117551: step 52420, loss = 17.85 (3.3 examples/sec; 4.793 sec/batch)
2018-05-04 14:00:57.563080: step 52430, loss = 18.05 (3.3 examples/sec; 4.804 sec/batch)
2018-05-04 14:01:46.953504: step 52440, loss = 18.26 (3.2 examples/sec; 4.928 sec/batch)
2018-05-04 14:02:36.671887: step 52450, loss = 17.78 (3.3 examples/sec; 4.897 sec/batch)
2018-05-04 14:03:26.830418: step 52460, loss = 18.23 (3.2 examples/sec; 4.984 sec/batch)
2018-05-04 14:04:16.617724: step 52470, loss = 17.95 (3.3 examples/sec; 4.859 sec/batch)
2018-05-04 14:05:06.528344: step 52480, loss = 18.17 (3.0 examples/sec; 5.272 sec/batch)
2018-05-04 14:05:54.902454: step 52490, loss = 18.28 (3.2 examples/sec; 5.003 sec/batch)
2018-05-04 14:06:44.467041: step 52500, loss = 17.80 (3.2 examples/sec; 4.964 sec/batch)
2018-05-04 14:07:37.725340: step 52510, loss = 18.22 (3.2 examples/sec; 4.994 sec/batch)
2018-05-04 14:08:27.099258: step 52520, loss = 18.68 (3.4 examples/sec; 4.696 sec/batch)
2018-05-04 14:09:16.613901: step 52530, loss = 18.28 (3.2 examples/sec; 4.954 sec/batch)
2018-05-04 14:10:02.965768: step 52540, loss = 18.04 (3.3 examples/sec; 4.914 sec/batch)
2018-05-04 14:10:52.664207: step 52550, loss = 17.98 (3.3 examples/sec; 4.800 sec/batch)
2018-05-04 14:11:42.147330: step 52560, loss = 18.67 (3.3 examples/sec; 4.892 sec/batch)
2018-05-04 14:12:31.936616: step 52570, loss = 18.20 (3.2 examples/sec; 4.949 sec/batch)
2018-05-04 14:13:20.836232: step 52580, loss = 18.35 (3.4 examples/sec; 4.718 sec/batch)
2018-05-04 14:14:09.875823: step 52590, loss = 18.50 (3.3 examples/sec; 4.809 sec/batch)
2018-05-04 14:15:00.268684: step 52600, loss = 18.25 (3.2 examples/sec; 5.033 sec/batch)
2018-05-04 14:15:53.722328: step 52610, loss = 18.15 (3.2 examples/sec; 5.044 sec/batch)
2018-05-04 14:16:43.450588: step 52620, loss = 17.83 (3.2 examples/sec; 5.030 sec/batch)
2018-05-04 14:17:32.666818: step 52630, loss = 18.50 (3.4 examples/sec; 4.762 sec/batch)
2018-05-04 14:18:22.287201: step 52640, loss = 17.92 (3.2 examples/sec; 4.965 sec/batch)
2018-05-04 14:19:11.883424: step 52650, loss = 18.02 (3.3 examples/sec; 4.886 sec/batch)
2018-05-04 14:19:58.501337: step 52660, loss = 18.18 (3.9 examples/sec; 4.093 sec/batch)
2018-05-04 14:20:48.381857: step 52670, loss = 17.92 (3.2 examples/sec; 5.064 sec/batch)
2018-05-04 14:21:38.062143: step 52680, loss = 18.31 (3.3 examples/sec; 4.812 sec/batch)
2018-05-04 14:22:28.305081: step 52690, loss = 18.33 (3.3 examples/sec; 4.920 sec/batch)
2018-05-04 14:23:18.157002: step 52700, loss = 18.13 (3.2 examples/sec; 4.942 sec/batch)
2018-05-04 14:24:11.011046: step 52710, loss = 18.76 (3.3 examples/sec; 4.783 sec/batch)
2018-05-04 14:25:00.115145: step 52720, loss = 18.21 (3.3 examples/sec; 4.892 sec/batch)
2018-05-04 14:25:48.959802: step 52730, loss = 18.29 (3.2 examples/sec; 5.018 sec/batch)
2018-05-04 14:26:38.279820: step 52740, loss = 17.85 (3.3 examples/sec; 4.866 sec/batch)
2018-05-04 14:27:27.285595: step 52750, loss = 17.97 (3.4 examples/sec; 4.772 sec/batch)
2018-05-04 14:28:16.375211: step 52760, loss = 18.57 (3.2 examples/sec; 4.962 sec/batch)
2018-05-04 14:29:05.656252: step 52770, loss = 18.21 (3.2 examples/sec; 4.955 sec/batch)
2018-05-04 14:29:55.524423: step 52780, loss = 17.95 (3.2 examples/sec; 4.974 sec/batch)
2018-05-04 14:30:41.916218: step 52790, loss = 17.88 (3.3 examples/sec; 4.880 sec/batch)
2018-05-04 14:31:31.795179: step 52800, loss = 18.31 (3.3 examples/sec; 4.866 sec/batch)
2018-05-04 14:32:24.415071: step 52810, loss = 17.72 (3.2 examples/sec; 4.935 sec/batch)
2018-05-04 14:33:14.136797: step 52820, loss = 17.98 (3.2 examples/sec; 4.999 sec/batch)
2018-05-04 14:34:03.768092: step 52830, loss = 17.91 (3.2 examples/sec; 4.999 sec/batch)
2018-05-04 14:34:52.792798: step 52840, loss = 17.77 (3.3 examples/sec; 4.870 sec/batch)
2018-05-04 14:35:41.711333: step 52850, loss = 18.16 (3.2 examples/sec; 4.934 sec/batch)
2018-05-04 14:36:30.869056: step 52860, loss = 18.08 (3.3 examples/sec; 4.785 sec/batch)
2018-05-04 14:37:19.941881: step 52870, loss = 18.55 (3.3 examples/sec; 4.790 sec/batch)
2018-05-04 14:38:09.420973: step 52880, loss = 18.03 (3.3 examples/sec; 4.915 sec/batch)
2018-05-04 14:38:58.377923: step 52890, loss = 18.17 (3.4 examples/sec; 4.765 sec/batch)
2018-05-04 14:39:48.130315: step 52900, loss = 18.05 (3.2 examples/sec; 4.972 sec/batch)
2018-05-04 14:40:37.356120: step 52910, loss = 18.51 (4.1 examples/sec; 3.862 sec/batch)
2018-05-04 14:41:26.617041: step 52920, loss = 18.35 (3.3 examples/sec; 4.854 sec/batch)
2018-05-04 14:42:16.645793: step 52930, loss = 18.20 (3.2 examples/sec; 4.961 sec/batch)
2018-05-04 14:43:05.963088: step 52940, loss = 17.98 (3.3 examples/sec; 4.890 sec/batch)
2018-05-04 14:43:55.448451: step 52950, loss = 18.38 (3.2 examples/sec; 4.973 sec/batch)
2018-05-04 14:44:45.519978: step 52960, loss = 18.23 (3.1 examples/sec; 5.095 sec/batch)
2018-05-04 14:45:34.656881: step 52970, loss = 17.85 (3.3 examples/sec; 4.813 sec/batch)
2018-05-04 14:46:23.818827: step 52980, loss = 17.78 (3.2 examples/sec; 4.986 sec/batch)
2018-05-04 14:47:13.845750: step 52990, loss = 18.40 (3.2 examples/sec; 4.989 sec/batch)
2018-05-04 14:48:03.461739: step 53000, loss = 17.79 (3.2 examples/sec; 4.935 sec/batch)
2018-05-04 14:48:56.298221: step 53010, loss = 18.20 (3.4 examples/sec; 4.721 sec/batch)
2018-05-04 14:49:45.820763: step 53020, loss = 19.21 (3.2 examples/sec; 4.976 sec/batch)
2018-05-04 14:50:35.649860: step 53030, loss = 18.12 (3.3 examples/sec; 4.853 sec/batch)
2018-05-04 14:51:22.156673: step 53040, loss = 18.05 (3.3 examples/sec; 4.850 sec/batch)
2018-05-04 14:52:12.009354: step 53050, loss = 18.04 (3.3 examples/sec; 4.862 sec/batch)
2018-05-04 14:53:02.093394: step 53060, loss = 18.91 (3.2 examples/sec; 4.991 sec/batch)
2018-05-04 14:53:51.232651: step 53070, loss = 17.90 (3.4 examples/sec; 4.763 sec/batch)
2018-05-04 14:54:40.672615: step 53080, loss = 17.96 (3.2 examples/sec; 4.977 sec/batch)
2018-05-04 14:55:30.676507: step 53090, loss = 17.95 (3.3 examples/sec; 4.879 sec/batch)
2018-05-04 14:56:20.392099: step 53100, loss = 18.04 (3.2 examples/sec; 4.939 sec/batch)
2018-05-04 14:57:13.513894: step 53110, loss = 18.12 (3.3 examples/sec; 4.895 sec/batch)
2018-05-04 14:58:03.445467: step 53120, loss = 17.78 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 14:58:53.045119: step 53130, loss = 18.09 (3.3 examples/sec; 4.863 sec/batch)
2018-05-04 14:59:42.413407: step 53140, loss = 17.94 (3.2 examples/sec; 4.929 sec/batch)
2018-05-04 15:00:32.700338: step 53150, loss = 17.99 (3.2 examples/sec; 5.011 sec/batch)
2018-05-04 15:01:19.200921: step 53160, loss = 18.14 (3.2 examples/sec; 5.005 sec/batch)
2018-05-04 15:02:08.191113: step 53170, loss = 18.30 (3.3 examples/sec; 4.780 sec/batch)
2018-05-04 15:02:57.347929: step 53180, loss = 18.01 (3.2 examples/sec; 4.951 sec/batch)
2018-05-04 15:03:46.216843: step 53190, loss = 18.24 (3.3 examples/sec; 4.873 sec/batch)
2018-05-04 15:04:36.034554: step 53200, loss = 18.38 (3.2 examples/sec; 5.062 sec/batch)
2018-05-04 15:05:29.328308: step 53210, loss = 17.80 (3.2 examples/sec; 4.976 sec/batch)
2018-05-04 15:06:19.011313: step 53220, loss = 18.32 (3.2 examples/sec; 5.057 sec/batch)
2018-05-04 15:07:09.220388: step 53230, loss = 18.27 (3.2 examples/sec; 4.929 sec/batch)
2018-05-04 15:07:59.241124: step 53240, loss = 17.82 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 15:08:48.612159: step 53250, loss = 17.84 (3.1 examples/sec; 5.133 sec/batch)
2018-05-04 15:09:37.939634: step 53260, loss = 18.10 (3.2 examples/sec; 4.983 sec/batch)
2018-05-04 15:10:27.140369: step 53270, loss = 17.96 (3.3 examples/sec; 4.856 sec/batch)
2018-05-04 15:11:16.906117: step 53280, loss = 17.82 (3.2 examples/sec; 5.048 sec/batch)
2018-05-04 15:12:03.613460: step 53290, loss = 18.38 (3.3 examples/sec; 4.858 sec/batch)
2018-05-04 15:12:53.210360: step 53300, loss = 17.98 (3.3 examples/sec; 4.884 sec/batch)
2018-05-04 15:13:46.414253: step 53310, loss = 18.20 (3.2 examples/sec; 5.005 sec/batch)
2018-05-04 15:14:35.716975: step 53320, loss = 18.27 (3.3 examples/sec; 4.900 sec/batch)
2018-05-04 15:15:24.919493: step 53330, loss = 18.34 (3.2 examples/sec; 5.006 sec/batch)
2018-05-04 15:16:13.624494: step 53340, loss = 17.79 (3.3 examples/sec; 4.915 sec/batch)
2018-05-04 15:17:03.260063: step 53350, loss = 17.70 (3.2 examples/sec; 4.941 sec/batch)
2018-05-04 15:17:53.127460: step 53360, loss = 18.36 (3.1 examples/sec; 5.090 sec/batch)
2018-05-04 15:18:43.319854: step 53370, loss = 17.95 (3.2 examples/sec; 4.988 sec/batch)
2018-05-04 15:19:32.688721: step 53380, loss = 17.88 (3.3 examples/sec; 4.861 sec/batch)
2018-05-04 15:20:21.611861: step 53390, loss = 18.08 (3.3 examples/sec; 4.862 sec/batch)
2018-05-04 15:21:10.792474: step 53400, loss = 17.74 (3.3 examples/sec; 4.884 sec/batch)
2018-05-04 15:22:00.136691: step 53410, loss = 18.48 (3.3 examples/sec; 4.851 sec/batch)
2018-05-04 15:22:48.868740: step 53420, loss = 18.10 (3.4 examples/sec; 4.726 sec/batch)
2018-05-04 15:23:38.026831: step 53430, loss = 18.88 (3.4 examples/sec; 4.765 sec/batch)
2018-05-04 15:24:28.052055: step 53440, loss = 19.15 (3.2 examples/sec; 4.950 sec/batch)
2018-05-04 15:25:17.845716: step 53450, loss = 18.11 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 15:26:08.318819: step 53460, loss = 18.02 (3.1 examples/sec; 5.195 sec/batch)
2018-05-04 15:26:58.187213: step 53470, loss = 18.27 (3.2 examples/sec; 5.024 sec/batch)
2018-05-04 15:27:47.499344: step 53480, loss = 18.88 (3.3 examples/sec; 4.904 sec/batch)
2018-05-04 15:28:36.787036: step 53490, loss = 18.53 (3.2 examples/sec; 5.059 sec/batch)
2018-05-04 15:29:25.874565: step 53500, loss = 18.49 (3.3 examples/sec; 4.910 sec/batch)
2018-05-04 15:30:19.048797: step 53510, loss = 17.77 (3.2 examples/sec; 5.016 sec/batch)
2018-05-04 15:31:07.676809: step 53520, loss = 17.94 (3.3 examples/sec; 4.914 sec/batch)
2018-05-04 15:31:55.869674: step 53530, loss = 17.87 (3.9 examples/sec; 4.082 sec/batch)
2018-05-04 15:32:43.243992: step 53540, loss = 18.23 (3.2 examples/sec; 4.942 sec/batch)
2018-05-04 15:33:32.199168: step 53550, loss = 17.84 (3.1 examples/sec; 5.092 sec/batch)
2018-05-04 15:34:21.242459: step 53560, loss = 18.78 (3.2 examples/sec; 4.979 sec/batch)
2018-05-04 15:35:10.832895: step 53570, loss = 18.08 (3.1 examples/sec; 5.094 sec/batch)
2018-05-04 15:36:00.281856: step 53580, loss = 18.58 (3.2 examples/sec; 4.953 sec/batch)
2018-05-04 15:36:49.784857: step 53590, loss = 17.89 (3.2 examples/sec; 4.964 sec/batch)
2018-05-04 15:37:38.741744: step 53600, loss = 17.94 (3.3 examples/sec; 4.859 sec/batch)
2018-05-04 15:38:31.725423: step 53610, loss = 18.20 (3.2 examples/sec; 4.950 sec/batch)
2018-05-04 15:39:20.442135: step 53620, loss = 17.91 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 15:40:09.295701: step 53630, loss = 18.27 (3.3 examples/sec; 4.850 sec/batch)
2018-05-04 15:40:57.929343: step 53640, loss = 18.19 (3.4 examples/sec; 4.690 sec/batch)
2018-05-04 15:41:47.033725: step 53650, loss = 18.33 (3.4 examples/sec; 4.773 sec/batch)
2018-05-04 15:42:33.444998: step 53660, loss = 18.16 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 15:43:23.587095: step 53670, loss = 19.27 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 15:44:13.515734: step 53680, loss = 18.06 (3.3 examples/sec; 4.885 sec/batch)
2018-05-04 15:45:02.790777: step 53690, loss = 17.92 (3.2 examples/sec; 4.962 sec/batch)
2018-05-04 15:45:52.073164: step 53700, loss = 17.72 (3.3 examples/sec; 4.874 sec/batch)
2018-05-04 15:46:44.698793: step 53710, loss = 17.96 (3.3 examples/sec; 4.828 sec/batch)
2018-05-04 15:47:33.845617: step 53720, loss = 17.94 (3.2 examples/sec; 5.062 sec/batch)
2018-05-04 15:48:22.767963: step 53730, loss = 17.88 (3.2 examples/sec; 5.023 sec/batch)
2018-05-04 15:49:11.817249: step 53740, loss = 18.71 (3.3 examples/sec; 4.921 sec/batch)
2018-05-04 15:50:00.862912: step 53750, loss = 18.07 (3.3 examples/sec; 4.877 sec/batch)
2018-05-04 15:50:50.065167: step 53760, loss = 18.28 (3.2 examples/sec; 4.950 sec/batch)
2018-05-04 15:51:39.628361: step 53770, loss = 18.06 (3.3 examples/sec; 4.882 sec/batch)
2018-05-04 15:52:28.385215: step 53780, loss = 18.55 (3.3 examples/sec; 4.901 sec/batch)
2018-05-04 15:53:15.152907: step 53790, loss = 18.02 (3.1 examples/sec; 5.180 sec/batch)
2018-05-04 15:54:04.267388: step 53800, loss = 17.79 (3.3 examples/sec; 4.842 sec/batch)
2018-05-04 15:54:57.026377: step 53810, loss = 18.34 (3.3 examples/sec; 4.817 sec/batch)
2018-05-04 15:55:46.163092: step 53820, loss = 18.08 (3.3 examples/sec; 4.910 sec/batch)
2018-05-04 15:56:35.190806: step 53830, loss = 18.25 (3.3 examples/sec; 4.790 sec/batch)
2018-05-04 15:57:24.309615: step 53840, loss = 18.52 (3.3 examples/sec; 4.805 sec/batch)
2018-05-04 15:58:13.528209: step 53850, loss = 17.81 (3.3 examples/sec; 4.881 sec/batch)
2018-05-04 15:59:02.204486: step 53860, loss = 18.83 (3.3 examples/sec; 4.873 sec/batch)
2018-05-04 15:59:50.824597: step 53870, loss = 18.95 (3.2 examples/sec; 4.988 sec/batch)
2018-05-04 16:00:39.930966: step 53880, loss = 18.37 (3.3 examples/sec; 4.835 sec/batch)
2018-05-04 16:01:29.118269: step 53890, loss = 18.07 (3.3 examples/sec; 4.920 sec/batch)
2018-05-04 16:02:18.330033: step 53900, loss = 17.89 (3.3 examples/sec; 4.912 sec/batch)
2018-05-04 16:03:07.913744: step 53910, loss = 17.74 (3.2 examples/sec; 4.950 sec/batch)
2018-05-04 16:03:57.471273: step 53920, loss = 18.37 (3.2 examples/sec; 4.923 sec/batch)
2018-05-04 16:04:46.685092: step 53930, loss = 17.73 (3.3 examples/sec; 4.853 sec/batch)
2018-05-04 16:05:36.221486: step 53940, loss = 18.40 (3.3 examples/sec; 4.854 sec/batch)
2018-05-04 16:06:25.196829: step 53950, loss = 18.22 (3.3 examples/sec; 4.892 sec/batch)
2018-05-04 16:07:14.018194: step 53960, loss = 17.90 (3.3 examples/sec; 4.917 sec/batch)
2018-05-04 16:08:03.376034: step 53970, loss = 18.13 (3.3 examples/sec; 4.921 sec/batch)
2018-05-04 16:08:52.957616: step 53980, loss = 17.93 (3.2 examples/sec; 4.942 sec/batch)
2018-05-04 16:09:42.404879: step 53990, loss = 18.66 (3.1 examples/sec; 5.137 sec/batch)
2018-05-04 16:10:32.188101: step 54000, loss = 18.05 (3.0 examples/sec; 5.315 sec/batch)
2018-05-04 16:11:24.993774: step 54010, loss = 17.96 (3.3 examples/sec; 4.863 sec/batch)
2018-05-04 16:12:14.083348: step 54020, loss = 17.97 (3.4 examples/sec; 4.729 sec/batch)
2018-05-04 16:13:03.417870: step 54030, loss = 17.76 (3.3 examples/sec; 4.878 sec/batch)
2018-05-04 16:13:49.933156: step 54040, loss = 18.30 (3.1 examples/sec; 5.126 sec/batch)
2018-05-04 16:14:39.146785: step 54050, loss = 18.82 (3.2 examples/sec; 5.000 sec/batch)
2018-05-04 16:15:28.841200: step 54060, loss = 17.79 (3.3 examples/sec; 4.919 sec/batch)
2018-05-04 16:16:18.350546: step 54070, loss = 18.19 (3.2 examples/sec; 5.048 sec/batch)
2018-05-04 16:17:07.306133: step 54080, loss = 17.96 (3.2 examples/sec; 5.005 sec/batch)
2018-05-04 16:17:56.970687: step 54090, loss = 17.92 (3.2 examples/sec; 4.990 sec/batch)
2018-05-04 16:18:46.365930: step 54100, loss = 18.65 (3.1 examples/sec; 5.167 sec/batch)
2018-05-04 16:19:40.129294: step 54110, loss = 17.97 (3.2 examples/sec; 4.942 sec/batch)
2018-05-04 16:20:29.415283: step 54120, loss = 18.05 (3.2 examples/sec; 4.988 sec/batch)
2018-05-04 16:21:18.778090: step 54130, loss = 18.10 (3.2 examples/sec; 5.011 sec/batch)
2018-05-04 16:22:08.585028: step 54140, loss = 18.67 (3.3 examples/sec; 4.923 sec/batch)
2018-05-04 16:22:57.802146: step 54150, loss = 18.29 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 16:23:44.192554: step 54160, loss = 18.03 (3.2 examples/sec; 5.066 sec/batch)
2018-05-04 16:24:33.863537: step 54170, loss = 18.70 (3.3 examples/sec; 4.851 sec/batch)
2018-05-04 16:25:23.083777: step 54180, loss = 17.77 (3.2 examples/sec; 5.033 sec/batch)
2018-05-04 16:26:12.150692: step 54190, loss = 18.02 (3.3 examples/sec; 4.880 sec/batch)
2018-05-04 16:27:01.243667: step 54200, loss = 17.87 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 16:27:53.764528: step 54210, loss = 17.79 (3.4 examples/sec; 4.734 sec/batch)
2018-05-04 16:28:42.830118: step 54220, loss = 18.01 (3.4 examples/sec; 4.760 sec/batch)
2018-05-04 16:29:31.597011: step 54230, loss = 18.03 (3.3 examples/sec; 4.806 sec/batch)
2018-05-04 16:30:21.034078: step 54240, loss = 18.42 (3.2 examples/sec; 5.012 sec/batch)
2018-05-04 16:31:10.511436: step 54250, loss = 17.96 (3.3 examples/sec; 4.922 sec/batch)
2018-05-04 16:31:59.361105: step 54260, loss = 18.19 (3.2 examples/sec; 4.979 sec/batch)
2018-05-04 16:32:48.515113: step 54270, loss = 18.10 (3.3 examples/sec; 4.849 sec/batch)
2018-05-04 16:33:38.713343: step 54280, loss = 18.50 (3.2 examples/sec; 5.068 sec/batch)
2018-05-04 16:34:24.790760: step 54290, loss = 17.87 (3.2 examples/sec; 4.955 sec/batch)
2018-05-04 16:35:13.585083: step 54300, loss = 17.89 (3.3 examples/sec; 4.819 sec/batch)
2018-05-04 16:36:06.355379: step 54310, loss = 17.92 (3.3 examples/sec; 4.871 sec/batch)
2018-05-04 16:36:55.971319: step 54320, loss = 18.35 (3.3 examples/sec; 4.911 sec/batch)
2018-05-04 16:37:45.019647: step 54330, loss = 18.44 (3.2 examples/sec; 4.963 sec/batch)
2018-05-04 16:38:34.385849: step 54340, loss = 17.71 (3.3 examples/sec; 4.912 sec/batch)
2018-05-04 16:39:23.468699: step 54350, loss = 18.65 (3.3 examples/sec; 4.920 sec/batch)
2018-05-04 16:40:12.579643: step 54360, loss = 17.87 (3.3 examples/sec; 4.813 sec/batch)
2018-05-04 16:41:01.991773: step 54370, loss = 17.87 (3.2 examples/sec; 4.990 sec/batch)
2018-05-04 16:41:51.306514: step 54380, loss = 18.15 (3.3 examples/sec; 4.846 sec/batch)
2018-05-04 16:42:40.212082: step 54390, loss = 18.15 (3.3 examples/sec; 4.851 sec/batch)
2018-05-04 16:43:29.675279: step 54400, loss = 17.80 (3.3 examples/sec; 4.919 sec/batch)
2018-05-04 16:44:19.177115: step 54410, loss = 18.05 (4.1 examples/sec; 3.907 sec/batch)
2018-05-04 16:45:08.371089: step 54420, loss = 18.02 (3.2 examples/sec; 5.047 sec/batch)
2018-05-04 16:45:57.421864: step 54430, loss = 18.24 (3.2 examples/sec; 4.975 sec/batch)
2018-05-04 16:46:46.480601: step 54440, loss = 17.73 (3.3 examples/sec; 4.884 sec/batch)
2018-05-04 16:47:35.789387: step 54450, loss = 18.34 (3.2 examples/sec; 4.985 sec/batch)
2018-05-04 16:48:24.975324: step 54460, loss = 17.94 (3.2 examples/sec; 4.937 sec/batch)
2018-05-04 16:49:14.928657: step 54470, loss = 17.72 (3.2 examples/sec; 5.027 sec/batch)
2018-05-04 16:50:04.766085: step 54480, loss = 18.72 (3.2 examples/sec; 5.008 sec/batch)
2018-05-04 16:50:54.375835: step 54490, loss = 18.48 (3.3 examples/sec; 4.862 sec/batch)
2018-05-04 16:51:44.190313: step 54500, loss = 17.94 (3.2 examples/sec; 5.036 sec/batch)
2018-05-04 16:52:36.571843: step 54510, loss = 18.33 (3.3 examples/sec; 4.856 sec/batch)
2018-05-04 16:53:25.434821: step 54520, loss = 18.09 (3.3 examples/sec; 4.885 sec/batch)
2018-05-04 16:54:14.800801: step 54530, loss = 18.05 (3.3 examples/sec; 4.877 sec/batch)
2018-05-04 16:55:01.907687: step 54540, loss = 18.27 (3.2 examples/sec; 4.971 sec/batch)
2018-05-04 16:55:51.695655: step 54550, loss = 17.93 (3.2 examples/sec; 4.983 sec/batch)
2018-05-04 16:56:41.544191: step 54560, loss = 18.36 (3.3 examples/sec; 4.894 sec/batch)
2018-05-04 16:57:30.897669: step 54570, loss = 17.91 (3.2 examples/sec; 5.042 sec/batch)
2018-05-04 16:58:20.365710: step 54580, loss = 18.14 (3.3 examples/sec; 4.853 sec/batch)
2018-05-04 16:59:09.452581: step 54590, loss = 18.24 (3.2 examples/sec; 5.003 sec/batch)
2018-05-04 16:59:58.877129: step 54600, loss = 18.58 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 17:00:52.818436: step 54610, loss = 17.87 (3.1 examples/sec; 5.099 sec/batch)
2018-05-04 17:01:42.284141: step 54620, loss = 18.27 (3.1 examples/sec; 5.091 sec/batch)
2018-05-04 17:02:31.383096: step 54630, loss = 18.39 (3.2 examples/sec; 4.986 sec/batch)
2018-05-04 17:03:20.423500: step 54640, loss = 18.33 (3.3 examples/sec; 4.909 sec/batch)
2018-05-04 17:04:10.329265: step 54650, loss = 18.16 (3.2 examples/sec; 4.949 sec/batch)
2018-05-04 17:04:56.730962: step 54660, loss = 17.98 (3.4 examples/sec; 4.704 sec/batch)
2018-05-04 17:05:46.084062: step 54670, loss = 18.29 (3.3 examples/sec; 4.896 sec/batch)
2018-05-04 17:06:35.820423: step 54680, loss = 18.02 (3.2 examples/sec; 4.985 sec/batch)
2018-05-04 17:07:25.321060: step 54690, loss = 18.57 (3.3 examples/sec; 4.838 sec/batch)
2018-05-04 17:08:14.713582: step 54700, loss = 18.24 (3.1 examples/sec; 5.134 sec/batch)
2018-05-04 17:09:08.157399: step 54710, loss = 18.69 (3.2 examples/sec; 4.963 sec/batch)
2018-05-04 17:09:57.958947: step 54720, loss = 18.26 (3.2 examples/sec; 4.947 sec/batch)
2018-05-04 17:10:47.694061: step 54730, loss = 18.02 (3.2 examples/sec; 4.964 sec/batch)
2018-05-04 17:11:37.258464: step 54740, loss = 18.05 (3.2 examples/sec; 4.948 sec/batch)
2018-05-04 17:12:26.545432: step 54750, loss = 18.03 (3.2 examples/sec; 4.942 sec/batch)
2018-05-04 17:13:15.438352: step 54760, loss = 18.24 (3.4 examples/sec; 4.731 sec/batch)
2018-05-04 17:14:04.881509: step 54770, loss = 17.88 (3.2 examples/sec; 5.004 sec/batch)
2018-05-04 17:14:53.964653: step 54780, loss = 18.26 (3.1 examples/sec; 5.096 sec/batch)
2018-05-04 17:15:40.621430: step 54790, loss = 17.99 (3.3 examples/sec; 4.861 sec/batch)
2018-05-04 17:16:29.822803: step 54800, loss = 18.61 (3.3 examples/sec; 4.896 sec/batch)
2018-05-04 17:17:23.019363: step 54810, loss = 17.82 (3.2 examples/sec; 4.966 sec/batch)
2018-05-04 17:18:13.080561: step 54820, loss = 17.99 (3.1 examples/sec; 5.109 sec/batch)
2018-05-04 17:19:02.831751: step 54830, loss = 18.43 (3.1 examples/sec; 5.135 sec/batch)
2018-05-04 17:19:52.091381: step 54840, loss = 18.72 (3.3 examples/sec; 4.848 sec/batch)
2018-05-04 17:20:41.529139: step 54850, loss = 17.73 (3.3 examples/sec; 4.850 sec/batch)
2018-05-04 17:21:31.456538: step 54860, loss = 18.30 (3.2 examples/sec; 4.943 sec/batch)
2018-05-04 17:22:21.344305: step 54870, loss = 18.02 (3.2 examples/sec; 4.962 sec/batch)
2018-05-04 17:23:10.756637: step 54880, loss = 18.58 (3.2 examples/sec; 4.931 sec/batch)
2018-05-04 17:24:00.513046: step 54890, loss = 17.85 (3.2 examples/sec; 4.954 sec/batch)
2018-05-04 17:24:49.971999: step 54900, loss = 17.95 (3.5 examples/sec; 4.602 sec/batch)
2018-05-04 17:25:39.751624: step 54910, loss = 17.96 (3.4 examples/sec; 4.671 sec/batch)
2018-05-04 17:26:29.674536: step 54920, loss = 18.29 (3.3 examples/sec; 4.836 sec/batch)
2018-05-04 17:27:19.128304: step 54930, loss = 18.50 (3.2 examples/sec; 5.017 sec/batch)
2018-05-04 17:28:08.893475: step 54940, loss = 17.86 (3.2 examples/sec; 5.011 sec/batch)
2018-05-04 17:28:58.357402: step 54950, loss = 17.80 (3.2 examples/sec; 5.038 sec/batch)
2018-05-04 17:29:48.090930: step 54960, loss = 17.75 (3.3 examples/sec; 4.817 sec/batch)
2018-05-04 17:30:37.396880: step 54970, loss = 18.03 (3.2 examples/sec; 4.971 sec/batch)
2018-05-04 17:31:26.966412: step 54980, loss = 17.90 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 17:32:16.069973: step 54990, loss = 17.77 (3.2 examples/sec; 5.014 sec/batch)
2018-05-04 17:33:05.409995: step 55000, loss = 18.56 (3.3 examples/sec; 4.831 sec/batch)
2018-05-04 17:33:58.134121: step 55010, loss = 17.98 (3.3 examples/sec; 4.901 sec/batch)
2018-05-04 17:34:47.557605: step 55020, loss = 17.94 (3.3 examples/sec; 4.822 sec/batch)
2018-05-04 17:35:36.960411: step 55030, loss = 17.89 (3.9 examples/sec; 4.132 sec/batch)
2018-05-04 17:36:24.114633: step 55040, loss = 18.24 (3.2 examples/sec; 4.962 sec/batch)
2018-05-04 17:37:13.785773: step 55050, loss = 17.97 (3.3 examples/sec; 4.909 sec/batch)
2018-05-04 17:38:03.149938: step 55060, loss = 18.10 (3.2 examples/sec; 4.963 sec/batch)
2018-05-04 17:38:52.765771: step 55070, loss = 18.29 (3.3 examples/sec; 4.865 sec/batch)
2018-05-04 17:39:41.801946: step 55080, loss = 18.08 (3.1 examples/sec; 5.110 sec/batch)
2018-05-04 17:40:31.067271: step 55090, loss = 18.01 (3.2 examples/sec; 5.003 sec/batch)
2018-05-04 17:41:20.895506: step 55100, loss = 18.00 (3.2 examples/sec; 5.006 sec/batch)
2018-05-04 17:42:14.327275: step 55110, loss = 18.31 (3.1 examples/sec; 5.095 sec/batch)
2018-05-04 17:43:03.289920: step 55120, loss = 18.46 (3.3 examples/sec; 4.892 sec/batch)
2018-05-04 17:43:52.470354: step 55130, loss = 18.44 (3.2 examples/sec; 5.031 sec/batch)
2018-05-04 17:44:41.571193: step 55140, loss = 18.77 (3.2 examples/sec; 4.931 sec/batch)
2018-05-04 17:45:30.906893: step 55150, loss = 18.46 (3.3 examples/sec; 4.822 sec/batch)
2018-05-04 17:46:17.492278: step 55160, loss = 18.22 (3.2 examples/sec; 5.067 sec/batch)
2018-05-04 17:47:07.675727: step 55170, loss = 18.20 (3.2 examples/sec; 4.936 sec/batch)
2018-05-04 17:47:57.419028: step 55180, loss = 18.23 (3.2 examples/sec; 4.963 sec/batch)
2018-05-04 17:48:46.927581: step 55190, loss = 18.08 (3.2 examples/sec; 4.943 sec/batch)
2018-05-04 17:49:36.151509: step 55200, loss = 17.97 (3.2 examples/sec; 4.924 sec/batch)
2018-05-04 17:50:29.584886: step 55210, loss = 18.05 (3.2 examples/sec; 4.942 sec/batch)
2018-05-04 17:51:19.575787: step 55220, loss = 17.99 (3.2 examples/sec; 5.029 sec/batch)
2018-05-04 17:52:09.023020: step 55230, loss = 18.20 (3.3 examples/sec; 4.911 sec/batch)
2018-05-04 17:52:57.947193: step 55240, loss = 18.64 (3.3 examples/sec; 4.855 sec/batch)
2018-05-04 17:53:47.343368: step 55250, loss = 18.14 (3.1 examples/sec; 5.089 sec/batch)
2018-05-04 17:54:36.345156: step 55260, loss = 17.95 (3.3 examples/sec; 4.834 sec/batch)
2018-05-04 17:55:25.501895: step 55270, loss = 17.77 (3.2 examples/sec; 4.962 sec/batch)
2018-05-04 17:56:14.695442: step 55280, loss = 18.08 (3.3 examples/sec; 4.895 sec/batch)
2018-05-04 17:57:00.728002: step 55290, loss = 18.11 (3.2 examples/sec; 5.066 sec/batch)
2018-05-04 17:57:50.699727: step 55300, loss = 18.14 (3.2 examples/sec; 4.981 sec/batch)
2018-05-04 17:58:43.767588: step 55310, loss = 18.28 (3.2 examples/sec; 4.972 sec/batch)
2018-05-04 17:59:33.248020: step 55320, loss = 17.74 (3.3 examples/sec; 4.837 sec/batch)
2018-05-04 18:00:22.712521: step 55330, loss = 18.15 (3.3 examples/sec; 4.917 sec/batch)
2018-05-04 18:01:12.208163: step 55340, loss = 18.48 (3.2 examples/sec; 4.973 sec/batch)
2018-05-04 18:02:01.674318: step 55350, loss = 18.00 (3.4 examples/sec; 4.755 sec/batch)
2018-05-04 18:02:51.196587: step 55360, loss = 17.96 (3.3 examples/sec; 4.909 sec/batch)
2018-05-04 18:03:40.602039: step 55370, loss = 18.50 (3.2 examples/sec; 4.970 sec/batch)
2018-05-04 18:04:29.746722: step 55380, loss = 17.93 (3.3 examples/sec; 4.921 sec/batch)
2018-05-04 18:05:19.205301: step 55390, loss = 18.28 (3.2 examples/sec; 5.032 sec/batch)
2018-05-04 18:06:08.729293: step 55400, loss = 18.17 (3.2 examples/sec; 4.977 sec/batch)
2018-05-04 18:06:58.705770: step 55410, loss = 18.04 (3.2 examples/sec; 4.929 sec/batch)
2018-05-04 18:07:48.283774: step 55420, loss = 18.18 (3.2 examples/sec; 4.982 sec/batch)
2018-05-04 18:08:37.500250: step 55430, loss = 18.81 (3.2 examples/sec; 5.015 sec/batch)
2018-05-04 18:09:26.507814: step 55440, loss = 17.68 (3.2 examples/sec; 5.014 sec/batch)
2018-05-04 18:10:15.730960: step 55450, loss = 17.78 (3.3 examples/sec; 4.865 sec/batch)
2018-05-04 18:11:04.634490: step 55460, loss = 17.87 (3.2 examples/sec; 4.986 sec/batch)
2018-05-04 18:11:54.357399: step 55470, loss = 18.04 (3.2 examples/sec; 5.031 sec/batch)
2018-05-04 18:12:43.740918: step 55480, loss = 17.73 (3.3 examples/sec; 4.847 sec/batch)
2018-05-04 18:13:33.343146: step 55490, loss = 18.20 (3.2 examples/sec; 4.966 sec/batch)
2018-05-04 18:14:22.825860: step 55500, loss = 18.85 (3.3 examples/sec; 4.863 sec/batch)
2018-05-04 18:15:15.707621: step 55510, loss = 18.30 (3.3 examples/sec; 4.811 sec/batch)
2018-05-04 18:16:05.159619: step 55520, loss = 17.86 (3.2 examples/sec; 5.045 sec/batch)
2018-05-04 18:16:54.260894: step 55530, loss = 18.28 (3.9 examples/sec; 4.140 sec/batch)
2018-05-04 18:17:41.059017: step 55540, loss = 18.31 (3.3 examples/sec; 4.783 sec/batch)
2018-05-04 18:18:30.619747: step 55550, loss = 17.96 (3.3 examples/sec; 4.895 sec/batch)
2018-05-04 18:19:20.551198: step 55560, loss = 18.53 (3.2 examples/sec; 5.020 sec/batch)
2018-05-04 18:20:10.423764: step 55570, loss = 18.41 (3.1 examples/sec; 5.094 sec/batch)
2018-05-04 18:20:59.933503: step 55580, loss = 18.10 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 18:21:49.346065: step 55590, loss = 18.61 (3.2 examples/sec; 5.037 sec/batch)
2018-05-04 18:22:38.669031: step 55600, loss = 18.29 (3.2 examples/sec; 5.004 sec/batch)
2018-05-04 18:23:31.541830: step 55610, loss = 18.45 (3.4 examples/sec; 4.771 sec/batch)
2018-05-04 18:24:21.967931: step 55620, loss = 18.28 (3.2 examples/sec; 4.944 sec/batch)
2018-05-04 18:25:11.507702: step 55630, loss = 18.26 (3.1 examples/sec; 5.113 sec/batch)
2018-05-04 18:26:01.135361: step 55640, loss = 18.23 (3.3 examples/sec; 4.849 sec/batch)
2018-05-04 18:26:50.424752: step 55650, loss = 18.11 (3.3 examples/sec; 4.787 sec/batch)
2018-05-04 18:27:36.681073: step 55660, loss = 18.07 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 18:28:26.087304: step 55670, loss = 18.34 (3.3 examples/sec; 4.828 sec/batch)
2018-05-04 18:29:16.101368: step 55680, loss = 18.06 (3.2 examples/sec; 4.947 sec/batch)
2018-05-04 18:30:05.678590: step 55690, loss = 17.89 (3.2 examples/sec; 5.016 sec/batch)
2018-05-04 18:30:55.342655: step 55700, loss = 18.02 (3.2 examples/sec; 4.943 sec/batch)
2018-05-04 18:31:48.538170: step 55710, loss = 18.50 (3.1 examples/sec; 5.225 sec/batch)
2018-05-04 18:32:38.493300: step 55720, loss = 18.01 (3.2 examples/sec; 5.055 sec/batch)
2018-05-04 18:33:27.997569: step 55730, loss = 17.99 (3.2 examples/sec; 5.005 sec/batch)
2018-05-04 18:34:17.572705: step 55740, loss = 18.39 (3.2 examples/sec; 5.032 sec/batch)
2018-05-04 18:35:07.503784: step 55750, loss = 18.38 (3.1 examples/sec; 5.107 sec/batch)
2018-05-04 18:35:56.746853: step 55760, loss = 17.95 (3.3 examples/sec; 4.881 sec/batch)
2018-05-04 18:36:46.124584: step 55770, loss = 18.22 (3.3 examples/sec; 4.889 sec/batch)
2018-05-04 18:37:34.470162: step 55780, loss = 18.15 (4.2 examples/sec; 3.801 sec/batch)
2018-05-04 18:38:22.381234: step 55790, loss = 17.75 (3.2 examples/sec; 4.937 sec/batch)
2018-05-04 18:39:11.345801: step 55800, loss = 18.65 (3.2 examples/sec; 4.973 sec/batch)
2018-05-04 18:40:04.759095: step 55810, loss = 17.79 (3.2 examples/sec; 5.017 sec/batch)
2018-05-04 18:40:54.351501: step 55820, loss = 18.59 (3.3 examples/sec; 4.836 sec/batch)
2018-05-04 18:41:43.716351: step 55830, loss = 18.34 (3.2 examples/sec; 4.937 sec/batch)
2018-05-04 18:42:33.583664: step 55840, loss = 18.15 (3.2 examples/sec; 5.015 sec/batch)
2018-05-04 18:43:22.888276: step 55850, loss = 17.68 (3.3 examples/sec; 4.850 sec/batch)
2018-05-04 18:44:12.086993: step 55860, loss = 18.55 (3.3 examples/sec; 4.918 sec/batch)
2018-05-04 18:45:01.725347: step 55870, loss = 18.03 (3.2 examples/sec; 4.953 sec/batch)
2018-05-04 18:45:51.412296: step 55880, loss = 18.10 (3.3 examples/sec; 4.853 sec/batch)
2018-05-04 18:46:41.370588: step 55890, loss = 18.18 (3.2 examples/sec; 5.047 sec/batch)
2018-05-04 18:47:30.921547: step 55900, loss = 18.07 (3.2 examples/sec; 4.977 sec/batch)
2018-05-04 18:48:20.578327: step 55910, loss = 17.75 (3.3 examples/sec; 4.828 sec/batch)
2018-05-04 18:49:09.039965: step 55920, loss = 19.25 (3.3 examples/sec; 4.798 sec/batch)
2018-05-04 18:49:57.962417: step 55930, loss = 17.95 (3.4 examples/sec; 4.733 sec/batch)
2018-05-04 18:50:47.135908: step 55940, loss = 18.60 (3.3 examples/sec; 4.913 sec/batch)
2018-05-04 18:51:36.575705: step 55950, loss = 18.26 (3.2 examples/sec; 5.063 sec/batch)
2018-05-04 18:52:26.191993: step 55960, loss = 18.26 (3.3 examples/sec; 4.852 sec/batch)
2018-05-04 18:53:16.143419: step 55970, loss = 18.16 (3.1 examples/sec; 5.110 sec/batch)
2018-05-04 18:54:05.608228: step 55980, loss = 19.07 (3.4 examples/sec; 4.708 sec/batch)
2018-05-04 18:54:54.270774: step 55990, loss = 17.71 (3.3 examples/sec; 4.778 sec/batch)
2018-05-04 18:55:43.458086: step 56000, loss = 17.84 (3.3 examples/sec; 4.919 sec/batch)
2018-05-04 18:56:36.581405: step 56010, loss = 18.00 (3.1 examples/sec; 5.133 sec/batch)
2018-05-04 18:57:25.313086: step 56020, loss = 17.89 (3.2 examples/sec; 4.929 sec/batch)
2018-05-04 18:58:13.290231: step 56030, loss = 17.76 (4.3 examples/sec; 3.712 sec/batch)
2018-05-04 18:59:01.754787: step 56040, loss = 18.57 (3.1 examples/sec; 5.125 sec/batch)
2018-05-04 18:59:51.209355: step 56050, loss = 18.10 (3.2 examples/sec; 5.020 sec/batch)
2018-05-04 19:00:40.343585: step 56060, loss = 18.05 (3.3 examples/sec; 4.830 sec/batch)
2018-05-04 19:01:30.138276: step 56070, loss = 17.74 (3.2 examples/sec; 5.011 sec/batch)
2018-05-04 19:02:19.724781: step 56080, loss = 18.28 (3.3 examples/sec; 4.800 sec/batch)
2018-05-04 19:03:09.216297: step 56090, loss = 18.35 (3.4 examples/sec; 4.769 sec/batch)
2018-05-04 19:03:58.586679: step 56100, loss = 17.98 (3.2 examples/sec; 5.022 sec/batch)
2018-05-04 19:04:50.983387: step 56110, loss = 18.11 (3.2 examples/sec; 4.937 sec/batch)
2018-05-04 19:05:39.991041: step 56120, loss = 18.64 (3.2 examples/sec; 5.014 sec/batch)
2018-05-04 19:06:29.330770: step 56130, loss = 17.87 (3.2 examples/sec; 5.010 sec/batch)
2018-05-04 19:07:18.400993: step 56140, loss = 18.20 (3.2 examples/sec; 4.953 sec/batch)
2018-05-04 19:08:08.375651: step 56150, loss = 18.42 (3.1 examples/sec; 5.165 sec/batch)
2018-05-04 19:08:54.422437: step 56160, loss = 18.25 (3.4 examples/sec; 4.764 sec/batch)
2018-05-04 19:09:43.890342: step 56170, loss = 18.11 (3.3 examples/sec; 4.914 sec/batch)
2018-05-04 19:10:33.305859: step 56180, loss = 17.82 (3.2 examples/sec; 4.955 sec/batch)
2018-05-04 19:11:22.283826: step 56190, loss = 18.26 (3.4 examples/sec; 4.761 sec/batch)
2018-05-04 19:12:11.434081: step 56200, loss = 18.52 (3.2 examples/sec; 4.959 sec/batch)
2018-05-04 19:13:03.797039: step 56210, loss = 18.29 (3.3 examples/sec; 4.900 sec/batch)
2018-05-04 19:13:52.773855: step 56220, loss = 17.97 (3.2 examples/sec; 5.001 sec/batch)
2018-05-04 19:14:41.804374: step 56230, loss = 18.45 (3.2 examples/sec; 5.016 sec/batch)
2018-05-04 19:15:31.170133: step 56240, loss = 17.99 (3.2 examples/sec; 5.036 sec/batch)
2018-05-04 19:16:20.138371: step 56250, loss = 18.62 (3.1 examples/sec; 5.090 sec/batch)
2018-05-04 19:17:09.546295: step 56260, loss = 18.56 (3.3 examples/sec; 4.890 sec/batch)
2018-05-04 19:17:59.279047: step 56270, loss = 17.93 (3.1 examples/sec; 5.100 sec/batch)
2018-05-04 19:18:47.895520: step 56280, loss = 18.83 (4.1 examples/sec; 3.857 sec/batch)
2018-05-04 19:19:35.666048: step 56290, loss = 17.98 (3.2 examples/sec; 5.070 sec/batch)
2018-05-04 19:20:24.927796: step 56300, loss = 17.93 (3.2 examples/sec; 4.925 sec/batch)
2018-05-04 19:21:17.519052: step 56310, loss = 18.33 (3.3 examples/sec; 4.842 sec/batch)
2018-05-04 19:22:07.546627: step 56320, loss = 18.80 (3.2 examples/sec; 4.984 sec/batch)
2018-05-04 19:22:56.648054: step 56330, loss = 18.48 (3.3 examples/sec; 4.910 sec/batch)
2018-05-04 19:23:45.936852: step 56340, loss = 18.22 (3.2 examples/sec; 4.994 sec/batch)
2018-05-04 19:24:36.156167: step 56350, loss = 18.00 (3.2 examples/sec; 4.927 sec/batch)
2018-05-04 19:25:25.728826: step 56360, loss = 17.84 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 19:26:15.216739: step 56370, loss = 17.80 (3.3 examples/sec; 4.818 sec/batch)
2018-05-04 19:27:05.104858: step 56380, loss = 18.10 (3.2 examples/sec; 5.045 sec/batch)
2018-05-04 19:27:54.303638: step 56390, loss = 18.04 (3.3 examples/sec; 4.830 sec/batch)
2018-05-04 19:28:44.222153: step 56400, loss = 18.29 (3.2 examples/sec; 4.988 sec/batch)
2018-05-04 19:29:33.727428: step 56410, loss = 18.20 (3.2 examples/sec; 4.999 sec/batch)
2018-05-04 19:30:23.053740: step 56420, loss = 18.41 (3.3 examples/sec; 4.904 sec/batch)
2018-05-04 19:31:12.503809: step 56430, loss = 18.27 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 19:32:02.323870: step 56440, loss = 17.76 (3.3 examples/sec; 4.902 sec/batch)
2018-05-04 19:32:52.455514: step 56450, loss = 18.18 (3.2 examples/sec; 5.005 sec/batch)
2018-05-04 19:33:41.733139: step 56460, loss = 18.15 (3.2 examples/sec; 5.058 sec/batch)
2018-05-04 19:34:31.711390: step 56470, loss = 18.50 (3.2 examples/sec; 5.068 sec/batch)
2018-05-04 19:35:21.306837: step 56480, loss = 17.81 (3.3 examples/sec; 4.907 sec/batch)
2018-05-04 19:36:10.690157: step 56490, loss = 18.15 (3.2 examples/sec; 5.027 sec/batch)
2018-05-04 19:36:59.487461: step 56500, loss = 18.25 (3.4 examples/sec; 4.766 sec/batch)
2018-05-04 19:37:51.786751: step 56510, loss = 17.70 (3.2 examples/sec; 4.929 sec/batch)
2018-05-04 19:38:40.961248: step 56520, loss = 18.20 (3.2 examples/sec; 5.056 sec/batch)
2018-05-04 19:39:27.286747: step 56530, loss = 18.58 (3.9 examples/sec; 4.104 sec/batch)
2018-05-04 19:40:15.810155: step 56540, loss = 17.67 (3.3 examples/sec; 4.888 sec/batch)
2018-05-04 19:41:05.174907: step 56550, loss = 18.61 (3.0 examples/sec; 5.322 sec/batch)
2018-05-04 19:41:55.143379: step 56560, loss = 17.92 (3.2 examples/sec; 5.051 sec/batch)
2018-05-04 19:42:44.427387: step 56570, loss = 18.67 (3.3 examples/sec; 4.802 sec/batch)
2018-05-04 19:43:33.672662: step 56580, loss = 18.03 (3.2 examples/sec; 4.977 sec/batch)
2018-05-04 19:44:22.350768: step 56590, loss = 18.09 (3.2 examples/sec; 4.976 sec/batch)
2018-05-04 19:45:11.408193: step 56600, loss = 18.20 (3.3 examples/sec; 4.919 sec/batch)
2018-05-04 19:46:04.811231: step 56610, loss = 17.92 (3.2 examples/sec; 5.032 sec/batch)
2018-05-04 19:46:54.700576: step 56620, loss = 18.19 (3.0 examples/sec; 5.379 sec/batch)
2018-05-04 19:47:44.108416: step 56630, loss = 18.20 (3.2 examples/sec; 4.971 sec/batch)
2018-05-04 19:48:33.412774: step 56640, loss = 18.36 (3.2 examples/sec; 5.051 sec/batch)
2018-05-04 19:49:22.553256: step 56650, loss = 18.12 (3.2 examples/sec; 4.945 sec/batch)
2018-05-04 19:50:08.343486: step 56660, loss = 17.99 (3.2 examples/sec; 4.979 sec/batch)
2018-05-04 19:50:57.370817: step 56670, loss = 18.12 (3.4 examples/sec; 4.662 sec/batch)
2018-05-04 19:51:46.663809: step 56680, loss = 17.97 (3.4 examples/sec; 4.713 sec/batch)
2018-05-04 19:52:36.323999: step 56690, loss = 18.19 (3.2 examples/sec; 5.005 sec/batch)
2018-05-04 19:53:25.334493: step 56700, loss = 18.28 (3.2 examples/sec; 4.935 sec/batch)
2018-05-04 19:54:18.308756: step 56710, loss = 17.89 (3.3 examples/sec; 4.906 sec/batch)
2018-05-04 19:55:07.224754: step 56720, loss = 18.19 (3.4 examples/sec; 4.655 sec/batch)
2018-05-04 19:55:56.739751: step 56730, loss = 18.12 (3.3 examples/sec; 4.826 sec/batch)
2018-05-04 19:56:46.947106: step 56740, loss = 18.78 (3.2 examples/sec; 4.930 sec/batch)
2018-05-04 19:57:36.423101: step 56750, loss = 18.40 (3.2 examples/sec; 4.951 sec/batch)
2018-05-04 19:58:25.420582: step 56760, loss = 18.15 (3.3 examples/sec; 4.807 sec/batch)
2018-05-04 19:59:14.906000: step 56770, loss = 17.91 (3.3 examples/sec; 4.917 sec/batch)
2018-05-04 20:00:02.099388: step 56780, loss = 19.04 (4.1 examples/sec; 3.902 sec/batch)
2018-05-04 20:00:49.992137: step 56790, loss = 18.28 (3.3 examples/sec; 4.854 sec/batch)
2018-05-04 20:01:38.649802: step 56800, loss = 18.88 (3.3 examples/sec; 4.877 sec/batch)
2018-05-04 20:02:30.970326: step 56810, loss = 18.77 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 20:03:20.500678: step 56820, loss = 18.19 (3.2 examples/sec; 5.002 sec/batch)
2018-05-04 20:04:09.833574: step 56830, loss = 18.18 (3.2 examples/sec; 4.945 sec/batch)
2018-05-04 20:04:59.643515: step 56840, loss = 18.43 (3.4 examples/sec; 4.750 sec/batch)
2018-05-04 20:05:48.965216: step 56850, loss = 18.44 (3.2 examples/sec; 4.990 sec/batch)
2018-05-04 20:06:38.412947: step 56860, loss = 18.06 (3.3 examples/sec; 4.795 sec/batch)
2018-05-04 20:07:28.255688: step 56870, loss = 18.11 (3.2 examples/sec; 5.016 sec/batch)
2018-05-04 20:08:17.501679: step 56880, loss = 17.97 (3.3 examples/sec; 4.891 sec/batch)
2018-05-04 20:09:07.401302: step 56890, loss = 18.35 (3.2 examples/sec; 4.969 sec/batch)
2018-05-04 20:09:56.743440: step 56900, loss = 18.27 (3.2 examples/sec; 4.973 sec/batch)
2018-05-04 20:10:46.363958: step 56910, loss = 17.75 (3.3 examples/sec; 4.877 sec/batch)
2018-05-04 20:11:35.137043: step 56920, loss = 18.04 (3.3 examples/sec; 4.805 sec/batch)
2018-05-04 20:12:24.377407: step 56930, loss = 18.16 (3.3 examples/sec; 4.907 sec/batch)
2018-05-04 20:13:13.750571: step 56940, loss = 17.84 (3.3 examples/sec; 4.836 sec/batch)
2018-05-04 20:14:03.362682: step 56950, loss = 18.27 (3.2 examples/sec; 5.073 sec/batch)
2018-05-04 20:14:53.785928: step 56960, loss = 18.73 (3.2 examples/sec; 4.980 sec/batch)
2018-05-04 20:15:43.408064: step 56970, loss = 17.82 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 20:16:32.712979: step 56980, loss = 18.44 (3.2 examples/sec; 5.006 sec/batch)
2018-05-04 20:17:21.830017: step 56990, loss = 18.25 (3.2 examples/sec; 4.930 sec/batch)
2018-05-04 20:18:10.992981: step 57000, loss = 18.34 (3.4 examples/sec; 4.750 sec/batch)
2018-05-04 20:19:03.567137: step 57010, loss = 18.33 (3.2 examples/sec; 5.022 sec/batch)
2018-05-04 20:19:52.602810: step 57020, loss = 18.37 (3.3 examples/sec; 4.907 sec/batch)
2018-05-04 20:20:40.686090: step 57030, loss = 17.92 (4.3 examples/sec; 3.760 sec/batch)
2018-05-04 20:21:28.921930: step 57040, loss = 18.02 (3.3 examples/sec; 4.877 sec/batch)
2018-05-04 20:22:18.200750: step 57050, loss = 17.94 (3.2 examples/sec; 4.990 sec/batch)
2018-05-04 20:23:07.697024: step 57060, loss = 18.04 (3.2 examples/sec; 5.066 sec/batch)
2018-05-04 20:23:57.189058: step 57070, loss = 17.93 (3.2 examples/sec; 5.029 sec/batch)
2018-05-04 20:24:46.814105: step 57080, loss = 18.01 (3.1 examples/sec; 5.102 sec/batch)
2018-05-04 20:25:35.559124: step 57090, loss = 18.02 (3.3 examples/sec; 4.876 sec/batch)
2018-05-04 20:26:25.356539: step 57100, loss = 17.93 (3.2 examples/sec; 5.015 sec/batch)
2018-05-04 20:27:19.250301: step 57110, loss = 17.73 (3.3 examples/sec; 4.922 sec/batch)
2018-05-04 20:28:08.346092: step 57120, loss = 18.40 (3.1 examples/sec; 5.127 sec/batch)
2018-05-04 20:28:57.550058: step 57130, loss = 18.29 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 20:29:47.302969: step 57140, loss = 18.30 (3.2 examples/sec; 4.943 sec/batch)
2018-05-04 20:30:36.727602: step 57150, loss = 18.64 (3.2 examples/sec; 4.968 sec/batch)
2018-05-04 20:31:22.621633: step 57160, loss = 17.77 (3.4 examples/sec; 4.740 sec/batch)
2018-05-04 20:32:12.398169: step 57170, loss = 18.05 (3.3 examples/sec; 4.849 sec/batch)
2018-05-04 20:33:01.940380: step 57180, loss = 18.48 (3.2 examples/sec; 4.948 sec/batch)
2018-05-04 20:33:51.388338: step 57190, loss = 18.10 (3.3 examples/sec; 4.844 sec/batch)
2018-05-04 20:34:40.957743: step 57200, loss = 18.15 (3.2 examples/sec; 4.985 sec/batch)
2018-05-04 20:35:34.009553: step 57210, loss = 18.16 (3.3 examples/sec; 4.910 sec/batch)
2018-05-04 20:36:23.774395: step 57220, loss = 18.11 (3.4 examples/sec; 4.772 sec/batch)
2018-05-04 20:37:13.288611: step 57230, loss = 18.36 (3.2 examples/sec; 4.946 sec/batch)
2018-05-04 20:38:03.113659: step 57240, loss = 18.11 (3.2 examples/sec; 4.993 sec/batch)
2018-05-04 20:38:52.051572: step 57250, loss = 17.98 (3.2 examples/sec; 4.989 sec/batch)
2018-05-04 20:39:41.276409: step 57260, loss = 19.06 (3.2 examples/sec; 4.965 sec/batch)
2018-05-04 20:40:31.201502: step 57270, loss = 17.80 (3.4 examples/sec; 4.726 sec/batch)
2018-05-04 20:41:18.345818: step 57280, loss = 18.35 (4.2 examples/sec; 3.801 sec/batch)
2018-05-04 20:42:06.847888: step 57290, loss = 18.40 (3.3 examples/sec; 4.783 sec/batch)
2018-05-04 20:42:55.993277: step 57300, loss = 18.19 (3.2 examples/sec; 4.946 sec/batch)
2018-05-04 20:43:48.431706: step 57310, loss = 18.08 (3.2 examples/sec; 5.009 sec/batch)
2018-05-04 20:44:37.774831: step 57320, loss = 18.20 (3.3 examples/sec; 4.861 sec/batch)
2018-05-04 20:45:27.094641: step 57330, loss = 17.82 (3.2 examples/sec; 5.040 sec/batch)
2018-05-04 20:46:17.013844: step 57340, loss = 17.75 (3.2 examples/sec; 5.018 sec/batch)
2018-05-04 20:47:06.019179: step 57350, loss = 18.15 (3.3 examples/sec; 4.838 sec/batch)
2018-05-04 20:47:56.025157: step 57360, loss = 18.14 (3.2 examples/sec; 5.011 sec/batch)
2018-05-04 20:48:45.272713: step 57370, loss = 17.70 (3.2 examples/sec; 4.936 sec/batch)
2018-05-04 20:49:34.552078: step 57380, loss = 18.61 (3.2 examples/sec; 5.027 sec/batch)
2018-05-04 20:50:24.024227: step 57390, loss = 17.86 (3.3 examples/sec; 4.821 sec/batch)
2018-05-04 20:51:13.316133: step 57400, loss = 18.11 (3.2 examples/sec; 5.050 sec/batch)
2018-05-04 20:52:03.002106: step 57410, loss = 18.24 (3.2 examples/sec; 4.924 sec/batch)
2018-05-04 20:52:51.550519: step 57420, loss = 18.49 (3.2 examples/sec; 4.970 sec/batch)
2018-05-04 20:53:40.707857: step 57430, loss = 17.72 (3.3 examples/sec; 4.861 sec/batch)
2018-05-04 20:54:30.141370: step 57440, loss = 17.89 (3.2 examples/sec; 4.948 sec/batch)
2018-05-04 20:55:19.669574: step 57450, loss = 18.54 (3.2 examples/sec; 4.962 sec/batch)
2018-05-04 20:56:08.389259: step 57460, loss = 17.66 (3.4 examples/sec; 4.769 sec/batch)
2018-05-04 20:56:57.409505: step 57470, loss = 18.17 (3.2 examples/sec; 4.976 sec/batch)
2018-05-04 20:57:45.839395: step 57480, loss = 18.35 (3.4 examples/sec; 4.640 sec/batch)
2018-05-04 20:58:34.916488: step 57490, loss = 18.16 (3.3 examples/sec; 4.869 sec/batch)
2018-05-04 20:59:24.685216: step 57500, loss = 18.07 (3.3 examples/sec; 4.849 sec/batch)
2018-05-04 21:00:17.416772: step 57510, loss = 17.75 (3.2 examples/sec; 4.984 sec/batch)
2018-05-04 21:01:07.547844: step 57520, loss = 18.21 (3.3 examples/sec; 4.920 sec/batch)
2018-05-04 21:01:54.239020: step 57530, loss = 18.15 (4.2 examples/sec; 3.789 sec/batch)
2018-05-04 21:02:43.385594: step 57540, loss = 18.03 (3.3 examples/sec; 4.913 sec/batch)
2018-05-04 21:03:32.567478: step 57550, loss = 17.83 (3.3 examples/sec; 4.861 sec/batch)
2018-05-04 21:04:22.150162: step 57560, loss = 18.07 (3.3 examples/sec; 4.915 sec/batch)
2018-05-04 21:05:11.594772: step 57570, loss = 18.29 (3.3 examples/sec; 4.849 sec/batch)
2018-05-04 21:06:00.529898: step 57580, loss = 18.17 (3.2 examples/sec; 4.939 sec/batch)
2018-05-04 21:06:49.594290: step 57590, loss = 18.14 (3.4 examples/sec; 4.773 sec/batch)
2018-05-04 21:07:39.876856: step 57600, loss = 18.25 (3.3 examples/sec; 4.892 sec/batch)
2018-05-04 21:08:32.650650: step 57610, loss = 17.77 (3.2 examples/sec; 4.933 sec/batch)
2018-05-04 21:09:22.510191: step 57620, loss = 18.09 (3.3 examples/sec; 4.916 sec/batch)
2018-05-04 21:10:12.511046: step 57630, loss = 18.09 (3.2 examples/sec; 4.937 sec/batch)
2018-05-04 21:11:01.798524: step 57640, loss = 18.21 (3.3 examples/sec; 4.821 sec/batch)
2018-05-04 21:11:51.430297: step 57650, loss = 18.10 (3.2 examples/sec; 5.019 sec/batch)
2018-05-04 21:12:37.098162: step 57660, loss = 17.89 (3.7 examples/sec; 4.308 sec/batch)
2018-05-04 21:13:26.975841: step 57670, loss = 18.17 (3.0 examples/sec; 5.318 sec/batch)
2018-05-04 21:14:16.836417: step 57680, loss = 18.17 (3.3 examples/sec; 4.889 sec/batch)
2018-05-04 21:15:06.147832: step 57690, loss = 18.13 (3.3 examples/sec; 4.921 sec/batch)
2018-05-04 21:15:55.319381: step 57700, loss = 17.85 (3.1 examples/sec; 5.100 sec/batch)
2018-05-04 21:16:48.523710: step 57710, loss = 18.51 (3.2 examples/sec; 5.017 sec/batch)
2018-05-04 21:17:38.134368: step 57720, loss = 18.20 (3.2 examples/sec; 5.066 sec/batch)
2018-05-04 21:18:28.581495: step 57730, loss = 18.36 (3.2 examples/sec; 4.928 sec/batch)
2018-05-04 21:19:17.912893: step 57740, loss = 18.20 (3.3 examples/sec; 4.914 sec/batch)
2018-05-04 21:20:07.620499: step 57750, loss = 18.57 (3.3 examples/sec; 4.779 sec/batch)
2018-05-04 21:20:56.074465: step 57760, loss = 17.77 (3.3 examples/sec; 4.834 sec/batch)
2018-05-04 21:21:45.000174: step 57770, loss = 17.72 (3.3 examples/sec; 4.810 sec/batch)
2018-05-04 21:22:31.570053: step 57780, loss = 18.28 (3.4 examples/sec; 4.728 sec/batch)
2018-05-04 21:23:21.178717: step 57790, loss = 17.93 (3.2 examples/sec; 4.975 sec/batch)
2018-05-04 21:24:10.532944: step 57800, loss = 18.11 (3.2 examples/sec; 5.054 sec/batch)
2018-05-04 21:25:03.209844: step 57810, loss = 18.05 (3.4 examples/sec; 4.767 sec/batch)
2018-05-04 21:25:53.259747: step 57820, loss = 18.66 (3.2 examples/sec; 4.926 sec/batch)
2018-05-04 21:26:42.446319: step 57830, loss = 18.37 (3.4 examples/sec; 4.740 sec/batch)
2018-05-04 21:27:31.771594: step 57840, loss = 17.69 (3.3 examples/sec; 4.809 sec/batch)
2018-05-04 21:28:21.548688: step 57850, loss = 17.91 (3.2 examples/sec; 4.930 sec/batch)
2018-05-04 21:29:11.243103: step 57860, loss = 18.33 (3.2 examples/sec; 5.063 sec/batch)
2018-05-04 21:30:01.173374: step 57870, loss = 18.10 (3.2 examples/sec; 4.970 sec/batch)
2018-05-04 21:30:50.419537: step 57880, loss = 18.06 (3.3 examples/sec; 4.905 sec/batch)
2018-05-04 21:31:39.494008: step 57890, loss = 18.34 (3.2 examples/sec; 4.965 sec/batch)
2018-05-04 21:32:28.607957: step 57900, loss = 18.41 (3.2 examples/sec; 4.953 sec/batch)
2018-05-04 21:33:18.434162: step 57910, loss = 17.95 (3.2 examples/sec; 4.926 sec/batch)
2018-05-04 21:34:07.632655: step 57920, loss = 18.35 (3.3 examples/sec; 4.832 sec/batch)
2018-05-04 21:34:57.092950: step 57930, loss = 18.12 (3.2 examples/sec; 5.077 sec/batch)
2018-05-04 21:35:46.396732: step 57940, loss = 17.99 (3.2 examples/sec; 4.972 sec/batch)
2018-05-04 21:36:35.201480: step 57950, loss = 17.92 (3.3 examples/sec; 4.804 sec/batch)
2018-05-04 21:37:24.673673: step 57960, loss = 18.37 (3.2 examples/sec; 4.930 sec/batch)
2018-05-04 21:38:14.409152: step 57970, loss = 18.12 (3.2 examples/sec; 4.952 sec/batch)
2018-05-04 21:39:03.930045: step 57980, loss = 18.21 (3.2 examples/sec; 4.957 sec/batch)
2018-05-04 21:39:52.886396: step 57990, loss = 18.05 (3.2 examples/sec; 4.993 sec/batch)
2018-05-04 21:40:42.004656: step 58000, loss = 17.82 (3.3 examples/sec; 4.823 sec/batch)
2018-05-04 21:41:35.771671: step 58010, loss = 18.27 (3.2 examples/sec; 5.022 sec/batch)
2018-05-04 21:42:25.100552: step 58020, loss = 17.84 (3.2 examples/sec; 5.053 sec/batch)
2018-05-04 21:43:10.770965: step 58030, loss = 18.54 (3.3 examples/sec; 4.852 sec/batch)
2018-05-04 21:44:00.616620: step 58040, loss = 18.34 (3.2 examples/sec; 5.075 sec/batch)
2018-05-04 21:44:50.049910: step 58050, loss = 18.17 (3.3 examples/sec; 4.906 sec/batch)
2018-05-04 21:45:39.628032: step 58060, loss = 18.53 (3.3 examples/sec; 4.891 sec/batch)
2018-05-04 21:46:28.697003: step 58070, loss = 18.03 (3.3 examples/sec; 4.896 sec/batch)
2018-05-04 21:47:18.262053: step 58080, loss = 17.96 (3.3 examples/sec; 4.820 sec/batch)
2018-05-04 21:48:08.348052: step 58090, loss = 17.95 (3.2 examples/sec; 4.944 sec/batch)
2018-05-04 21:48:57.835682: step 58100, loss = 18.10 (3.2 examples/sec; 4.949 sec/batch)
2018-05-04 21:49:50.720038: step 58110, loss = 17.91 (3.3 examples/sec; 4.801 sec/batch)
2018-05-04 21:50:40.816555: step 58120, loss = 18.24 (3.1 examples/sec; 5.127 sec/batch)
2018-05-04 21:51:30.277497: step 58130, loss = 17.88 (3.4 examples/sec; 4.759 sec/batch)
2018-05-04 21:52:20.017392: step 58140, loss = 18.10 (3.2 examples/sec; 5.045 sec/batch)
2018-05-04 21:53:09.302165: step 58150, loss = 18.10 (3.2 examples/sec; 5.029 sec/batch)
2018-05-04 21:53:55.142035: step 58160, loss = 18.20 (3.2 examples/sec; 4.937 sec/batch)
2018-05-04 21:54:44.519158: step 58170, loss = 17.59 (3.3 examples/sec; 4.921 sec/batch)
2018-05-04 21:55:33.365154: step 58180, loss = 17.98 (3.3 examples/sec; 4.829 sec/batch)
2018-05-04 21:56:22.679455: step 58190, loss = 18.12 (3.4 examples/sec; 4.724 sec/batch)
2018-05-04 21:57:11.629345: step 58200, loss = 18.30 (3.4 examples/sec; 4.774 sec/batch)
2018-05-04 21:58:04.650593: step 58210, loss = 17.72 (3.4 examples/sec; 4.744 sec/batch)
2018-05-04 21:58:54.031381: step 58220, loss = 18.20 (3.3 examples/sec; 4.912 sec/batch)
2018-05-04 21:59:43.140236: step 58230, loss = 17.80 (3.3 examples/sec; 4.883 sec/batch)
2018-05-04 22:00:32.061589: step 58240, loss = 18.84 (3.2 examples/sec; 4.940 sec/batch)
2018-05-04 22:01:21.538561: step 58250, loss = 17.73 (3.2 examples/sec; 4.968 sec/batch)
2018-05-04 22:02:10.759361: step 58260, loss = 18.02 (3.2 examples/sec; 5.008 sec/batch)
2018-05-04 22:03:00.035081: step 58270, loss = 18.80 (3.2 examples/sec; 4.980 sec/batch)
2018-05-04 22:03:45.666502: step 58280, loss = 18.20 (4.0 examples/sec; 4.017 sec/batch)
2018-05-04 22:04:34.663388: step 58290, loss = 18.40 (3.2 examples/sec; 5.028 sec/batch)
2018-05-04 22:05:23.954550: step 58300, loss = 18.03 (3.2 examples/sec; 5.006 sec/batch)
2018-05-04 22:06:16.372298: step 58310, loss = 18.03 (3.3 examples/sec; 4.893 sec/batch)
2018-05-04 22:07:05.462240: step 58320, loss = 18.44 (3.2 examples/sec; 4.935 sec/batch)
2018-05-04 22:07:54.749281: step 58330, loss = 18.18 (3.4 examples/sec; 4.766 sec/batch)
2018-05-04 22:08:44.279775: step 58340, loss = 18.32 (3.2 examples/sec; 4.980 sec/batch)
2018-05-04 22:09:34.345892: step 58350, loss = 17.70 (3.2 examples/sec; 4.988 sec/batch)
2018-05-04 22:10:23.637006: step 58360, loss = 18.21 (3.2 examples/sec; 4.954 sec/batch)
2018-05-04 22:11:12.865585: step 58370, loss = 18.92 (3.3 examples/sec; 4.857 sec/batch)
2018-05-04 22:12:02.949786: step 58380, loss = 18.21 (3.2 examples/sec; 4.932 sec/batch)
2018-05-04 22:12:52.659040: step 58390, loss = 18.24 (3.3 examples/sec; 4.865 sec/batch)
2018-05-04 22:13:42.236476: step 58400, loss = 18.71 (3.2 examples/sec; 4.961 sec/batch)
2018-05-04 22:14:31.950629: step 58410, loss = 18.14 (3.2 examples/sec; 4.993 sec/batch)
2018-05-04 22:15:21.853993: step 58420, loss = 18.04 (3.1 examples/sec; 5.089 sec/batch)
2018-05-04 22:16:11.620869: step 58430, loss = 17.85 (3.3 examples/sec; 4.858 sec/batch)
2018-05-04 22:17:01.921228: step 58440, loss = 18.19 (3.1 examples/sec; 5.144 sec/batch)
2018-05-04 22:17:51.485831: step 58450, loss = 18.07 (3.2 examples/sec; 5.033 sec/batch)
2018-05-04 22:18:41.151610: step 58460, loss = 18.54 (3.3 examples/sec; 4.920 sec/batch)
2018-05-04 22:19:30.971123: step 58470, loss = 17.92 (3.2 examples/sec; 5.041 sec/batch)
2018-05-04 22:20:20.600506: step 58480, loss = 18.70 (3.3 examples/sec; 4.780 sec/batch)
2018-05-04 22:21:10.019624: step 58490, loss = 17.96 (3.2 examples/sec; 5.078 sec/batch)
2018-05-04 22:21:59.359582: step 58500, loss = 18.57 (3.2 examples/sec; 5.002 sec/batch)
2018-05-04 22:22:52.521760: step 58510, loss = 18.28 (3.1 examples/sec; 5.182 sec/batch)
2018-05-04 22:23:42.827451: step 58520, loss = 18.73 (3.1 examples/sec; 5.137 sec/batch)
2018-05-04 22:24:29.308861: step 58530, loss = 18.09 (3.2 examples/sec; 5.038 sec/batch)
2018-05-04 22:25:18.852112: step 58540, loss = 18.19 (3.2 examples/sec; 4.984 sec/batch)
2018-05-04 22:26:08.199561: step 58550, loss = 17.93 (3.1 examples/sec; 5.168 sec/batch)
2018-05-04 22:26:56.804274: step 58560, loss = 18.21 (3.2 examples/sec; 4.961 sec/batch)
2018-05-04 22:27:46.991360: step 58570, loss = 18.05 (3.2 examples/sec; 5.042 sec/batch)
2018-05-04 22:28:37.316093: step 58580, loss = 17.99 (3.3 examples/sec; 4.910 sec/batch)
2018-05-04 22:29:27.521349: step 58590, loss = 18.02 (3.2 examples/sec; 5.047 sec/batch)
2018-05-04 22:30:17.534055: step 58600, loss = 17.84 (3.2 examples/sec; 4.999 sec/batch)
2018-05-04 22:31:10.862419: step 58610, loss = 18.49 (3.2 examples/sec; 5.002 sec/batch)
2018-05-04 22:32:00.256127: step 58620, loss = 18.16 (3.3 examples/sec; 4.907 sec/batch)
2018-05-04 22:32:50.231354: step 58630, loss = 18.38 (3.2 examples/sec; 5.016 sec/batch)
2018-05-04 22:33:40.151546: step 58640, loss = 18.63 (3.2 examples/sec; 5.061 sec/batch)
2018-05-04 22:34:26.807771: step 58650, loss = 18.09 (4.2 examples/sec; 3.816 sec/batch)
2018-05-04 22:35:15.724120: step 58660, loss = 17.93 (3.2 examples/sec; 4.964 sec/batch)
2018-05-04 22:36:05.238032: step 58670, loss = 18.19 (3.2 examples/sec; 4.981 sec/batch)
2018-05-04 22:36:54.696406: step 58680, loss = 18.00 (3.2 examples/sec; 4.996 sec/batch)
2018-05-04 22:37:44.109532: step 58690, loss = 18.44 (3.2 examples/sec; 4.944 sec/batch)
2018-05-04 22:38:33.260446: step 58700, loss = 18.31 (3.2 examples/sec; 5.007 sec/batch)
2018-05-04 22:39:26.214563: step 58710, loss = 18.08 (3.3 examples/sec; 4.864 sec/batch)
2018-05-04 22:40:15.476067: step 58720, loss = 18.37 (3.2 examples/sec; 4.998 sec/batch)
2018-05-04 22:41:05.557600: step 58730, loss = 17.99 (3.2 examples/sec; 4.930 sec/batch)
2018-05-04 22:41:54.685961: step 58740, loss = 18.24 (3.3 examples/sec; 4.918 sec/batch)
2018-05-04 22:42:44.427967: step 58750, loss = 17.76 (3.2 examples/sec; 4.958 sec/batch)
2018-05-04 22:43:33.740675: step 58760, loss = 18.11 (3.3 examples/sec; 4.922 sec/batch)
2018-05-04 22:44:22.952439: step 58770, loss = 18.04 (3.2 examples/sec; 5.055 sec/batch)
2018-05-04 22:45:08.634330: step 58780, loss = 17.98 (3.3 examples/sec; 4.873 sec/batch)
2018-05-04 22:45:57.946293: step 58790, loss = 18.06 (3.3 examples/sec; 4.877 sec/batch)
2018-05-04 22:46:47.564478: step 58800, loss = 17.75 (3.3 examples/sec; 4.920 sec/batch)
2018-05-04 22:47:40.595945: step 58810, loss = 18.02 (3.4 examples/sec; 4.768 sec/batch)
2018-05-04 22:48:29.879934: step 58820, loss = 18.55 (3.2 examples/sec; 4.977 sec/batch)
2018-05-04 22:49:19.414922: step 58830, loss = 17.76 (3.2 examples/sec; 5.015 sec/batch)
2018-05-04 22:50:09.542198: step 58840, loss = 17.96 (3.3 examples/sec; 4.820 sec/batch)
2018-05-04 22:50:59.157457: step 58850, loss = 18.09 (3.2 examples/sec; 5.047 sec/batch)
2018-05-04 22:51:47.801318: step 58860, loss = 18.39 (3.5 examples/sec; 4.624 sec/batch)
2018-05-04 22:52:36.875132: step 58870, loss = 18.34 (3.2 examples/sec; 5.022 sec/batch)
2018-05-04 22:53:25.636848: step 58880, loss = 17.83 (3.2 examples/sec; 5.020 sec/batch)
2018-05-04 22:54:14.805120: step 58890, loss = 18.28 (3.3 examples/sec; 4.844 sec/batch)
2018-05-04 22:55:01.505689: step 58900, loss = 18.70 (4.2 examples/sec; 3.806 sec/batch)
2018-05-04 22:55:53.992831: step 58910, loss = 18.20 (3.3 examples/sec; 4.878 sec/batch)
2018-05-04 22:56:43.367011: step 58920, loss = 17.78 (3.1 examples/sec; 5.093 sec/batch)
2018-05-04 22:57:33.083969: step 58930, loss = 18.02 (3.1 examples/sec; 5.119 sec/batch)
2018-05-04 22:58:22.038792: step 58940, loss = 18.18 (3.2 examples/sec; 4.954 sec/batch)
2018-05-04 22:59:11.261023: step 58950, loss = 17.93 (3.3 examples/sec; 4.904 sec/batch)
2018-05-04 23:00:01.315186: step 58960, loss = 18.35 (3.3 examples/sec; 4.881 sec/batch)
2018-05-04 23:00:50.184412: step 58970, loss = 17.98 (3.2 examples/sec; 4.987 sec/batch)
2018-05-04 23:01:40.107717: step 58980, loss = 17.90 (3.2 examples/sec; 5.012 sec/batch)
2018-05-04 23:02:29.423593: step 58990, loss = 18.35 (3.2 examples/sec; 4.941 sec/batch)
2018-05-04 23:03:19.462120: step 59000, loss = 18.80 (3.2 examples/sec; 4.961 sec/batch)
2018-05-04 23:04:12.516213: step 59010, loss = 18.20 (3.1 examples/sec; 5.086 sec/batch)
2018-05-04 23:05:01.947339: step 59020, loss = 18.12 (3.2 examples/sec; 5.003 sec/batch)
2018-05-04 23:05:47.698690: step 59030, loss = 17.89 (3.3 examples/sec; 4.860 sec/batch)
2018-05-04 23:06:36.730499: step 59040, loss = 18.00 (3.2 examples/sec; 4.955 sec/batch)
2018-05-04 23:07:25.653058: step 59050, loss = 18.58 (3.3 examples/sec; 4.880 sec/batch)
2018-05-04 23:08:15.579237: step 59060, loss = 18.64 (3.2 examples/sec; 4.999 sec/batch)
2018-05-04 23:09:04.727045: step 59070, loss = 17.98 (3.2 examples/sec; 4.952 sec/batch)
2018-05-04 23:09:54.361021: step 59080, loss = 18.17 (3.2 examples/sec; 5.056 sec/batch)
2018-05-04 23:10:43.920953: step 59090, loss = 18.39 (3.3 examples/sec; 4.866 sec/batch)
2018-05-04 23:11:33.468975: step 59100, loss = 17.86 (3.2 examples/sec; 5.008 sec/batch)
2018-05-04 23:12:27.421180: step 59110, loss = 18.17 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 23:13:17.115315: step 59120, loss = 18.11 (3.2 examples/sec; 4.941 sec/batch)
2018-05-04 23:14:06.673096: step 59130, loss = 18.01 (3.3 examples/sec; 4.852 sec/batch)
2018-05-04 23:14:55.843766: step 59140, loss = 18.37 (3.2 examples/sec; 4.933 sec/batch)
2018-05-04 23:15:42.425858: step 59150, loss = 18.60 (3.3 examples/sec; 4.862 sec/batch)
2018-05-04 23:16:31.980042: step 59160, loss = 18.36 (3.3 examples/sec; 4.886 sec/batch)
2018-05-04 23:17:21.365985: step 59170, loss = 17.64 (3.2 examples/sec; 4.986 sec/batch)
2018-05-04 23:18:10.346734: step 59180, loss = 17.78 (3.2 examples/sec; 5.044 sec/batch)
2018-05-04 23:18:59.678792: step 59190, loss = 18.19 (3.2 examples/sec; 5.054 sec/batch)
2018-05-04 23:19:48.996535: step 59200, loss = 18.26 (3.3 examples/sec; 4.831 sec/batch)
2018-05-04 23:20:42.206139: step 59210, loss = 18.91 (3.2 examples/sec; 5.041 sec/batch)
2018-05-04 23:21:31.848955: step 59220, loss = 18.54 (3.2 examples/sec; 4.983 sec/batch)
2018-05-04 23:22:21.326235: step 59230, loss = 18.25 (3.3 examples/sec; 4.792 sec/batch)
2018-05-04 23:23:10.757785: step 59240, loss = 17.99 (3.2 examples/sec; 4.926 sec/batch)
2018-05-04 23:24:00.095528: step 59250, loss = 18.64 (3.3 examples/sec; 4.918 sec/batch)
2018-05-04 23:24:48.916514: step 59260, loss = 18.64 (3.4 examples/sec; 4.660 sec/batch)
2018-05-04 23:25:37.884091: step 59270, loss = 18.41 (3.2 examples/sec; 5.050 sec/batch)
2018-05-04 23:26:23.590644: step 59280, loss = 17.79 (3.2 examples/sec; 5.011 sec/batch)
2018-05-04 23:27:12.937075: step 59290, loss = 18.00 (3.4 examples/sec; 4.757 sec/batch)
2018-05-04 23:28:02.113280: step 59300, loss = 17.95 (3.2 examples/sec; 5.023 sec/batch)
2018-05-04 23:28:54.604858: step 59310, loss = 18.78 (3.2 examples/sec; 4.943 sec/batch)
2018-05-04 23:29:44.050339: step 59320, loss = 17.99 (3.2 examples/sec; 4.925 sec/batch)
2018-05-04 23:30:33.792834: step 59330, loss = 18.07 (3.3 examples/sec; 4.905 sec/batch)
2018-05-04 23:31:23.316056: step 59340, loss = 17.88 (3.4 examples/sec; 4.752 sec/batch)
2018-05-04 23:32:12.428489: step 59350, loss = 18.04 (3.2 examples/sec; 5.042 sec/batch)
2018-05-04 23:33:01.417310: step 59360, loss = 18.50 (3.3 examples/sec; 4.809 sec/batch)
2018-05-04 23:33:51.462871: step 59370, loss = 17.93 (3.3 examples/sec; 4.864 sec/batch)
2018-05-04 23:34:41.076412: step 59380, loss = 18.33 (3.2 examples/sec; 5.074 sec/batch)
2018-05-04 23:35:30.901396: step 59390, loss = 18.04 (3.3 examples/sec; 4.904 sec/batch)
2018-05-04 23:36:16.959922: step 59400, loss = 18.11 (3.8 examples/sec; 4.186 sec/batch)
2018-05-04 23:37:09.872016: step 59410, loss = 18.20 (3.2 examples/sec; 4.928 sec/batch)
2018-05-04 23:37:59.904317: step 59420, loss = 17.82 (3.1 examples/sec; 5.140 sec/batch)
2018-05-04 23:38:49.564170: step 59430, loss = 18.83 (3.3 examples/sec; 4.903 sec/batch)
2018-05-04 23:39:39.659545: step 59440, loss = 18.13 (3.1 examples/sec; 5.115 sec/batch)
2018-05-04 23:40:29.797428: step 59450, loss = 18.12 (3.2 examples/sec; 5.043 sec/batch)
2018-05-04 23:41:19.225337: step 59460, loss = 18.35 (3.4 examples/sec; 4.743 sec/batch)
2018-05-04 23:42:08.635298: step 59470, loss = 17.94 (3.2 examples/sec; 4.931 sec/batch)
2018-05-04 23:42:57.570483: step 59480, loss = 17.74 (3.2 examples/sec; 4.985 sec/batch)
2018-05-04 23:43:46.756678: step 59490, loss = 17.85 (3.2 examples/sec; 4.967 sec/batch)
2018-05-04 23:44:36.846679: step 59500, loss = 18.19 (3.0 examples/sec; 5.323 sec/batch)
2018-05-04 23:45:30.051951: step 59510, loss = 17.83 (3.3 examples/sec; 4.891 sec/batch)
2018-05-04 23:46:19.931050: step 59520, loss = 18.07 (3.4 examples/sec; 4.666 sec/batch)
2018-05-04 23:47:07.023039: step 59530, loss = 18.49 (3.2 examples/sec; 4.965 sec/batch)
2018-05-04 23:47:56.738819: step 59540, loss = 18.04 (3.2 examples/sec; 5.047 sec/batch)
2018-05-04 23:48:46.366892: step 59550, loss = 18.42 (3.1 examples/sec; 5.155 sec/batch)
2018-05-04 23:49:35.894921: step 59560, loss = 17.92 (3.1 examples/sec; 5.242 sec/batch)
2018-05-04 23:50:25.322846: step 59570, loss = 17.98 (3.3 examples/sec; 4.833 sec/batch)
2018-05-04 23:51:14.646970: step 59580, loss = 18.16 (3.2 examples/sec; 5.036 sec/batch)
2018-05-04 23:52:04.529180: step 59590, loss = 17.76 (3.1 examples/sec; 5.086 sec/batch)
2018-05-04 23:52:54.241734: step 59600, loss = 18.14 (3.2 examples/sec; 4.979 sec/batch)
2018-05-04 23:53:47.269744: step 59610, loss = 18.04 (3.2 examples/sec; 5.002 sec/batch)
2018-05-04 23:54:36.728710: step 59620, loss = 18.31 (3.2 examples/sec; 4.959 sec/batch)
2018-05-04 23:55:25.675375: step 59630, loss = 17.91 (3.2 examples/sec; 4.923 sec/batch)
2018-05-04 23:56:15.467861: step 59640, loss = 18.08 (3.2 examples/sec; 4.978 sec/batch)
2018-05-04 23:57:01.465890: step 59650, loss = 18.16 (3.3 examples/sec; 4.920 sec/batch)
2018-05-04 23:57:50.521538: step 59660, loss = 17.92 (3.2 examples/sec; 5.022 sec/batch)
2018-05-04 23:58:39.320735: step 59670, loss = 18.22 (3.2 examples/sec; 4.944 sec/batch)
2018-05-04 23:59:28.322133: step 59680, loss = 18.07 (3.3 examples/sec; 4.888 sec/batch)
2018-05-05 00:00:17.339593: step 59690, loss = 18.26 (3.3 examples/sec; 4.907 sec/batch)
2018-05-05 00:01:06.654796: step 59700, loss = 17.90 (3.3 examples/sec; 4.796 sec/batch)
2018-05-05 00:01:59.761032: step 59710, loss = 18.31 (3.2 examples/sec; 5.000 sec/batch)
2018-05-05 00:02:48.963871: step 59720, loss = 18.10 (3.3 examples/sec; 4.827 sec/batch)
2018-05-05 00:03:37.735823: step 59730, loss = 17.91 (3.2 examples/sec; 4.977 sec/batch)
2018-05-05 00:04:26.687436: step 59740, loss = 18.00 (3.2 examples/sec; 5.022 sec/batch)
2018-05-05 00:05:16.594965: step 59750, loss = 18.25 (3.2 examples/sec; 5.023 sec/batch)
2018-05-05 00:06:06.316256: step 59760, loss = 17.78 (3.2 examples/sec; 5.035 sec/batch)
2018-05-05 00:06:54.900290: step 59770, loss = 18.26 (4.2 examples/sec; 3.773 sec/batch)
2018-05-05 00:07:41.634114: step 59780, loss = 18.29 (3.4 examples/sec; 4.693 sec/batch)
2018-05-05 00:08:30.856481: step 59790, loss = 18.08 (3.2 examples/sec; 4.940 sec/batch)
2018-05-05 00:09:20.281932: step 59800, loss = 18.06 (3.2 examples/sec; 4.996 sec/batch)
2018-05-05 00:10:13.090337: step 59810, loss = 17.77 (3.3 examples/sec; 4.859 sec/batch)
2018-05-05 00:11:02.676500: step 59820, loss = 18.02 (3.2 examples/sec; 4.959 sec/batch)
2018-05-05 00:11:51.712934: step 59830, loss = 17.83 (3.3 examples/sec; 4.850 sec/batch)
2018-05-05 00:12:40.862924: step 59840, loss = 18.49 (3.3 examples/sec; 4.911 sec/batch)
2018-05-05 00:13:29.856392: step 59850, loss = 18.18 (3.2 examples/sec; 4.958 sec/batch)
2018-05-05 00:14:18.822564: step 59860, loss = 18.33 (3.3 examples/sec; 4.909 sec/batch)
2018-05-05 00:15:08.377544: step 59870, loss = 18.09 (3.2 examples/sec; 4.965 sec/batch)
2018-05-05 00:15:58.181387: step 59880, loss = 18.39 (3.2 examples/sec; 5.021 sec/batch)
2018-05-05 00:16:47.806214: step 59890, loss = 17.98 (3.3 examples/sec; 4.869 sec/batch)
2018-05-05 00:17:33.555474: step 59900, loss = 17.67 (3.3 examples/sec; 4.849 sec/batch)
2018-05-05 00:18:26.524287: step 59910, loss = 18.06 (3.3 examples/sec; 4.899 sec/batch)
2018-05-05 00:19:15.883467: step 59920, loss = 18.54 (3.2 examples/sec; 5.046 sec/batch)
2018-05-05 00:20:04.710281: step 59930, loss = 18.41 (3.3 examples/sec; 4.844 sec/batch)
2018-05-05 00:20:54.546971: step 59940, loss = 17.76 (3.2 examples/sec; 5.064 sec/batch)
2018-05-05 00:21:44.303964: step 59950, loss = 18.00 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 00:22:33.891528: step 59960, loss = 18.56 (3.2 examples/sec; 4.985 sec/batch)
2018-05-05 00:23:24.528211: step 59970, loss = 18.79 (3.1 examples/sec; 5.154 sec/batch)
2018-05-05 00:24:17.026742: step 59980, loss = 18.25 (3.3 examples/sec; 4.794 sec/batch)
2018-05-05 00:25:06.496558: step 59990, loss = 18.31 (3.1 examples/sec; 5.127 sec/batch)
2018-05-05 00:25:56.258350: step 60000, loss = 17.95 (3.1 examples/sec; 5.125 sec/batch)
2018-05-05 00:26:49.574348: step 60010, loss = 18.58 (3.3 examples/sec; 4.785 sec/batch)
2018-05-05 00:27:35.237040: step 60020, loss = 18.18 (4.3 examples/sec; 3.712 sec/batch)
2018-05-05 00:28:24.093628: step 60030, loss = 17.84 (3.3 examples/sec; 4.874 sec/batch)
2018-05-05 00:29:13.833854: step 60040, loss = 17.67 (3.2 examples/sec; 4.995 sec/batch)
2018-05-05 00:30:03.567797: step 60050, loss = 18.21 (3.1 examples/sec; 5.087 sec/batch)
2018-05-05 00:30:53.072610: step 60060, loss = 17.94 (3.3 examples/sec; 4.823 sec/batch)
2018-05-05 00:31:42.990956: step 60070, loss = 18.12 (3.1 examples/sec; 5.099 sec/batch)
2018-05-05 00:32:32.772692: step 60080, loss = 17.92 (3.2 examples/sec; 5.016 sec/batch)
2018-05-05 00:33:22.132888: step 60090, loss = 18.44 (3.2 examples/sec; 4.979 sec/batch)
2018-05-05 00:34:11.433124: step 60100, loss = 18.08 (3.2 examples/sec; 5.071 sec/batch)
2018-05-05 00:35:05.175914: step 60110, loss = 18.07 (3.2 examples/sec; 5.056 sec/batch)
2018-05-05 00:35:54.838586: step 60120, loss = 17.80 (3.2 examples/sec; 5.041 sec/batch)
2018-05-05 00:36:44.494955: step 60130, loss = 17.99 (3.3 examples/sec; 4.923 sec/batch)
2018-05-05 00:37:33.733932: step 60140, loss = 17.82 (3.3 examples/sec; 4.911 sec/batch)
2018-05-05 00:38:19.950289: step 60150, loss = 17.92 (3.1 examples/sec; 5.080 sec/batch)
2018-05-05 00:39:09.816932: step 60160, loss = 18.07 (3.2 examples/sec; 4.968 sec/batch)
2018-05-05 00:39:59.693174: step 60170, loss = 17.85 (3.1 examples/sec; 5.233 sec/batch)
2018-05-05 00:40:49.721505: step 60180, loss = 17.89 (3.2 examples/sec; 5.076 sec/batch)
2018-05-05 00:41:39.613175: step 60190, loss = 18.11 (3.2 examples/sec; 4.930 sec/batch)
2018-05-05 00:42:28.948188: step 60200, loss = 17.87 (3.3 examples/sec; 4.860 sec/batch)
2018-05-05 00:43:21.890740: step 60210, loss = 17.76 (3.2 examples/sec; 4.980 sec/batch)
2018-05-05 00:44:11.787875: step 60220, loss = 18.08 (3.2 examples/sec; 5.047 sec/batch)
2018-05-05 00:45:02.125880: step 60230, loss = 18.40 (3.2 examples/sec; 4.935 sec/batch)
2018-05-05 00:45:52.183896: step 60240, loss = 17.86 (3.3 examples/sec; 4.919 sec/batch)
2018-05-05 00:46:41.655647: step 60250, loss = 18.57 (3.3 examples/sec; 4.918 sec/batch)
2018-05-05 00:47:31.020691: step 60260, loss = 18.07 (3.2 examples/sec; 5.069 sec/batch)
2018-05-05 00:48:18.134709: step 60270, loss = 18.30 (3.1 examples/sec; 5.181 sec/batch)
2018-05-05 00:49:07.116246: step 60280, loss = 18.26 (3.3 examples/sec; 4.815 sec/batch)
2018-05-05 00:49:57.325348: step 60290, loss = 18.13 (3.1 examples/sec; 5.098 sec/batch)
2018-05-05 00:50:46.780345: step 60300, loss = 17.94 (3.3 examples/sec; 4.877 sec/batch)
2018-05-05 00:51:40.230323: step 60310, loss = 18.38 (3.2 examples/sec; 4.936 sec/batch)
2018-05-05 00:52:30.457512: step 60320, loss = 17.80 (3.2 examples/sec; 4.948 sec/batch)
2018-05-05 00:53:20.720434: step 60330, loss = 18.24 (3.4 examples/sec; 4.774 sec/batch)
2018-05-05 00:54:09.743095: step 60340, loss = 18.08 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 00:54:59.564548: step 60350, loss = 18.15 (3.2 examples/sec; 5.059 sec/batch)
2018-05-05 00:55:48.982403: step 60360, loss = 17.69 (3.2 examples/sec; 4.942 sec/batch)
2018-05-05 00:56:38.949094: step 60370, loss = 18.20 (3.1 examples/sec; 5.195 sec/batch)
2018-05-05 00:57:28.075509: step 60380, loss = 18.10 (3.2 examples/sec; 4.978 sec/batch)
2018-05-05 00:58:15.847155: step 60390, loss = 18.53 (4.2 examples/sec; 3.800 sec/batch)
2018-05-05 00:59:03.939623: step 60400, loss = 17.82 (3.2 examples/sec; 5.068 sec/batch)
2018-05-05 00:59:56.809508: step 60410, loss = 17.87 (3.1 examples/sec; 5.182 sec/batch)
2018-05-05 01:00:46.823135: step 60420, loss = 18.02 (3.2 examples/sec; 5.013 sec/batch)
2018-05-05 01:01:36.233630: step 60430, loss = 18.17 (3.2 examples/sec; 4.968 sec/batch)
2018-05-05 01:02:26.463967: step 60440, loss = 18.32 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 01:03:16.272181: step 60450, loss = 18.03 (3.2 examples/sec; 4.984 sec/batch)
2018-05-05 01:04:05.956833: step 60460, loss = 18.23 (3.2 examples/sec; 4.960 sec/batch)
2018-05-05 01:04:55.996132: step 60470, loss = 17.68 (3.2 examples/sec; 5.059 sec/batch)
2018-05-05 01:05:46.322888: step 60480, loss = 17.87 (3.2 examples/sec; 5.037 sec/batch)
2018-05-05 01:06:35.963898: step 60490, loss = 18.30 (3.1 examples/sec; 5.179 sec/batch)
2018-05-05 01:07:25.820373: step 60500, loss = 18.05 (3.1 examples/sec; 5.087 sec/batch)
2018-05-05 01:08:18.885400: step 60510, loss = 18.27 (3.3 examples/sec; 4.809 sec/batch)
2018-05-05 01:09:04.894241: step 60520, loss = 18.45 (3.2 examples/sec; 5.008 sec/batch)
2018-05-05 01:09:54.749573: step 60530, loss = 17.77 (3.1 examples/sec; 5.082 sec/batch)
2018-05-05 01:10:44.267761: step 60540, loss = 18.07 (3.2 examples/sec; 5.043 sec/batch)
2018-05-05 01:11:33.730777: step 60550, loss = 17.89 (3.2 examples/sec; 5.028 sec/batch)
2018-05-05 01:12:22.783447: step 60560, loss = 18.30 (3.2 examples/sec; 4.994 sec/batch)
2018-05-05 01:13:12.752519: step 60570, loss = 18.20 (3.2 examples/sec; 5.054 sec/batch)
2018-05-05 01:14:02.737321: step 60580, loss = 17.70 (3.2 examples/sec; 4.959 sec/batch)
2018-05-05 01:14:51.988192: step 60590, loss = 18.10 (3.2 examples/sec; 4.986 sec/batch)
2018-05-05 01:15:42.007908: step 60600, loss = 17.80 (3.3 examples/sec; 4.877 sec/batch)
2018-05-05 01:16:35.040693: step 60610, loss = 17.91 (3.3 examples/sec; 4.903 sec/batch)
2018-05-05 01:17:24.078431: step 60620, loss = 18.48 (3.1 examples/sec; 5.129 sec/batch)
2018-05-05 01:18:13.530910: step 60630, loss = 18.06 (3.3 examples/sec; 4.871 sec/batch)
2018-05-05 01:18:59.490432: step 60640, loss = 17.82 (3.3 examples/sec; 4.828 sec/batch)
2018-05-05 01:19:49.714878: step 60650, loss = 18.23 (3.2 examples/sec; 4.932 sec/batch)
2018-05-05 01:20:38.870124: step 60660, loss = 17.87 (3.3 examples/sec; 4.855 sec/batch)
2018-05-05 01:21:27.749663: step 60670, loss = 17.98 (3.3 examples/sec; 4.860 sec/batch)
2018-05-05 01:22:17.006237: step 60680, loss = 18.04 (3.3 examples/sec; 4.883 sec/batch)
2018-05-05 01:23:05.852930: step 60690, loss = 17.96 (3.4 examples/sec; 4.761 sec/batch)
2018-05-05 01:23:55.396130: step 60700, loss = 17.98 (3.1 examples/sec; 5.085 sec/batch)
2018-05-05 01:24:48.412871: step 60710, loss = 17.66 (3.3 examples/sec; 4.892 sec/batch)
2018-05-05 01:25:37.668505: step 60720, loss = 17.73 (3.2 examples/sec; 4.962 sec/batch)
2018-05-05 01:26:27.217361: step 60730, loss = 18.13 (3.3 examples/sec; 4.918 sec/batch)
2018-05-05 01:27:16.604854: step 60740, loss = 18.64 (3.3 examples/sec; 4.778 sec/batch)
2018-05-05 01:28:06.396120: step 60750, loss = 18.10 (3.2 examples/sec; 5.028 sec/batch)
2018-05-05 01:28:55.637579: step 60760, loss = 17.88 (3.2 examples/sec; 4.935 sec/batch)
2018-05-05 01:29:42.024061: step 60770, loss = 17.99 (3.3 examples/sec; 4.819 sec/batch)
2018-05-05 01:30:32.012730: step 60780, loss = 17.69 (3.2 examples/sec; 4.961 sec/batch)
2018-05-05 01:31:21.700313: step 60790, loss = 17.98 (3.2 examples/sec; 4.937 sec/batch)
2018-05-05 01:32:11.046714: step 60800, loss = 18.06 (3.2 examples/sec; 4.945 sec/batch)
2018-05-05 01:33:03.708427: step 60810, loss = 18.10 (3.3 examples/sec; 4.838 sec/batch)
2018-05-05 01:33:53.115861: step 60820, loss = 17.92 (3.3 examples/sec; 4.781 sec/batch)
2018-05-05 01:34:43.198061: step 60830, loss = 18.34 (3.3 examples/sec; 4.910 sec/batch)
2018-05-05 01:35:32.581007: step 60840, loss = 18.76 (3.2 examples/sec; 4.982 sec/batch)
2018-05-05 01:36:22.419511: step 60850, loss = 18.23 (3.2 examples/sec; 5.056 sec/batch)
2018-05-05 01:37:11.180129: step 60860, loss = 17.98 (3.3 examples/sec; 4.899 sec/batch)
2018-05-05 01:38:01.278740: step 60870, loss = 17.83 (3.1 examples/sec; 5.184 sec/batch)
2018-05-05 01:38:50.745874: step 60880, loss = 17.80 (3.3 examples/sec; 4.903 sec/batch)
2018-05-05 01:39:36.674781: step 60890, loss = 18.42 (3.3 examples/sec; 4.847 sec/batch)
2018-05-05 01:40:25.456367: step 60900, loss = 17.97 (3.3 examples/sec; 4.850 sec/batch)
2018-05-05 01:41:18.781021: step 60910, loss = 18.32 (3.1 examples/sec; 5.198 sec/batch)
2018-05-05 01:42:08.324669: step 60920, loss = 18.65 (3.2 examples/sec; 4.936 sec/batch)
2018-05-05 01:42:58.315584: step 60930, loss = 18.01 (3.3 examples/sec; 4.874 sec/batch)
2018-05-05 01:43:47.633378: step 60940, loss = 17.76 (3.3 examples/sec; 4.858 sec/batch)
2018-05-05 01:44:36.316039: step 60950, loss = 18.17 (3.3 examples/sec; 4.895 sec/batch)
2018-05-05 01:45:25.321439: step 60960, loss = 18.63 (3.2 examples/sec; 5.070 sec/batch)
2018-05-05 01:46:14.580428: step 60970, loss = 18.23 (3.2 examples/sec; 4.943 sec/batch)
2018-05-05 01:47:03.796362: step 60980, loss = 18.73 (3.2 examples/sec; 5.013 sec/batch)
2018-05-05 01:47:54.007743: step 60990, loss = 18.12 (3.3 examples/sec; 4.915 sec/batch)
2018-05-05 01:48:43.912729: step 61000, loss = 18.31 (3.2 examples/sec; 4.983 sec/batch)
2018-05-05 01:49:35.910665: step 61010, loss = 18.28 (4.1 examples/sec; 3.909 sec/batch)
2018-05-05 01:50:24.690854: step 61020, loss = 17.81 (3.2 examples/sec; 5.069 sec/batch)
2018-05-05 01:51:14.369770: step 61030, loss = 17.74 (3.2 examples/sec; 4.949 sec/batch)
2018-05-05 01:52:03.909832: step 61040, loss = 17.95 (3.2 examples/sec; 4.938 sec/batch)
2018-05-05 01:52:54.178759: step 61050, loss = 18.23 (3.2 examples/sec; 4.957 sec/batch)
2018-05-05 01:53:43.839027: step 61060, loss = 17.97 (3.2 examples/sec; 4.932 sec/batch)
2018-05-05 01:54:32.962262: step 61070, loss = 18.30 (3.2 examples/sec; 4.938 sec/batch)
2018-05-05 01:55:22.141405: step 61080, loss = 17.98 (3.2 examples/sec; 4.978 sec/batch)
2018-05-05 01:56:12.329807: step 61090, loss = 18.06 (3.3 examples/sec; 4.837 sec/batch)
2018-05-05 01:57:01.838843: step 61100, loss = 18.42 (3.2 examples/sec; 4.957 sec/batch)
2018-05-05 01:57:55.660667: step 61110, loss = 17.86 (3.4 examples/sec; 4.748 sec/batch)
2018-05-05 01:58:45.167548: step 61120, loss = 18.39 (3.2 examples/sec; 4.953 sec/batch)
2018-05-05 01:59:35.114282: step 61130, loss = 18.48 (3.2 examples/sec; 4.927 sec/batch)
2018-05-05 02:00:20.897356: step 61140, loss = 18.04 (3.2 examples/sec; 4.959 sec/batch)
2018-05-05 02:01:10.549022: step 61150, loss = 17.85 (3.2 examples/sec; 4.938 sec/batch)
2018-05-05 02:02:00.095177: step 61160, loss = 17.76 (3.2 examples/sec; 5.049 sec/batch)
2018-05-05 02:02:50.300581: step 61170, loss = 18.25 (3.2 examples/sec; 5.014 sec/batch)
2018-05-05 02:03:39.914314: step 61180, loss = 17.89 (3.3 examples/sec; 4.883 sec/batch)
2018-05-05 02:04:29.588247: step 61190, loss = 18.55 (3.2 examples/sec; 5.062 sec/batch)
2018-05-05 02:05:19.788134: step 61200, loss = 18.16 (3.2 examples/sec; 5.056 sec/batch)
2018-05-05 02:06:13.373276: step 61210, loss = 18.76 (3.2 examples/sec; 4.965 sec/batch)
2018-05-05 02:07:02.295476: step 61220, loss = 17.90 (3.2 examples/sec; 4.999 sec/batch)
2018-05-05 02:07:52.340206: step 61230, loss = 18.38 (3.3 examples/sec; 4.901 sec/batch)
2018-05-05 02:08:41.401503: step 61240, loss = 17.87 (3.4 examples/sec; 4.764 sec/batch)
2018-05-05 02:09:30.974400: step 61250, loss = 17.77 (3.2 examples/sec; 5.009 sec/batch)
2018-05-05 02:10:16.804114: step 61260, loss = 18.37 (3.3 examples/sec; 4.875 sec/batch)
2018-05-05 02:11:06.549734: step 61270, loss = 18.32 (3.3 examples/sec; 4.908 sec/batch)
2018-05-05 02:11:55.799194: step 61280, loss = 17.80 (3.2 examples/sec; 5.008 sec/batch)
2018-05-05 02:12:44.593629: step 61290, loss = 17.65 (3.3 examples/sec; 4.846 sec/batch)
2018-05-05 02:13:34.682270: step 61300, loss = 18.01 (3.1 examples/sec; 5.088 sec/batch)
2018-05-05 02:14:27.672383: step 61310, loss = 17.74 (3.3 examples/sec; 4.860 sec/batch)
2018-05-05 02:15:17.158918: step 61320, loss = 18.34 (3.4 examples/sec; 4.752 sec/batch)
2018-05-05 02:16:06.470746: step 61330, loss = 18.84 (3.2 examples/sec; 5.005 sec/batch)
2018-05-05 02:16:56.176403: step 61340, loss = 18.40 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 02:17:46.078754: step 61350, loss = 17.73 (3.3 examples/sec; 4.855 sec/batch)
2018-05-05 02:18:35.206888: step 61360, loss = 18.38 (3.2 examples/sec; 4.976 sec/batch)
2018-05-05 02:19:25.580581: step 61370, loss = 18.26 (3.1 examples/sec; 5.163 sec/batch)
2018-05-05 02:20:14.695251: step 61380, loss = 18.20 (3.3 examples/sec; 4.857 sec/batch)
2018-05-05 02:21:00.980974: step 61390, loss = 18.09 (3.2 examples/sec; 4.983 sec/batch)
2018-05-05 02:21:50.609331: step 61400, loss = 17.84 (3.3 examples/sec; 4.856 sec/batch)
2018-05-05 02:22:44.037049: step 61410, loss = 18.46 (3.2 examples/sec; 4.986 sec/batch)
2018-05-05 02:23:33.369281: step 61420, loss = 18.77 (3.3 examples/sec; 4.821 sec/batch)
2018-05-05 02:24:23.727296: step 61430, loss = 17.81 (3.2 examples/sec; 4.934 sec/batch)
2018-05-05 02:25:12.924217: step 61440, loss = 18.34 (3.3 examples/sec; 4.834 sec/batch)
2018-05-05 02:26:03.212883: step 61450, loss = 17.99 (3.2 examples/sec; 5.074 sec/batch)
2018-05-05 02:26:52.696548: step 61460, loss = 18.07 (3.1 examples/sec; 5.085 sec/batch)
2018-05-05 02:27:41.605107: step 61470, loss = 17.90 (3.2 examples/sec; 4.963 sec/batch)
2018-05-05 02:28:30.431960: step 61480, loss = 17.95 (3.1 examples/sec; 5.094 sec/batch)
2018-05-05 02:29:19.097082: step 61490, loss = 17.98 (3.3 examples/sec; 4.890 sec/batch)
2018-05-05 02:30:07.922462: step 61500, loss = 17.97 (3.2 examples/sec; 4.970 sec/batch)
2018-05-05 02:30:57.550947: step 61510, loss = 18.28 (3.2 examples/sec; 4.984 sec/batch)
2018-05-05 02:31:47.805510: step 61520, loss = 18.19 (3.2 examples/sec; 5.047 sec/batch)
2018-05-05 02:32:37.376996: step 61530, loss = 17.98 (3.3 examples/sec; 4.876 sec/batch)
2018-05-05 02:33:26.773467: step 61540, loss = 17.93 (3.2 examples/sec; 5.059 sec/batch)
2018-05-05 02:34:16.100153: step 61550, loss = 18.48 (3.3 examples/sec; 4.910 sec/batch)
2018-05-05 02:35:05.283741: step 61560, loss = 17.91 (3.3 examples/sec; 4.835 sec/batch)
2018-05-05 02:35:55.780560: step 61570, loss = 18.33 (3.2 examples/sec; 4.928 sec/batch)
2018-05-05 02:36:45.467337: step 61580, loss = 18.04 (3.2 examples/sec; 4.969 sec/batch)
2018-05-05 02:37:34.451707: step 61590, loss = 17.74 (3.2 examples/sec; 4.987 sec/batch)
2018-05-05 02:38:23.667360: step 61600, loss = 17.78 (3.2 examples/sec; 4.993 sec/batch)
2018-05-05 02:39:16.643213: step 61610, loss = 18.60 (3.2 examples/sec; 5.009 sec/batch)
2018-05-05 02:40:05.579746: step 61620, loss = 17.92 (3.3 examples/sec; 4.918 sec/batch)
2018-05-05 02:40:53.750909: step 61630, loss = 17.86 (4.2 examples/sec; 3.799 sec/batch)
2018-05-05 02:41:40.765576: step 61640, loss = 18.02 (3.3 examples/sec; 4.898 sec/batch)
2018-05-05 02:42:29.364205: step 61650, loss = 17.78 (3.2 examples/sec; 4.980 sec/batch)
2018-05-05 02:43:18.505401: step 61660, loss = 17.86 (3.2 examples/sec; 5.038 sec/batch)
2018-05-05 02:44:08.047728: step 61670, loss = 17.65 (3.2 examples/sec; 5.017 sec/batch)
2018-05-05 02:44:57.061628: step 61680, loss = 17.71 (3.3 examples/sec; 4.862 sec/batch)
2018-05-05 02:45:46.195659: step 61690, loss = 17.86 (3.3 examples/sec; 4.780 sec/batch)
2018-05-05 02:46:36.338343: step 61700, loss = 17.72 (3.3 examples/sec; 4.879 sec/batch)
2018-05-05 02:47:29.660709: step 61710, loss = 18.32 (3.2 examples/sec; 4.951 sec/batch)
2018-05-05 02:48:19.224216: step 61720, loss = 17.80 (3.2 examples/sec; 5.027 sec/batch)
2018-05-05 02:49:08.482078: step 61730, loss = 17.96 (3.2 examples/sec; 4.934 sec/batch)
2018-05-05 02:49:58.617385: step 61740, loss = 18.64 (3.1 examples/sec; 5.104 sec/batch)
2018-05-05 02:50:48.514193: step 61750, loss = 18.86 (3.2 examples/sec; 4.977 sec/batch)
2018-05-05 02:51:34.762989: step 61760, loss = 17.65 (3.3 examples/sec; 4.835 sec/batch)
2018-05-05 02:52:24.121120: step 61770, loss = 17.70 (3.2 examples/sec; 4.937 sec/batch)
2018-05-05 02:53:13.563343: step 61780, loss = 18.01 (3.2 examples/sec; 5.009 sec/batch)
2018-05-05 02:54:02.653873: step 61790, loss = 18.04 (3.3 examples/sec; 4.778 sec/batch)
2018-05-05 02:54:52.067319: step 61800, loss = 18.11 (3.2 examples/sec; 5.044 sec/batch)
2018-05-05 02:55:44.719091: step 61810, loss = 18.39 (3.3 examples/sec; 4.885 sec/batch)
2018-05-05 02:56:34.521260: step 61820, loss = 17.99 (3.3 examples/sec; 4.838 sec/batch)
2018-05-05 02:57:23.932547: step 61830, loss = 17.63 (3.2 examples/sec; 4.927 sec/batch)
2018-05-05 02:58:13.794611: step 61840, loss = 17.98 (3.1 examples/sec; 5.210 sec/batch)
2018-05-05 02:59:03.177363: step 61850, loss = 18.41 (3.2 examples/sec; 4.987 sec/batch)
2018-05-05 02:59:52.852222: step 61860, loss = 18.06 (3.3 examples/sec; 4.918 sec/batch)
2018-05-05 03:00:42.253016: step 61870, loss = 18.16 (3.3 examples/sec; 4.874 sec/batch)
2018-05-05 03:01:31.320380: step 61880, loss = 18.55 (3.5 examples/sec; 4.563 sec/batch)
2018-05-05 03:02:17.380445: step 61890, loss = 18.12 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 03:03:07.580533: step 61900, loss = 18.27 (3.1 examples/sec; 5.158 sec/batch)
2018-05-05 03:04:00.971089: step 61910, loss = 17.91 (3.3 examples/sec; 4.853 sec/batch)
2018-05-05 03:04:50.571018: step 61920, loss = 18.38 (3.3 examples/sec; 4.904 sec/batch)
2018-05-05 03:05:39.534883: step 61930, loss = 17.96 (3.3 examples/sec; 4.869 sec/batch)
2018-05-05 03:06:29.171500: step 61940, loss = 18.16 (3.2 examples/sec; 4.931 sec/batch)
2018-05-05 03:07:19.013831: step 61950, loss = 17.99 (3.2 examples/sec; 4.993 sec/batch)
2018-05-05 03:08:07.603834: step 61960, loss = 17.98 (3.3 examples/sec; 4.879 sec/batch)
2018-05-05 03:08:56.863107: step 61970, loss = 17.90 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 03:09:46.133784: step 61980, loss = 18.27 (3.3 examples/sec; 4.857 sec/batch)
2018-05-05 03:10:35.457430: step 61990, loss = 18.12 (3.3 examples/sec; 4.850 sec/batch)
2018-05-05 03:11:24.732876: step 62000, loss = 17.88 (3.3 examples/sec; 4.887 sec/batch)
2018-05-05 03:12:14.392790: step 62010, loss = 17.81 (3.3 examples/sec; 4.898 sec/batch)
2018-05-05 03:13:03.272885: step 62020, loss = 17.75 (3.3 examples/sec; 4.920 sec/batch)
2018-05-05 03:13:53.091022: step 62030, loss = 17.96 (3.3 examples/sec; 4.875 sec/batch)
2018-05-05 03:14:43.169947: step 62040, loss = 18.13 (3.2 examples/sec; 5.014 sec/batch)
2018-05-05 03:15:33.120238: step 62050, loss = 18.04 (3.2 examples/sec; 4.984 sec/batch)
2018-05-05 03:16:22.682920: step 62060, loss = 18.72 (3.2 examples/sec; 4.965 sec/batch)
2018-05-05 03:17:12.359133: step 62070, loss = 18.13 (3.2 examples/sec; 5.050 sec/batch)
2018-05-05 03:18:01.986006: step 62080, loss = 18.02 (3.3 examples/sec; 4.856 sec/batch)
2018-05-05 03:18:51.466251: step 62090, loss = 17.89 (3.2 examples/sec; 4.998 sec/batch)
2018-05-05 03:19:40.867861: step 62100, loss = 17.89 (3.4 examples/sec; 4.752 sec/batch)
2018-05-05 03:20:33.630823: step 62110, loss = 17.94 (3.3 examples/sec; 4.888 sec/batch)
2018-05-05 03:21:23.659277: step 62120, loss = 18.70 (3.2 examples/sec; 4.953 sec/batch)
2018-05-05 03:22:11.311936: step 62130, loss = 18.63 (4.2 examples/sec; 3.785 sec/batch)
2018-05-05 03:22:58.599679: step 62140, loss = 17.87 (3.4 examples/sec; 4.730 sec/batch)
2018-05-05 03:23:48.451589: step 62150, loss = 18.18 (3.2 examples/sec; 4.990 sec/batch)
2018-05-05 03:24:37.754298: step 62160, loss = 18.30 (3.3 examples/sec; 4.798 sec/batch)
2018-05-05 03:25:27.411647: step 62170, loss = 18.15 (3.3 examples/sec; 4.845 sec/batch)
2018-05-05 03:26:16.559301: step 62180, loss = 18.19 (3.3 examples/sec; 4.887 sec/batch)
2018-05-05 03:27:05.796305: step 62190, loss = 17.73 (3.3 examples/sec; 4.785 sec/batch)
2018-05-05 03:27:55.161246: step 62200, loss = 18.12 (3.3 examples/sec; 4.921 sec/batch)
2018-05-05 03:28:48.181158: step 62210, loss = 18.68 (3.3 examples/sec; 4.918 sec/batch)
2018-05-05 03:29:36.997497: step 62220, loss = 17.69 (3.2 examples/sec; 5.040 sec/batch)
2018-05-05 03:30:26.409653: step 62230, loss = 17.90 (3.2 examples/sec; 5.012 sec/batch)
2018-05-05 03:31:15.562728: step 62240, loss = 18.18 (3.2 examples/sec; 4.926 sec/batch)
2018-05-05 03:32:04.766843: step 62250, loss = 17.83 (3.3 examples/sec; 4.883 sec/batch)
2018-05-05 03:32:50.770596: step 62260, loss = 18.19 (3.2 examples/sec; 4.996 sec/batch)
2018-05-05 03:33:39.779609: step 62270, loss = 17.74 (3.2 examples/sec; 4.954 sec/batch)
2018-05-05 03:34:29.189678: step 62280, loss = 17.87 (3.4 examples/sec; 4.745 sec/batch)
2018-05-05 03:35:18.285259: step 62290, loss = 17.84 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 03:36:08.431244: step 62300, loss = 18.03 (3.2 examples/sec; 4.945 sec/batch)
2018-05-05 03:37:01.806383: step 62310, loss = 18.15 (3.2 examples/sec; 4.924 sec/batch)
2018-05-05 03:37:51.743767: step 62320, loss = 18.22 (3.1 examples/sec; 5.117 sec/batch)
2018-05-05 03:38:41.122353: step 62330, loss = 18.09 (3.2 examples/sec; 4.958 sec/batch)
2018-05-05 03:39:30.458628: step 62340, loss = 18.19 (3.2 examples/sec; 4.980 sec/batch)
2018-05-05 03:40:20.222182: step 62350, loss = 18.57 (3.1 examples/sec; 5.091 sec/batch)
2018-05-05 03:41:10.248323: step 62360, loss = 18.27 (3.1 examples/sec; 5.089 sec/batch)
2018-05-05 03:41:59.674915: step 62370, loss = 17.68 (3.2 examples/sec; 5.016 sec/batch)
2018-05-05 03:42:48.893946: step 62380, loss = 18.52 (3.3 examples/sec; 4.804 sec/batch)
2018-05-05 03:43:36.236347: step 62390, loss = 17.90 (3.1 examples/sec; 5.122 sec/batch)
2018-05-05 03:44:25.709073: step 62400, loss = 18.30 (3.2 examples/sec; 5.014 sec/batch)
2018-05-05 03:45:19.303217: step 62410, loss = 18.03 (3.2 examples/sec; 5.066 sec/batch)
2018-05-05 03:46:08.901969: step 62420, loss = 17.99 (3.1 examples/sec; 5.088 sec/batch)
2018-05-05 03:46:58.689026: step 62430, loss = 18.51 (3.1 examples/sec; 5.113 sec/batch)
2018-05-05 03:47:47.989141: step 62440, loss = 17.98 (3.3 examples/sec; 4.917 sec/batch)
2018-05-05 03:48:37.173471: step 62450, loss = 17.92 (3.3 examples/sec; 4.896 sec/batch)
2018-05-05 03:49:26.957903: step 62460, loss = 18.33 (3.1 examples/sec; 5.222 sec/batch)
2018-05-05 03:50:16.854478: step 62470, loss = 18.21 (3.2 examples/sec; 4.982 sec/batch)
2018-05-05 03:51:06.475898: step 62480, loss = 17.71 (3.2 examples/sec; 5.071 sec/batch)
2018-05-05 03:51:56.664211: step 62490, loss = 17.76 (3.1 examples/sec; 5.147 sec/batch)
2018-05-05 03:52:47.113965: step 62500, loss = 18.31 (3.3 examples/sec; 4.826 sec/batch)
2018-05-05 03:53:36.656690: step 62510, loss = 17.87 (3.2 examples/sec; 5.002 sec/batch)
2018-05-05 03:54:26.840666: step 62520, loss = 18.04 (3.3 examples/sec; 4.835 sec/batch)
2018-05-05 03:55:16.706863: step 62530, loss = 17.86 (3.3 examples/sec; 4.800 sec/batch)
2018-05-05 03:56:06.793110: step 62540, loss = 18.81 (3.1 examples/sec; 5.116 sec/batch)
2018-05-05 03:56:55.638950: step 62550, loss = 18.10 (3.3 examples/sec; 4.885 sec/batch)
2018-05-05 03:57:45.154876: step 62560, loss = 18.21 (3.3 examples/sec; 4.881 sec/batch)
2018-05-05 03:58:34.509249: step 62570, loss = 17.90 (3.1 examples/sec; 5.142 sec/batch)
2018-05-05 03:59:24.282358: step 62580, loss = 18.26 (3.1 examples/sec; 5.091 sec/batch)
2018-05-05 04:00:14.490899: step 62590, loss = 18.47 (3.1 examples/sec; 5.094 sec/batch)
2018-05-05 04:01:03.698145: step 62600, loss = 17.92 (3.2 examples/sec; 4.984 sec/batch)
2018-05-05 04:01:56.505107: step 62610, loss = 18.08 (3.2 examples/sec; 4.982 sec/batch)
2018-05-05 04:02:45.376921: step 62620, loss = 17.99 (3.4 examples/sec; 4.766 sec/batch)
2018-05-05 04:03:31.303274: step 62630, loss = 18.69 (4.2 examples/sec; 3.807 sec/batch)
2018-05-05 04:04:20.349755: step 62640, loss = 17.81 (3.3 examples/sec; 4.876 sec/batch)
2018-05-05 04:05:09.218991: step 62650, loss = 17.81 (3.2 examples/sec; 4.966 sec/batch)
2018-05-05 04:05:59.547265: step 62660, loss = 18.01 (3.2 examples/sec; 4.991 sec/batch)
2018-05-05 04:06:48.694692: step 62670, loss = 18.01 (3.3 examples/sec; 4.885 sec/batch)
2018-05-05 04:07:38.395397: step 62680, loss = 17.82 (3.1 examples/sec; 5.128 sec/batch)
2018-05-05 04:08:27.780869: step 62690, loss = 18.23 (3.2 examples/sec; 4.933 sec/batch)
2018-05-05 04:09:17.379034: step 62700, loss = 17.93 (3.2 examples/sec; 5.022 sec/batch)
2018-05-05 04:10:10.158649: step 62710, loss = 18.50 (3.3 examples/sec; 4.866 sec/batch)
2018-05-05 04:10:59.654831: step 62720, loss = 17.72 (3.2 examples/sec; 5.031 sec/batch)
2018-05-05 04:11:49.074138: step 62730, loss = 17.94 (3.2 examples/sec; 4.926 sec/batch)
2018-05-05 04:12:38.104900: step 62740, loss = 18.00 (3.3 examples/sec; 4.915 sec/batch)
2018-05-05 04:13:28.186789: step 62750, loss = 18.13 (3.1 examples/sec; 5.091 sec/batch)
2018-05-05 04:14:14.593829: step 62760, loss = 17.67 (3.2 examples/sec; 4.948 sec/batch)
2018-05-05 04:15:04.119395: step 62770, loss = 17.71 (3.3 examples/sec; 4.857 sec/batch)
2018-05-05 04:15:53.989857: step 62780, loss = 18.39 (3.3 examples/sec; 4.893 sec/batch)
2018-05-05 04:16:43.211626: step 62790, loss = 18.94 (3.2 examples/sec; 4.953 sec/batch)
2018-05-05 04:17:32.920234: step 62800, loss = 18.19 (3.1 examples/sec; 5.088 sec/batch)
2018-05-05 04:18:25.357138: step 62810, loss = 17.82 (3.2 examples/sec; 4.986 sec/batch)
2018-05-05 04:19:15.321733: step 62820, loss = 17.81 (3.2 examples/sec; 5.030 sec/batch)
2018-05-05 04:20:05.056661: step 62830, loss = 17.74 (3.2 examples/sec; 4.971 sec/batch)
2018-05-05 04:20:54.524091: step 62840, loss = 18.18 (3.1 examples/sec; 5.081 sec/batch)
2018-05-05 04:21:44.100962: step 62850, loss = 17.77 (3.3 examples/sec; 4.861 sec/batch)
2018-05-05 04:22:33.285455: step 62860, loss = 18.41 (3.3 examples/sec; 4.836 sec/batch)
2018-05-05 04:23:22.295692: step 62870, loss = 18.05 (3.2 examples/sec; 4.971 sec/batch)
2018-05-05 04:24:08.732384: step 62880, loss = 17.93 (3.8 examples/sec; 4.201 sec/batch)
2018-05-05 04:24:58.470880: step 62890, loss = 18.26 (3.3 examples/sec; 4.899 sec/batch)
2018-05-05 04:25:48.317388: step 62900, loss = 17.78 (3.2 examples/sec; 4.960 sec/batch)
2018-05-05 04:26:41.249060: step 62910, loss = 18.00 (3.3 examples/sec; 4.922 sec/batch)
2018-05-05 04:27:30.793328: step 62920, loss = 17.93 (3.3 examples/sec; 4.847 sec/batch)
2018-05-05 04:28:19.814733: step 62930, loss = 18.03 (3.2 examples/sec; 5.044 sec/batch)
2018-05-05 04:29:08.781766: step 62940, loss = 18.09 (3.2 examples/sec; 4.928 sec/batch)
2018-05-05 04:29:58.103792: step 62950, loss = 18.83 (3.3 examples/sec; 4.804 sec/batch)
2018-05-05 04:30:47.354057: step 62960, loss = 18.22 (3.3 examples/sec; 4.819 sec/batch)
2018-05-05 04:31:37.915842: step 62970, loss = 17.68 (3.2 examples/sec; 5.064 sec/batch)
2018-05-05 04:32:27.691776: step 62980, loss = 18.29 (3.2 examples/sec; 4.995 sec/batch)
2018-05-05 04:33:16.758308: step 62990, loss = 17.78 (3.4 examples/sec; 4.738 sec/batch)
2018-05-05 04:34:07.128089: step 63000, loss = 17.93 (3.2 examples/sec; 5.052 sec/batch)
2018-05-05 04:34:57.093649: step 63010, loss = 17.95 (3.2 examples/sec; 5.031 sec/batch)
2018-05-05 04:35:46.013727: step 63020, loss = 18.92 (3.3 examples/sec; 4.861 sec/batch)
2018-05-05 04:36:36.212014: step 63030, loss = 18.49 (3.2 examples/sec; 5.043 sec/batch)
2018-05-05 04:37:25.638360: step 63040, loss = 18.46 (3.3 examples/sec; 4.813 sec/batch)
2018-05-05 04:38:15.123293: step 63050, loss = 18.50 (3.2 examples/sec; 4.982 sec/batch)
2018-05-05 04:39:04.976118: step 63060, loss = 18.10 (3.2 examples/sec; 5.054 sec/batch)
2018-05-05 04:39:54.282293: step 63070, loss = 18.44 (3.2 examples/sec; 4.971 sec/batch)
2018-05-05 04:40:44.466260: step 63080, loss = 17.79 (3.2 examples/sec; 4.955 sec/batch)
2018-05-05 04:41:34.389666: step 63090, loss = 18.00 (3.3 examples/sec; 4.846 sec/batch)
2018-05-05 04:42:24.256292: step 63100, loss = 18.61 (3.1 examples/sec; 5.128 sec/batch)
2018-05-05 04:43:17.976685: step 63110, loss = 18.32 (3.3 examples/sec; 4.906 sec/batch)
2018-05-05 04:44:07.585344: step 63120, loss = 17.96 (3.3 examples/sec; 4.839 sec/batch)
2018-05-05 04:44:53.440137: step 63130, loss = 18.03 (3.2 examples/sec; 4.964 sec/batch)
2018-05-05 04:45:43.117623: step 63140, loss = 18.33 (3.2 examples/sec; 4.980 sec/batch)
2018-05-05 04:46:31.929068: step 63150, loss = 17.69 (3.3 examples/sec; 4.859 sec/batch)
2018-05-05 04:47:21.839516: step 63160, loss = 18.30 (3.2 examples/sec; 4.996 sec/batch)
2018-05-05 04:48:11.347431: step 63170, loss = 18.01 (3.2 examples/sec; 5.070 sec/batch)
2018-05-05 04:49:01.001185: step 63180, loss = 18.22 (3.2 examples/sec; 4.969 sec/batch)
2018-05-05 04:49:50.732790: step 63190, loss = 17.74 (3.1 examples/sec; 5.152 sec/batch)
2018-05-05 04:50:39.543611: step 63200, loss = 17.69 (3.3 examples/sec; 4.801 sec/batch)
2018-05-05 04:51:32.996840: step 63210, loss = 18.36 (3.3 examples/sec; 4.911 sec/batch)
2018-05-05 04:52:22.814324: step 63220, loss = 18.14 (3.2 examples/sec; 4.980 sec/batch)
2018-05-05 04:53:12.570125: step 63230, loss = 17.94 (3.2 examples/sec; 5.065 sec/batch)
2018-05-05 04:54:02.612740: step 63240, loss = 18.16 (3.1 examples/sec; 5.193 sec/batch)
2018-05-05 04:54:49.941989: step 63250, loss = 17.92 (4.2 examples/sec; 3.815 sec/batch)
2018-05-05 04:55:38.877405: step 63260, loss = 17.98 (3.4 examples/sec; 4.705 sec/batch)
2018-05-05 04:56:28.507938: step 63270, loss = 18.24 (3.4 examples/sec; 4.713 sec/batch)
2018-05-05 04:57:17.355964: step 63280, loss = 17.99 (3.3 examples/sec; 4.883 sec/batch)
2018-05-05 04:58:07.553769: step 63290, loss = 18.42 (3.2 examples/sec; 5.071 sec/batch)
2018-05-05 04:58:57.212390: step 63300, loss = 19.60 (3.3 examples/sec; 4.823 sec/batch)
2018-05-05 04:59:50.478192: step 63310, loss = 17.75 (3.2 examples/sec; 5.033 sec/batch)
2018-05-05 05:00:40.564339: step 63320, loss = 18.03 (3.3 examples/sec; 4.859 sec/batch)
2018-05-05 05:01:30.186176: step 63330, loss = 17.99 (3.2 examples/sec; 4.924 sec/batch)
2018-05-05 05:02:19.299872: step 63340, loss = 18.11 (3.2 examples/sec; 4.955 sec/batch)
2018-05-05 05:03:08.564089: step 63350, loss = 18.14 (3.3 examples/sec; 4.839 sec/batch)
2018-05-05 05:03:58.165525: step 63360, loss = 18.33 (3.1 examples/sec; 5.241 sec/batch)
2018-05-05 05:04:46.676530: step 63370, loss = 18.20 (3.4 examples/sec; 4.699 sec/batch)
2018-05-05 05:05:33.612065: step 63380, loss = 18.46 (3.1 examples/sec; 5.190 sec/batch)
2018-05-05 05:06:23.456913: step 63390, loss = 17.91 (3.2 examples/sec; 5.033 sec/batch)
2018-05-05 05:07:12.964451: step 63400, loss = 17.96 (3.3 examples/sec; 4.899 sec/batch)
2018-05-05 05:08:06.109696: step 63410, loss = 18.11 (3.1 examples/sec; 5.090 sec/batch)
2018-05-05 05:08:56.211360: step 63420, loss = 17.94 (3.1 examples/sec; 5.095 sec/batch)
2018-05-05 05:09:45.758217: step 63430, loss = 18.45 (3.2 examples/sec; 5.012 sec/batch)
2018-05-05 05:10:35.712390: step 63440, loss = 18.12 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 05:11:25.872954: step 63450, loss = 18.15 (3.2 examples/sec; 5.005 sec/batch)
2018-05-05 05:12:15.632914: step 63460, loss = 17.84 (3.2 examples/sec; 4.949 sec/batch)
2018-05-05 05:13:05.306968: step 63470, loss = 18.25 (3.2 examples/sec; 5.038 sec/batch)
2018-05-05 05:13:55.662192: step 63480, loss = 18.03 (3.3 examples/sec; 4.895 sec/batch)
2018-05-05 05:14:45.742935: step 63490, loss = 17.81 (3.2 examples/sec; 5.071 sec/batch)
2018-05-05 05:15:32.151073: step 63500, loss = 17.93 (3.2 examples/sec; 4.961 sec/batch)
2018-05-05 05:16:25.052929: step 63510, loss = 18.06 (3.2 examples/sec; 4.964 sec/batch)
2018-05-05 05:17:14.626567: step 63520, loss = 18.40 (3.2 examples/sec; 4.984 sec/batch)
2018-05-05 05:18:04.183519: step 63530, loss = 17.85 (3.2 examples/sec; 4.992 sec/batch)
2018-05-05 05:18:54.038666: step 63540, loss = 18.39 (3.3 examples/sec; 4.876 sec/batch)
2018-05-05 05:19:44.149208: step 63550, loss = 18.09 (3.3 examples/sec; 4.837 sec/batch)
2018-05-05 05:20:33.625109: step 63560, loss = 18.69 (3.2 examples/sec; 5.035 sec/batch)
2018-05-05 05:21:23.078831: step 63570, loss = 17.76 (3.2 examples/sec; 4.957 sec/batch)
2018-05-05 05:22:12.059356: step 63580, loss = 18.09 (3.3 examples/sec; 4.850 sec/batch)
2018-05-05 05:23:01.369800: step 63590, loss = 17.95 (3.3 examples/sec; 4.890 sec/batch)
2018-05-05 05:23:51.089973: step 63600, loss = 17.72 (3.3 examples/sec; 4.883 sec/batch)
2018-05-05 05:24:44.007978: step 63610, loss = 17.76 (3.3 examples/sec; 4.892 sec/batch)
2018-05-05 05:25:32.240406: step 63620, loss = 17.93 (4.1 examples/sec; 3.900 sec/batch)
2018-05-05 05:26:18.949515: step 63630, loss = 18.05 (3.3 examples/sec; 4.915 sec/batch)
2018-05-05 05:27:08.836175: step 63640, loss = 18.30 (3.4 examples/sec; 4.743 sec/batch)
2018-05-05 05:27:58.462548: step 63650, loss = 17.83 (3.2 examples/sec; 5.024 sec/batch)
2018-05-05 05:28:48.494251: step 63660, loss = 17.74 (3.3 examples/sec; 4.881 sec/batch)
2018-05-05 05:29:38.217330: step 63670, loss = 18.12 (3.3 examples/sec; 4.862 sec/batch)
2018-05-05 05:30:27.393307: step 63680, loss = 18.02 (3.2 examples/sec; 5.007 sec/batch)
2018-05-05 05:31:17.281499: step 63690, loss = 17.65 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 05:32:06.579686: step 63700, loss = 18.40 (3.2 examples/sec; 4.971 sec/batch)
2018-05-05 05:32:59.480841: step 63710, loss = 17.80 (3.2 examples/sec; 5.002 sec/batch)
2018-05-05 05:33:48.699185: step 63720, loss = 18.55 (3.2 examples/sec; 5.010 sec/batch)
2018-05-05 05:34:37.830931: step 63730, loss = 18.54 (3.3 examples/sec; 4.899 sec/batch)
2018-05-05 05:35:27.666541: step 63740, loss = 17.83 (3.2 examples/sec; 4.929 sec/batch)
2018-05-05 05:36:13.496642: step 63750, loss = 17.99 (3.3 examples/sec; 4.879 sec/batch)
2018-05-05 05:37:02.915054: step 63760, loss = 18.70 (3.2 examples/sec; 4.960 sec/batch)
2018-05-05 05:37:52.665165: step 63770, loss = 17.82 (3.2 examples/sec; 4.976 sec/batch)
2018-05-05 05:38:42.120822: step 63780, loss = 18.35 (3.3 examples/sec; 4.870 sec/batch)
2018-05-05 05:39:32.162465: step 63790, loss = 17.78 (3.1 examples/sec; 5.134 sec/batch)
2018-05-05 05:40:21.948269: step 63800, loss = 17.72 (3.2 examples/sec; 4.993 sec/batch)
2018-05-05 05:41:15.105522: step 63810, loss = 17.89 (3.2 examples/sec; 4.953 sec/batch)
2018-05-05 05:42:04.477752: step 63820, loss = 18.43 (3.2 examples/sec; 4.960 sec/batch)
2018-05-05 05:42:53.495300: step 63830, loss = 17.77 (3.4 examples/sec; 4.730 sec/batch)
2018-05-05 05:43:43.444183: step 63840, loss = 18.59 (3.2 examples/sec; 4.973 sec/batch)
2018-05-05 05:44:33.239588: step 63850, loss = 17.79 (3.2 examples/sec; 4.978 sec/batch)
2018-05-05 05:45:22.761163: step 63860, loss = 18.05 (3.2 examples/sec; 4.975 sec/batch)
2018-05-05 05:46:10.854692: step 63870, loss = 17.88 (4.1 examples/sec; 3.894 sec/batch)
2018-05-05 05:46:59.223080: step 63880, loss = 17.97 (3.3 examples/sec; 4.904 sec/batch)
2018-05-05 05:47:48.479947: step 63890, loss = 17.98 (3.2 examples/sec; 4.966 sec/batch)
2018-05-05 05:48:37.791935: step 63900, loss = 18.14 (3.2 examples/sec; 4.991 sec/batch)
2018-05-05 05:49:31.808676: step 63910, loss = 18.02 (3.2 examples/sec; 5.044 sec/batch)
2018-05-05 05:50:21.657002: step 63920, loss = 17.79 (3.2 examples/sec; 5.006 sec/batch)
2018-05-05 05:51:10.505845: step 63930, loss = 18.08 (3.2 examples/sec; 4.974 sec/batch)
2018-05-05 05:52:00.235702: step 63940, loss = 17.78 (3.4 examples/sec; 4.769 sec/batch)
2018-05-05 05:52:49.567677: step 63950, loss = 18.05 (3.3 examples/sec; 4.790 sec/batch)
2018-05-05 05:53:38.995222: step 63960, loss = 18.19 (3.3 examples/sec; 4.776 sec/batch)
2018-05-05 05:54:28.843186: step 63970, loss = 18.31 (3.2 examples/sec; 4.933 sec/batch)
2018-05-05 05:55:18.307456: step 63980, loss = 18.24 (3.3 examples/sec; 4.892 sec/batch)
2018-05-05 05:56:08.082539: step 63990, loss = 18.08 (3.2 examples/sec; 4.992 sec/batch)
2018-05-05 05:56:54.554326: step 64000, loss = 18.24 (3.2 examples/sec; 5.068 sec/batch)
2018-05-05 05:57:47.005981: step 64010, loss = 18.10 (3.3 examples/sec; 4.870 sec/batch)
2018-05-05 05:58:36.919831: step 64020, loss = 17.98 (3.2 examples/sec; 4.967 sec/batch)
2018-05-05 05:59:25.883363: step 64030, loss = 18.36 (3.2 examples/sec; 4.961 sec/batch)
2018-05-05 06:00:16.135400: step 64040, loss = 18.05 (3.2 examples/sec; 4.986 sec/batch)
2018-05-05 06:01:05.382990: step 64050, loss = 18.40 (3.2 examples/sec; 4.979 sec/batch)
2018-05-05 06:01:54.920186: step 64060, loss = 17.87 (3.3 examples/sec; 4.866 sec/batch)
2018-05-05 06:02:44.576178: step 64070, loss = 18.07 (3.2 examples/sec; 5.042 sec/batch)
2018-05-05 06:03:33.840998: step 64080, loss = 17.83 (3.2 examples/sec; 4.929 sec/batch)
2018-05-05 06:04:23.520446: step 64090, loss = 17.98 (3.2 examples/sec; 5.005 sec/batch)
2018-05-05 06:05:13.002584: step 64100, loss = 17.71 (3.2 examples/sec; 4.952 sec/batch)
2018-05-05 06:06:05.732027: step 64110, loss = 18.58 (3.3 examples/sec; 4.857 sec/batch)
2018-05-05 06:06:51.795896: step 64120, loss = 17.74 (3.4 examples/sec; 4.733 sec/batch)
2018-05-05 06:07:41.217806: step 64130, loss = 17.91 (3.2 examples/sec; 5.019 sec/batch)
2018-05-05 06:08:30.558113: step 64140, loss = 17.62 (3.2 examples/sec; 5.065 sec/batch)
2018-05-05 06:09:20.049702: step 64150, loss = 17.81 (3.3 examples/sec; 4.876 sec/batch)
2018-05-05 06:10:09.910339: step 64160, loss = 17.85 (3.2 examples/sec; 4.973 sec/batch)
2018-05-05 06:10:59.060552: step 64170, loss = 18.07 (3.3 examples/sec; 4.859 sec/batch)
2018-05-05 06:11:48.899918: step 64180, loss = 17.78 (3.2 examples/sec; 4.971 sec/batch)
2018-05-05 06:12:38.604114: step 64190, loss = 17.90 (3.2 examples/sec; 5.007 sec/batch)
2018-05-05 06:13:28.784818: step 64200, loss = 18.26 (3.1 examples/sec; 5.113 sec/batch)
2018-05-05 06:14:21.756794: step 64210, loss = 17.62 (3.2 examples/sec; 4.994 sec/batch)
2018-05-05 06:15:11.385160: step 64220, loss = 17.87 (3.3 examples/sec; 4.887 sec/batch)
2018-05-05 06:16:00.827709: step 64230, loss = 17.84 (3.3 examples/sec; 4.900 sec/batch)
2018-05-05 06:16:50.269778: step 64240, loss = 18.05 (3.3 examples/sec; 4.779 sec/batch)
2018-05-05 06:17:36.494568: step 64250, loss = 18.23 (3.1 examples/sec; 5.121 sec/batch)
2018-05-05 06:18:26.058621: step 64260, loss = 18.21 (3.3 examples/sec; 4.902 sec/batch)
2018-05-05 06:19:15.241414: step 64270, loss = 17.97 (3.2 examples/sec; 5.014 sec/batch)
2018-05-05 06:20:04.336652: step 64280, loss = 18.92 (3.2 examples/sec; 4.940 sec/batch)
2018-05-05 06:20:53.867069: step 64290, loss = 17.88 (3.2 examples/sec; 4.991 sec/batch)
2018-05-05 06:21:44.288105: step 64300, loss = 18.18 (3.2 examples/sec; 5.007 sec/batch)
2018-05-05 06:22:37.183701: step 64310, loss = 18.66 (3.4 examples/sec; 4.748 sec/batch)
2018-05-05 06:23:26.408718: step 64320, loss = 17.92 (3.2 examples/sec; 5.047 sec/batch)
2018-05-05 06:24:16.107186: step 64330, loss = 17.91 (3.1 examples/sec; 5.114 sec/batch)
2018-05-05 06:25:05.800917: step 64340, loss = 18.15 (3.1 examples/sec; 5.145 sec/batch)
2018-05-05 06:25:55.607844: step 64350, loss = 18.12 (3.3 examples/sec; 4.807 sec/batch)
2018-05-05 06:26:45.113884: step 64360, loss = 17.84 (3.3 examples/sec; 4.880 sec/batch)
2018-05-05 06:27:31.620422: step 64370, loss = 18.64 (3.2 examples/sec; 4.989 sec/batch)
2018-05-05 06:28:21.092014: step 64380, loss = 17.66 (3.3 examples/sec; 4.784 sec/batch)
2018-05-05 06:29:09.973118: step 64390, loss = 18.05 (3.2 examples/sec; 4.970 sec/batch)
2018-05-05 06:29:58.993015: step 64400, loss = 18.14 (3.2 examples/sec; 4.987 sec/batch)
2018-05-05 06:30:52.189174: step 64410, loss = 17.93 (3.3 examples/sec; 4.811 sec/batch)
2018-05-05 06:31:42.245519: step 64420, loss = 18.18 (3.1 examples/sec; 5.086 sec/batch)
2018-05-05 06:32:31.898233: step 64430, loss = 18.09 (3.2 examples/sec; 5.001 sec/batch)
2018-05-05 06:33:21.172282: step 64440, loss = 17.89 (3.2 examples/sec; 5.014 sec/batch)
2018-05-05 06:34:10.410574: step 64450, loss = 18.56 (3.2 examples/sec; 5.050 sec/batch)
2018-05-05 06:35:00.757454: step 64460, loss = 18.36 (3.1 examples/sec; 5.089 sec/batch)
2018-05-05 06:35:50.350775: step 64470, loss = 17.75 (3.2 examples/sec; 5.005 sec/batch)
2018-05-05 06:36:40.768662: step 64480, loss = 18.51 (3.2 examples/sec; 5.053 sec/batch)
2018-05-05 06:37:31.059098: step 64490, loss = 18.53 (3.2 examples/sec; 4.952 sec/batch)
2018-05-05 06:38:17.014007: step 64500, loss = 18.28 (3.3 examples/sec; 4.918 sec/batch)
2018-05-05 06:39:10.005828: step 64510, loss = 18.58 (3.2 examples/sec; 5.078 sec/batch)
2018-05-05 06:39:58.988900: step 64520, loss = 17.88 (3.3 examples/sec; 4.818 sec/batch)
2018-05-05 06:40:48.372419: step 64530, loss = 17.88 (3.2 examples/sec; 5.003 sec/batch)
2018-05-05 06:41:40.882397: step 64540, loss = 17.72 (3.2 examples/sec; 4.999 sec/batch)
2018-05-05 06:42:30.801682: step 64550, loss = 17.66 (3.3 examples/sec; 4.888 sec/batch)
2018-05-05 06:43:20.221033: step 64560, loss = 17.71 (3.2 examples/sec; 5.027 sec/batch)
2018-05-05 06:44:10.236143: step 64570, loss = 17.93 (3.2 examples/sec; 5.015 sec/batch)
2018-05-05 06:45:00.077616: step 64580, loss = 17.83 (3.2 examples/sec; 4.981 sec/batch)
2018-05-05 06:45:49.524735: step 64590, loss = 17.91 (3.2 examples/sec; 4.967 sec/batch)
2018-05-05 06:46:38.655491: step 64600, loss = 18.93 (3.2 examples/sec; 4.969 sec/batch)
2018-05-05 06:47:32.194868: step 64610, loss = 18.16 (3.1 examples/sec; 5.092 sec/batch)
2018-05-05 06:48:18.222663: step 64620, loss = 17.79 (3.2 examples/sec; 4.985 sec/batch)
2018-05-05 06:49:07.419952: step 64630, loss = 18.19 (3.2 examples/sec; 5.029 sec/batch)
2018-05-05 06:49:57.464216: step 64640, loss = 17.71 (3.3 examples/sec; 4.906 sec/batch)
2018-05-05 06:50:47.427229: step 64650, loss = 17.99 (2.8 examples/sec; 5.746 sec/batch)
2018-05-05 06:51:37.528807: step 64660, loss = 18.43 (3.3 examples/sec; 4.801 sec/batch)
2018-05-05 06:52:26.711249: step 64670, loss = 17.89 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 06:53:16.091996: step 64680, loss = 17.98 (3.2 examples/sec; 4.926 sec/batch)
2018-05-05 06:54:05.286207: step 64690, loss = 18.29 (3.3 examples/sec; 4.917 sec/batch)
2018-05-05 06:54:54.590721: step 64700, loss = 18.07 (3.3 examples/sec; 4.846 sec/batch)
2018-05-05 06:55:47.519392: step 64710, loss = 18.18 (3.3 examples/sec; 4.871 sec/batch)
2018-05-05 06:56:36.649178: step 64720, loss = 17.96 (3.3 examples/sec; 4.884 sec/batch)
2018-05-05 06:57:26.613153: step 64730, loss = 18.35 (3.3 examples/sec; 4.916 sec/batch)
2018-05-05 06:58:15.971326: step 64740, loss = 18.22 (3.2 examples/sec; 4.960 sec/batch)
2018-05-05 06:59:01.790643: step 64750, loss = 17.90 (3.3 examples/sec; 4.882 sec/batch)
2018-05-05 06:59:51.304535: step 64760, loss = 17.87 (3.2 examples/sec; 4.926 sec/batch)
2018-05-05 07:00:40.572432: step 64770, loss = 17.92 (3.2 examples/sec; 5.005 sec/batch)
2018-05-05 07:01:30.092445: step 64780, loss = 18.00 (3.3 examples/sec; 4.872 sec/batch)
2018-05-05 07:02:19.653112: step 64790, loss = 18.04 (3.2 examples/sec; 5.048 sec/batch)
2018-05-05 07:03:09.316583: step 64800, loss = 17.74 (3.3 examples/sec; 4.891 sec/batch)
2018-05-05 07:04:03.070932: step 64810, loss = 17.99 (3.1 examples/sec; 5.092 sec/batch)
2018-05-05 07:04:52.492455: step 64820, loss = 18.55 (3.0 examples/sec; 5.420 sec/batch)
2018-05-05 07:05:42.239574: step 64830, loss = 18.10 (3.2 examples/sec; 4.963 sec/batch)
2018-05-05 07:06:31.781829: step 64840, loss = 17.86 (3.3 examples/sec; 4.909 sec/batch)
2018-05-05 07:07:20.962754: step 64850, loss = 17.83 (3.5 examples/sec; 4.550 sec/batch)
2018-05-05 07:08:10.046954: step 64860, loss = 17.86 (3.2 examples/sec; 4.944 sec/batch)
2018-05-05 07:08:55.618191: step 64870, loss = 18.01 (4.2 examples/sec; 3.855 sec/batch)
2018-05-05 07:09:45.134194: step 64880, loss = 18.00 (3.4 examples/sec; 4.713 sec/batch)
2018-05-05 07:10:34.594435: step 64890, loss = 18.22 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 07:11:24.951537: step 64900, loss = 17.71 (3.1 examples/sec; 5.088 sec/batch)
2018-05-05 07:12:17.646789: step 64910, loss = 18.25 (3.2 examples/sec; 4.938 sec/batch)
2018-05-05 07:13:07.563528: step 64920, loss = 18.64 (3.3 examples/sec; 4.785 sec/batch)
2018-05-05 07:13:57.111054: step 64930, loss = 18.00 (3.2 examples/sec; 4.974 sec/batch)
2018-05-05 07:14:47.251106: step 64940, loss = 18.07 (3.2 examples/sec; 5.050 sec/batch)
2018-05-05 07:15:36.745907: step 64950, loss = 18.18 (3.3 examples/sec; 4.814 sec/batch)
2018-05-05 07:16:26.670260: step 64960, loss = 17.88 (3.2 examples/sec; 4.938 sec/batch)
2018-05-05 07:17:16.432998: step 64970, loss = 17.73 (3.2 examples/sec; 5.020 sec/batch)
2018-05-05 07:18:06.286631: step 64980, loss = 17.90 (3.2 examples/sec; 5.016 sec/batch)
2018-05-05 07:18:56.097054: step 64990, loss = 18.32 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 07:19:42.084361: step 65000, loss = 18.08 (3.2 examples/sec; 5.047 sec/batch)
2018-05-05 07:20:34.859288: step 65010, loss = 17.86 (3.2 examples/sec; 4.982 sec/batch)
2018-05-05 07:21:24.356486: step 65020, loss = 18.07 (3.2 examples/sec; 4.925 sec/batch)
2018-05-05 07:22:14.138234: step 65030, loss = 17.64 (3.1 examples/sec; 5.096 sec/batch)
2018-05-05 07:23:04.112657: step 65040, loss = 17.83 (3.1 examples/sec; 5.086 sec/batch)
2018-05-05 07:23:53.376179: step 65050, loss = 17.78 (3.3 examples/sec; 4.907 sec/batch)
2018-05-05 07:24:42.404881: step 65060, loss = 18.29 (3.3 examples/sec; 4.896 sec/batch)
2018-05-05 07:25:32.488992: step 65070, loss = 18.09 (3.2 examples/sec; 5.052 sec/batch)
2018-05-05 07:26:21.794058: step 65080, loss = 18.70 (3.2 examples/sec; 5.073 sec/batch)
2018-05-05 07:27:11.537403: step 65090, loss = 17.96 (3.2 examples/sec; 4.931 sec/batch)
2018-05-05 07:28:01.004482: step 65100, loss = 17.93 (3.2 examples/sec; 5.012 sec/batch)
2018-05-05 07:28:53.528189: step 65110, loss = 18.02 (3.3 examples/sec; 4.802 sec/batch)
2018-05-05 07:29:39.404835: step 65120, loss = 18.56 (3.2 examples/sec; 4.970 sec/batch)
2018-05-05 07:30:28.473607: step 65130, loss = 17.57 (3.2 examples/sec; 5.051 sec/batch)
2018-05-05 07:31:17.629335: step 65140, loss = 18.05 (3.3 examples/sec; 4.777 sec/batch)
2018-05-05 07:32:07.040741: step 65150, loss = 17.76 (3.3 examples/sec; 4.879 sec/batch)
2018-05-05 07:32:56.014409: step 65160, loss = 18.11 (3.3 examples/sec; 4.895 sec/batch)
2018-05-05 07:33:44.857010: step 65170, loss = 17.73 (3.3 examples/sec; 4.871 sec/batch)
2018-05-05 07:34:34.124410: step 65180, loss = 18.26 (3.2 examples/sec; 4.931 sec/batch)
2018-05-05 07:35:22.535665: step 65190, loss = 18.17 (3.3 examples/sec; 4.812 sec/batch)
2018-05-05 07:36:12.086613: step 65200, loss = 18.56 (3.3 examples/sec; 4.869 sec/batch)
2018-05-05 07:37:05.075838: step 65210, loss = 17.67 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 07:37:54.184356: step 65220, loss = 17.98 (3.3 examples/sec; 4.829 sec/batch)
2018-05-05 07:38:43.559629: step 65230, loss = 17.70 (3.3 examples/sec; 4.912 sec/batch)
2018-05-05 07:39:32.663071: step 65240, loss = 18.65 (3.2 examples/sec; 5.033 sec/batch)
2018-05-05 07:40:18.697031: step 65250, loss = 17.90 (3.2 examples/sec; 5.009 sec/batch)
2018-05-05 07:41:07.941597: step 65260, loss = 18.58 (3.2 examples/sec; 5.002 sec/batch)
2018-05-05 07:41:56.920612: step 65270, loss = 18.02 (3.2 examples/sec; 5.039 sec/batch)
2018-05-05 07:42:46.528392: step 65280, loss = 18.06 (3.3 examples/sec; 4.904 sec/batch)
2018-05-05 07:43:35.725792: step 65290, loss = 18.07 (3.2 examples/sec; 4.937 sec/batch)
2018-05-05 07:44:25.380394: step 65300, loss = 18.15 (3.4 examples/sec; 4.774 sec/batch)
2018-05-05 07:45:18.642001: step 65310, loss = 18.06 (3.3 examples/sec; 4.841 sec/batch)
2018-05-05 07:46:07.946577: step 65320, loss = 18.02 (3.3 examples/sec; 4.913 sec/batch)
2018-05-05 07:46:57.347630: step 65330, loss = 18.15 (3.3 examples/sec; 4.841 sec/batch)
2018-05-05 07:47:47.241920: step 65340, loss = 17.60 (3.4 examples/sec; 4.758 sec/batch)
2018-05-05 07:48:36.876608: step 65350, loss = 17.87 (3.2 examples/sec; 4.939 sec/batch)
2018-05-05 07:49:26.512531: step 65360, loss = 18.44 (3.2 examples/sec; 5.001 sec/batch)
2018-05-05 07:50:12.541495: step 65370, loss = 17.88 (3.3 examples/sec; 4.808 sec/batch)
2018-05-05 07:51:01.999377: step 65380, loss = 17.97 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 07:51:51.338928: step 65390, loss = 18.15 (3.3 examples/sec; 4.869 sec/batch)
2018-05-05 07:52:40.935571: step 65400, loss = 18.31 (3.2 examples/sec; 4.944 sec/batch)
2018-05-05 07:53:34.342256: step 65410, loss = 18.03 (3.1 examples/sec; 5.095 sec/batch)
2018-05-05 07:54:24.114784: step 65420, loss = 17.87 (3.2 examples/sec; 5.059 sec/batch)
2018-05-05 07:55:13.395432: step 65430, loss = 18.14 (3.3 examples/sec; 4.835 sec/batch)
2018-05-05 07:56:02.784693: step 65440, loss = 18.02 (3.3 examples/sec; 4.912 sec/batch)
2018-05-05 07:56:51.884188: step 65450, loss = 17.98 (3.2 examples/sec; 5.018 sec/batch)
2018-05-05 07:57:41.446992: step 65460, loss = 17.68 (3.1 examples/sec; 5.124 sec/batch)
2018-05-05 07:58:30.950563: step 65470, loss = 17.69 (3.2 examples/sec; 5.007 sec/batch)
2018-05-05 07:59:19.872992: step 65480, loss = 18.00 (3.4 examples/sec; 4.757 sec/batch)
2018-05-05 08:00:09.072067: step 65490, loss = 18.30 (3.3 examples/sec; 4.875 sec/batch)
2018-05-05 08:00:54.609379: step 65500, loss = 17.83 (3.3 examples/sec; 4.793 sec/batch)
2018-05-05 08:01:48.021645: step 65510, loss = 18.66 (3.3 examples/sec; 4.898 sec/batch)
2018-05-05 08:02:37.735779: step 65520, loss = 19.67 (3.5 examples/sec; 4.570 sec/batch)
2018-05-05 08:03:27.499564: step 65530, loss = 18.08 (3.1 examples/sec; 5.105 sec/batch)
2018-05-05 08:04:17.482713: step 65540, loss = 17.93 (3.2 examples/sec; 5.040 sec/batch)
2018-05-05 08:05:06.536900: step 65550, loss = 18.11 (3.4 examples/sec; 4.762 sec/batch)
2018-05-05 08:05:55.790813: step 65560, loss = 18.17 (3.3 examples/sec; 4.911 sec/batch)
2018-05-05 08:06:45.310762: step 65570, loss = 18.17 (3.2 examples/sec; 4.969 sec/batch)
2018-05-05 08:07:34.728602: step 65580, loss = 18.62 (3.3 examples/sec; 4.792 sec/batch)
2018-05-05 08:08:24.648504: step 65590, loss = 17.63 (3.2 examples/sec; 4.948 sec/batch)
2018-05-05 08:09:14.049635: step 65600, loss = 18.16 (3.2 examples/sec; 5.029 sec/batch)
2018-05-05 08:10:07.091722: step 65610, loss = 17.90 (3.1 examples/sec; 5.102 sec/batch)
2018-05-05 08:10:53.335395: step 65620, loss = 18.05 (3.2 examples/sec; 5.050 sec/batch)
2018-05-05 08:11:42.277248: step 65630, loss = 17.96 (3.2 examples/sec; 4.927 sec/batch)
2018-05-05 08:12:31.686838: step 65640, loss = 18.02 (3.3 examples/sec; 4.890 sec/batch)
2018-05-05 08:13:20.474685: step 65650, loss = 17.80 (3.4 examples/sec; 4.740 sec/batch)
2018-05-05 08:14:09.800777: step 65660, loss = 18.05 (3.2 examples/sec; 4.963 sec/batch)
2018-05-05 08:14:59.299673: step 65670, loss = 17.85 (3.2 examples/sec; 5.054 sec/batch)
2018-05-05 08:15:48.441620: step 65680, loss = 17.99 (3.3 examples/sec; 4.897 sec/batch)
2018-05-05 08:16:37.406381: step 65690, loss = 17.84 (3.3 examples/sec; 4.914 sec/batch)
2018-05-05 08:17:26.715665: step 65700, loss = 17.97 (3.2 examples/sec; 5.028 sec/batch)
2018-05-05 08:18:19.521430: step 65710, loss = 17.86 (3.3 examples/sec; 4.918 sec/batch)
2018-05-05 08:19:09.764762: step 65720, loss = 17.72 (3.1 examples/sec; 5.177 sec/batch)
2018-05-05 08:19:58.952403: step 65730, loss = 18.36 (3.3 examples/sec; 4.847 sec/batch)
2018-05-05 08:20:48.539990: step 65740, loss = 17.94 (3.1 examples/sec; 5.216 sec/batch)
2018-05-05 08:21:34.230895: step 65750, loss = 18.24 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 08:22:24.135353: step 65760, loss = 18.25 (3.2 examples/sec; 4.963 sec/batch)
2018-05-05 08:23:13.395577: step 65770, loss = 18.12 (3.3 examples/sec; 4.920 sec/batch)
2018-05-05 08:24:02.829408: step 65780, loss = 18.14 (3.3 examples/sec; 4.890 sec/batch)
2018-05-05 08:24:51.966975: step 65790, loss = 17.84 (3.3 examples/sec; 4.787 sec/batch)
2018-05-05 08:25:40.998276: step 65800, loss = 18.04 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 08:26:34.419295: step 65810, loss = 18.11 (3.3 examples/sec; 4.875 sec/batch)
2018-05-05 08:27:24.320858: step 65820, loss = 18.59 (3.2 examples/sec; 5.046 sec/batch)
2018-05-05 08:28:13.351860: step 65830, loss = 18.23 (3.2 examples/sec; 4.942 sec/batch)
2018-05-05 08:29:03.139728: step 65840, loss = 18.50 (3.2 examples/sec; 4.931 sec/batch)
2018-05-05 08:29:52.275214: step 65850, loss = 18.01 (3.2 examples/sec; 4.976 sec/batch)
2018-05-05 08:30:41.877192: step 65860, loss = 18.15 (3.1 examples/sec; 5.191 sec/batch)
2018-05-05 08:31:28.109788: step 65870, loss = 17.84 (4.1 examples/sec; 3.856 sec/batch)
2018-05-05 08:32:18.354291: step 65880, loss = 18.18 (3.2 examples/sec; 5.035 sec/batch)
2018-05-05 08:33:07.434005: step 65890, loss = 18.28 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 08:33:56.320596: step 65900, loss = 17.75 (3.2 examples/sec; 5.029 sec/batch)
2018-05-05 08:34:49.287175: step 65910, loss = 18.50 (3.1 examples/sec; 5.097 sec/batch)
2018-05-05 08:35:39.401011: step 65920, loss = 18.37 (3.2 examples/sec; 5.021 sec/batch)
2018-05-05 08:36:28.704834: step 65930, loss = 17.94 (3.3 examples/sec; 4.891 sec/batch)
2018-05-05 08:37:18.395324: step 65940, loss = 18.24 (3.2 examples/sec; 5.055 sec/batch)
2018-05-05 08:38:08.389463: step 65950, loss = 17.88 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 08:38:57.554377: step 65960, loss = 18.20 (3.3 examples/sec; 4.894 sec/batch)
2018-05-05 08:39:47.193465: step 65970, loss = 17.88 (3.1 examples/sec; 5.099 sec/batch)
2018-05-05 08:40:36.486621: step 65980, loss = 18.55 (3.3 examples/sec; 4.863 sec/batch)
2018-05-05 08:41:25.719892: step 65990, loss = 18.68 (3.2 examples/sec; 4.927 sec/batch)
2018-05-05 08:42:11.532804: step 66000, loss = 18.25 (3.3 examples/sec; 4.807 sec/batch)
2018-05-05 08:43:04.343483: step 66010, loss = 18.27 (3.2 examples/sec; 5.016 sec/batch)
2018-05-05 08:43:53.699271: step 66020, loss = 18.23 (3.2 examples/sec; 4.943 sec/batch)
2018-05-05 08:44:44.528198: step 66030, loss = 17.85 (3.2 examples/sec; 4.991 sec/batch)
2018-05-05 08:45:33.945149: step 66040, loss = 17.91 (3.1 examples/sec; 5.142 sec/batch)
2018-05-05 08:46:23.629039: step 66050, loss = 18.26 (3.2 examples/sec; 5.011 sec/batch)
2018-05-05 08:47:12.972941: step 66060, loss = 17.80 (3.1 examples/sec; 5.211 sec/batch)
2018-05-05 08:48:03.416138: step 66070, loss = 18.06 (3.1 examples/sec; 5.132 sec/batch)
2018-05-05 08:48:52.918244: step 66080, loss = 17.63 (3.3 examples/sec; 4.851 sec/batch)
2018-05-05 08:49:42.932257: step 66090, loss = 17.99 (3.1 examples/sec; 5.193 sec/batch)
2018-05-05 08:50:32.602281: step 66100, loss = 18.45 (3.2 examples/sec; 4.968 sec/batch)
2018-05-05 08:51:26.147002: step 66110, loss = 17.78 (3.2 examples/sec; 4.945 sec/batch)
2018-05-05 08:52:12.255242: step 66120, loss = 17.97 (3.4 examples/sec; 4.650 sec/batch)
2018-05-05 08:53:01.927307: step 66130, loss = 18.23 (3.2 examples/sec; 4.977 sec/batch)
2018-05-05 08:53:51.115471: step 66140, loss = 17.62 (3.3 examples/sec; 4.840 sec/batch)
2018-05-05 08:54:40.406091: step 66150, loss = 18.29 (3.3 examples/sec; 4.778 sec/batch)
2018-05-05 08:55:28.939728: step 66160, loss = 18.36 (3.3 examples/sec; 4.899 sec/batch)
2018-05-05 08:56:18.028598: step 66170, loss = 18.65 (3.3 examples/sec; 4.793 sec/batch)
2018-05-05 08:57:07.142694: step 66180, loss = 17.80 (3.3 examples/sec; 4.845 sec/batch)
2018-05-05 08:57:56.196351: step 66190, loss = 18.33 (3.2 examples/sec; 5.006 sec/batch)
2018-05-05 08:58:45.539911: step 66200, loss = 18.00 (3.2 examples/sec; 4.945 sec/batch)
2018-05-05 08:59:38.519836: step 66210, loss = 17.92 (3.3 examples/sec; 4.838 sec/batch)
2018-05-05 09:00:27.625239: step 66220, loss = 18.66 (3.1 examples/sec; 5.107 sec/batch)
2018-05-05 09:01:16.971245: step 66230, loss = 18.52 (3.3 examples/sec; 4.830 sec/batch)
2018-05-05 09:02:06.249187: step 66240, loss = 17.92 (3.3 examples/sec; 4.920 sec/batch)
2018-05-05 09:02:51.664983: step 66250, loss = 18.34 (3.2 examples/sec; 4.952 sec/batch)
2018-05-05 09:03:40.919730: step 66260, loss = 18.14 (3.1 examples/sec; 5.173 sec/batch)
2018-05-05 09:04:30.560844: step 66270, loss = 18.01 (3.1 examples/sec; 5.119 sec/batch)
2018-05-05 09:05:20.511615: step 66280, loss = 17.70 (3.2 examples/sec; 5.046 sec/batch)
2018-05-05 09:06:09.797525: step 66290, loss = 18.67 (3.2 examples/sec; 4.929 sec/batch)
2018-05-05 09:06:59.677658: step 66300, loss = 17.88 (3.2 examples/sec; 4.965 sec/batch)
2018-05-05 09:07:52.785712: step 66310, loss = 17.87 (3.2 examples/sec; 5.011 sec/batch)
2018-05-05 09:08:42.175905: step 66320, loss = 17.84 (3.2 examples/sec; 5.033 sec/batch)
2018-05-05 09:09:31.978117: step 66330, loss = 17.92 (3.2 examples/sec; 4.961 sec/batch)
2018-05-05 09:10:21.437740: step 66340, loss = 17.92 (3.2 examples/sec; 5.047 sec/batch)
2018-05-05 09:11:10.891579: step 66350, loss = 18.16 (3.2 examples/sec; 4.931 sec/batch)
2018-05-05 09:12:00.729480: step 66360, loss = 18.22 (3.2 examples/sec; 4.942 sec/batch)
2018-05-05 09:12:46.759586: step 66370, loss = 18.00 (3.4 examples/sec; 4.646 sec/batch)
2018-05-05 09:13:36.408197: step 66380, loss = 18.50 (3.3 examples/sec; 4.886 sec/batch)
2018-05-05 09:14:26.506782: step 66390, loss = 17.55 (3.2 examples/sec; 4.938 sec/batch)
2018-05-05 09:15:15.304748: step 66400, loss = 18.55 (3.2 examples/sec; 4.944 sec/batch)
2018-05-05 09:16:08.019897: step 66410, loss = 17.96 (3.3 examples/sec; 4.853 sec/batch)
2018-05-05 09:16:57.126591: step 66420, loss = 17.90 (3.3 examples/sec; 4.920 sec/batch)
2018-05-05 09:17:46.434050: step 66430, loss = 17.75 (3.1 examples/sec; 5.104 sec/batch)
2018-05-05 09:18:36.104613: step 66440, loss = 17.78 (3.3 examples/sec; 4.899 sec/batch)
2018-05-05 09:19:25.475200: step 66450, loss = 18.45 (3.1 examples/sec; 5.090 sec/batch)
2018-05-05 09:20:15.085276: step 66460, loss = 18.12 (3.1 examples/sec; 5.082 sec/batch)
2018-05-05 09:21:04.655963: step 66470, loss = 18.63 (3.2 examples/sec; 5.065 sec/batch)
2018-05-05 09:21:54.322829: step 66480, loss = 18.50 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 09:22:43.607688: step 66490, loss = 18.20 (3.2 examples/sec; 4.966 sec/batch)
2018-05-05 09:23:29.490955: step 66500, loss = 18.51 (3.3 examples/sec; 4.796 sec/batch)
2018-05-05 09:24:22.547233: step 66510, loss = 17.65 (3.3 examples/sec; 4.894 sec/batch)
2018-05-05 09:25:11.579564: step 66520, loss = 17.96 (3.2 examples/sec; 4.990 sec/batch)
2018-05-05 09:26:01.011504: step 66530, loss = 18.24 (3.3 examples/sec; 4.875 sec/batch)
2018-05-05 09:26:50.289921: step 66540, loss = 17.79 (3.2 examples/sec; 5.038 sec/batch)
2018-05-05 09:27:39.104745: step 66550, loss = 18.05 (3.2 examples/sec; 4.939 sec/batch)
2018-05-05 09:28:29.481984: step 66560, loss = 17.88 (3.3 examples/sec; 4.896 sec/batch)
2018-05-05 09:29:19.161600: step 66570, loss = 17.81 (3.2 examples/sec; 4.993 sec/batch)
2018-05-05 09:30:08.565650: step 66580, loss = 18.39 (3.2 examples/sec; 4.941 sec/batch)
2018-05-05 09:30:57.825850: step 66590, loss = 18.01 (3.3 examples/sec; 4.913 sec/batch)
2018-05-05 09:31:46.912303: step 66600, loss = 17.71 (3.2 examples/sec; 4.929 sec/batch)
2018-05-05 09:32:39.922375: step 66610, loss = 18.19 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 09:33:25.557842: step 66620, loss = 18.52 (4.1 examples/sec; 3.860 sec/batch)
2018-05-05 09:34:14.905104: step 66630, loss = 18.00 (3.2 examples/sec; 4.933 sec/batch)
2018-05-05 09:35:03.768101: step 66640, loss = 17.73 (3.2 examples/sec; 4.925 sec/batch)
2018-05-05 09:35:52.931104: step 66650, loss = 17.78 (3.2 examples/sec; 5.054 sec/batch)
2018-05-05 09:36:41.729922: step 66660, loss = 18.03 (3.4 examples/sec; 4.736 sec/batch)
2018-05-05 09:37:30.926252: step 66670, loss = 17.95 (3.2 examples/sec; 4.994 sec/batch)
2018-05-05 09:38:20.027977: step 66680, loss = 18.04 (3.3 examples/sec; 4.909 sec/batch)
2018-05-05 09:39:09.376727: step 66690, loss = 17.93 (3.2 examples/sec; 4.951 sec/batch)
2018-05-05 09:39:59.033600: step 66700, loss = 18.15 (3.1 examples/sec; 5.087 sec/batch)
2018-05-05 09:40:52.158538: step 66710, loss = 18.17 (3.3 examples/sec; 4.803 sec/batch)
2018-05-05 09:41:41.151716: step 66720, loss = 18.11 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 09:42:30.267648: step 66730, loss = 17.79 (3.2 examples/sec; 4.965 sec/batch)
2018-05-05 09:43:19.534652: step 66740, loss = 17.85 (3.4 examples/sec; 4.732 sec/batch)
2018-05-05 09:44:05.503796: step 66750, loss = 18.16 (3.1 examples/sec; 5.096 sec/batch)
2018-05-05 09:44:54.194953: step 66760, loss = 17.86 (3.3 examples/sec; 4.886 sec/batch)
2018-05-05 09:45:43.155891: step 66770, loss = 18.14 (3.2 examples/sec; 4.943 sec/batch)
2018-05-05 09:46:32.402094: step 66780, loss = 18.71 (3.2 examples/sec; 5.051 sec/batch)
2018-05-05 09:47:22.414113: step 66790, loss = 18.50 (3.2 examples/sec; 5.033 sec/batch)
2018-05-05 09:48:11.743438: step 66800, loss = 18.38 (3.2 examples/sec; 4.997 sec/batch)
2018-05-05 09:49:05.019434: step 66810, loss = 18.02 (3.2 examples/sec; 5.021 sec/batch)
2018-05-05 09:49:54.738318: step 66820, loss = 18.10 (3.2 examples/sec; 4.985 sec/batch)
2018-05-05 09:50:44.805010: step 66830, loss = 17.79 (3.2 examples/sec; 4.937 sec/batch)
2018-05-05 09:51:33.748358: step 66840, loss = 18.39 (3.3 examples/sec; 4.853 sec/batch)
2018-05-05 09:52:23.329729: step 66850, loss = 17.65 (3.2 examples/sec; 5.001 sec/batch)
2018-05-05 09:53:12.812248: step 66860, loss = 18.03 (3.1 examples/sec; 5.139 sec/batch)
2018-05-05 09:54:00.991115: step 66870, loss = 18.32 (4.2 examples/sec; 3.796 sec/batch)
2018-05-05 09:54:47.920864: step 66880, loss = 18.02 (3.2 examples/sec; 4.991 sec/batch)
2018-05-05 09:55:37.504624: step 66890, loss = 17.86 (3.2 examples/sec; 4.973 sec/batch)
2018-05-05 09:56:27.213108: step 66900, loss = 18.33 (3.1 examples/sec; 5.133 sec/batch)
2018-05-05 09:57:20.113684: step 66910, loss = 17.89 (3.2 examples/sec; 4.951 sec/batch)
2018-05-05 09:58:09.516653: step 66920, loss = 18.13 (3.2 examples/sec; 4.957 sec/batch)
2018-05-05 09:58:59.046307: step 66930, loss = 17.85 (3.3 examples/sec; 4.895 sec/batch)
2018-05-05 09:59:48.879733: step 66940, loss = 18.41 (3.2 examples/sec; 5.037 sec/batch)
2018-05-05 10:00:37.982725: step 66950, loss = 18.28 (3.1 examples/sec; 5.147 sec/batch)
2018-05-05 10:01:26.434560: step 66960, loss = 18.58 (3.3 examples/sec; 4.792 sec/batch)
2018-05-05 10:02:15.850666: step 66970, loss = 17.82 (3.2 examples/sec; 4.982 sec/batch)
2018-05-05 10:03:04.922382: step 66980, loss = 18.12 (3.2 examples/sec; 4.938 sec/batch)
2018-05-05 10:03:54.366271: step 66990, loss = 18.39 (3.2 examples/sec; 4.982 sec/batch)
2018-05-05 10:04:40.244526: step 67000, loss = 18.70 (3.4 examples/sec; 4.649 sec/batch)
2018-05-05 10:05:32.950486: step 67010, loss = 18.43 (3.5 examples/sec; 4.616 sec/batch)
2018-05-05 10:06:21.821071: step 67020, loss = 18.29 (3.3 examples/sec; 4.810 sec/batch)
2018-05-05 10:07:11.360431: step 67030, loss = 18.31 (3.1 examples/sec; 5.138 sec/batch)
2018-05-05 10:08:00.963879: step 67040, loss = 18.00 (3.3 examples/sec; 4.892 sec/batch)
2018-05-05 10:08:50.550374: step 67050, loss = 18.23 (3.2 examples/sec; 4.934 sec/batch)
2018-05-05 10:09:40.075363: step 67060, loss = 18.18 (3.1 examples/sec; 5.118 sec/batch)
2018-05-05 10:10:28.936120: step 67070, loss = 18.28 (3.3 examples/sec; 4.891 sec/batch)
2018-05-05 10:11:18.228939: step 67080, loss = 17.68 (3.2 examples/sec; 5.018 sec/batch)
2018-05-05 10:12:08.073553: step 67090, loss = 18.16 (3.3 examples/sec; 4.896 sec/batch)
2018-05-05 10:12:57.715603: step 67100, loss = 18.21 (3.1 examples/sec; 5.094 sec/batch)
2018-05-05 10:13:50.808030: step 67110, loss = 18.17 (3.2 examples/sec; 4.972 sec/batch)
2018-05-05 10:14:39.126699: step 67120, loss = 18.02 (4.1 examples/sec; 3.907 sec/batch)
2018-05-05 10:15:26.116418: step 67130, loss = 18.12 (3.3 examples/sec; 4.850 sec/batch)
2018-05-05 10:16:15.327126: step 67140, loss = 18.30 (3.2 examples/sec; 4.998 sec/batch)
2018-05-05 10:17:04.595355: step 67150, loss = 17.70 (3.2 examples/sec; 5.051 sec/batch)
2018-05-05 10:17:54.301422: step 67160, loss = 18.11 (3.2 examples/sec; 4.985 sec/batch)
2018-05-05 10:18:44.336958: step 67170, loss = 17.98 (3.1 examples/sec; 5.149 sec/batch)
2018-05-05 10:19:33.504204: step 67180, loss = 18.49 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 10:20:22.774818: step 67190, loss = 17.98 (3.2 examples/sec; 4.958 sec/batch)
2018-05-05 10:21:11.557433: step 67200, loss = 17.56 (3.4 examples/sec; 4.762 sec/batch)
2018-05-05 10:22:04.501503: step 67210, loss = 17.78 (3.3 examples/sec; 4.811 sec/batch)
2018-05-05 10:22:53.614253: step 67220, loss = 17.64 (3.3 examples/sec; 4.917 sec/batch)
2018-05-05 10:23:42.871215: step 67230, loss = 18.20 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 10:24:32.597000: step 67240, loss = 17.74 (3.3 examples/sec; 4.847 sec/batch)
2018-05-05 10:25:18.512155: step 67250, loss = 18.33 (3.3 examples/sec; 4.863 sec/batch)
2018-05-05 10:26:07.510244: step 67260, loss = 17.80 (3.3 examples/sec; 4.863 sec/batch)
2018-05-05 10:26:56.759270: step 67270, loss = 17.88 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 10:27:46.426532: step 67280, loss = 17.91 (3.2 examples/sec; 5.058 sec/batch)
2018-05-05 10:28:35.794075: step 67290, loss = 18.17 (3.3 examples/sec; 4.846 sec/batch)
2018-05-05 10:29:25.108634: step 67300, loss = 18.16 (3.2 examples/sec; 4.972 sec/batch)
2018-05-05 10:30:17.189641: step 67310, loss = 17.71 (3.4 examples/sec; 4.711 sec/batch)
2018-05-05 10:31:06.521175: step 67320, loss = 18.03 (3.3 examples/sec; 4.914 sec/batch)
2018-05-05 10:31:55.403344: step 67330, loss = 18.61 (3.3 examples/sec; 4.790 sec/batch)
2018-05-05 10:32:44.675985: step 67340, loss = 17.89 (3.2 examples/sec; 5.039 sec/batch)
2018-05-05 10:33:33.801263: step 67350, loss = 17.81 (3.2 examples/sec; 4.938 sec/batch)
2018-05-05 10:34:22.950231: step 67360, loss = 17.64 (3.3 examples/sec; 4.892 sec/batch)
2018-05-05 10:35:12.226549: step 67370, loss = 17.97 (3.3 examples/sec; 4.814 sec/batch)
2018-05-05 10:35:58.022383: step 67380, loss = 17.90 (3.3 examples/sec; 4.919 sec/batch)
2018-05-05 10:36:47.598833: step 67390, loss = 17.87 (3.4 examples/sec; 4.759 sec/batch)
2018-05-05 10:37:37.988744: step 67400, loss = 17.80 (3.3 examples/sec; 4.892 sec/batch)
2018-05-05 10:38:30.924808: step 67410, loss = 17.89 (3.3 examples/sec; 4.864 sec/batch)
2018-05-05 10:39:20.657952: step 67420, loss = 18.06 (3.2 examples/sec; 4.930 sec/batch)
2018-05-05 10:40:10.830108: step 67430, loss = 17.78 (3.0 examples/sec; 5.294 sec/batch)
2018-05-05 10:41:00.551248: step 67440, loss = 18.21 (3.2 examples/sec; 4.994 sec/batch)
2018-05-05 10:41:50.426109: step 67450, loss = 17.97 (3.2 examples/sec; 4.955 sec/batch)
2018-05-05 10:42:39.679915: step 67460, loss = 18.15 (3.3 examples/sec; 4.799 sec/batch)
2018-05-05 10:43:29.170408: step 67470, loss = 17.84 (3.2 examples/sec; 4.940 sec/batch)
2018-05-05 10:44:19.523088: step 67480, loss = 18.27 (3.2 examples/sec; 5.006 sec/batch)
2018-05-05 10:45:09.329797: step 67490, loss = 17.76 (3.2 examples/sec; 5.059 sec/batch)
2018-05-05 10:45:55.505086: step 67500, loss = 17.85 (3.2 examples/sec; 5.065 sec/batch)
2018-05-05 10:46:49.106612: step 67510, loss = 17.74 (3.1 examples/sec; 5.199 sec/batch)
2018-05-05 10:47:38.720391: step 67520, loss = 17.78 (3.2 examples/sec; 5.019 sec/batch)
2018-05-05 10:48:27.712891: step 67530, loss = 17.71 (3.3 examples/sec; 4.846 sec/batch)
2018-05-05 10:49:17.622027: step 67540, loss = 17.86 (3.1 examples/sec; 5.152 sec/batch)
2018-05-05 10:50:06.509995: step 67550, loss = 18.07 (3.3 examples/sec; 4.868 sec/batch)
2018-05-05 10:50:55.329423: step 67560, loss = 18.23 (3.4 examples/sec; 4.743 sec/batch)
2018-05-05 10:51:45.335346: step 67570, loss = 18.20 (3.2 examples/sec; 4.992 sec/batch)
2018-05-05 10:52:35.045838: step 67580, loss = 17.97 (3.2 examples/sec; 4.962 sec/batch)
2018-05-05 10:53:24.362576: step 67590, loss = 17.93 (3.2 examples/sec; 5.001 sec/batch)
2018-05-05 10:54:13.032842: step 67600, loss = 17.78 (3.3 examples/sec; 4.827 sec/batch)
2018-05-05 10:55:05.908119: step 67610, loss = 18.01 (3.3 examples/sec; 4.883 sec/batch)
2018-05-05 10:55:53.631268: step 67620, loss = 17.82 (4.2 examples/sec; 3.854 sec/batch)
2018-05-05 10:56:41.108592: step 67630, loss = 17.97 (3.4 examples/sec; 4.732 sec/batch)
2018-05-05 10:57:30.475522: step 67640, loss = 18.20 (3.2 examples/sec; 5.014 sec/batch)
2018-05-05 10:58:20.329701: step 67650, loss = 17.91 (3.2 examples/sec; 4.985 sec/batch)
2018-05-05 10:59:09.688940: step 67660, loss = 18.00 (3.1 examples/sec; 5.089 sec/batch)
2018-05-05 10:59:58.671887: step 67670, loss = 17.98 (3.2 examples/sec; 5.012 sec/batch)
2018-05-05 11:00:48.871830: step 67680, loss = 17.90 (3.2 examples/sec; 4.976 sec/batch)
2018-05-05 11:01:38.151324: step 67690, loss = 18.22 (3.2 examples/sec; 4.933 sec/batch)
2018-05-05 11:02:27.513946: step 67700, loss = 18.71 (3.2 examples/sec; 5.066 sec/batch)
2018-05-05 11:03:20.173169: step 67710, loss = 17.74 (3.4 examples/sec; 4.751 sec/batch)
2018-05-05 11:04:10.135868: step 67720, loss = 18.09 (3.2 examples/sec; 5.008 sec/batch)
2018-05-05 11:04:59.082039: step 67730, loss = 17.99 (3.1 examples/sec; 5.080 sec/batch)
2018-05-05 11:05:47.900161: step 67740, loss = 18.79 (3.2 examples/sec; 4.927 sec/batch)
2018-05-05 11:06:33.903185: step 67750, loss = 17.79 (3.3 examples/sec; 4.824 sec/batch)
2018-05-05 11:07:22.070453: step 67760, loss = 18.58 (3.3 examples/sec; 4.787 sec/batch)
2018-05-05 11:08:11.137050: step 67770, loss = 18.01 (3.3 examples/sec; 4.860 sec/batch)
2018-05-05 11:09:00.675421: step 67780, loss = 18.18 (3.2 examples/sec; 5.039 sec/batch)
2018-05-05 11:09:49.860839: step 67790, loss = 17.98 (3.3 examples/sec; 4.816 sec/batch)
2018-05-05 11:10:39.343757: step 67800, loss = 17.96 (3.2 examples/sec; 4.944 sec/batch)
2018-05-05 11:11:32.218363: step 67810, loss = 18.24 (3.2 examples/sec; 4.980 sec/batch)
2018-05-05 11:12:22.295492: step 67820, loss = 18.05 (3.2 examples/sec; 5.039 sec/batch)
2018-05-05 11:13:12.140084: step 67830, loss = 17.81 (3.2 examples/sec; 4.954 sec/batch)
2018-05-05 11:14:01.018365: step 67840, loss = 18.08 (3.2 examples/sec; 5.049 sec/batch)
2018-05-05 11:14:50.367728: step 67850, loss = 17.94 (3.3 examples/sec; 4.879 sec/batch)
2018-05-05 11:15:39.206241: step 67860, loss = 18.31 (3.4 examples/sec; 4.689 sec/batch)
2018-05-05 11:16:26.898081: step 67870, loss = 18.25 (4.3 examples/sec; 3.743 sec/batch)
2018-05-05 11:17:13.855811: step 67880, loss = 18.64 (3.2 examples/sec; 4.973 sec/batch)
2018-05-05 11:18:03.612691: step 67890, loss = 17.72 (3.3 examples/sec; 4.778 sec/batch)
2018-05-05 11:18:53.462361: step 67900, loss = 18.18 (3.2 examples/sec; 4.943 sec/batch)
2018-05-05 11:19:47.507658: step 67910, loss = 18.27 (3.3 examples/sec; 4.884 sec/batch)
2018-05-05 11:20:36.696086: step 67920, loss = 18.29 (3.2 examples/sec; 4.929 sec/batch)
2018-05-05 11:21:25.924151: step 67930, loss = 17.84 (3.2 examples/sec; 4.948 sec/batch)
2018-05-05 11:22:16.171733: step 67940, loss = 18.03 (3.1 examples/sec; 5.237 sec/batch)
2018-05-05 11:23:05.868333: step 67950, loss = 18.04 (3.3 examples/sec; 4.899 sec/batch)
2018-05-05 11:23:55.954143: step 67960, loss = 17.82 (3.2 examples/sec; 5.075 sec/batch)
2018-05-05 11:24:45.442877: step 67970, loss = 17.86 (3.2 examples/sec; 5.029 sec/batch)
2018-05-05 11:25:35.502180: step 67980, loss = 17.66 (3.2 examples/sec; 5.026 sec/batch)
2018-05-05 11:26:25.017648: step 67990, loss = 18.25 (3.3 examples/sec; 4.849 sec/batch)
2018-05-05 11:27:10.535872: step 68000, loss = 18.42 (3.3 examples/sec; 4.884 sec/batch)
2018-05-05 11:28:03.581443: step 68010, loss = 17.80 (3.3 examples/sec; 4.903 sec/batch)
2018-05-05 11:28:53.493933: step 68020, loss = 18.36 (3.3 examples/sec; 4.869 sec/batch)
2018-05-05 11:29:43.578109: step 68030, loss = 18.09 (3.2 examples/sec; 5.066 sec/batch)
2018-05-05 11:30:32.613101: step 68040, loss = 18.96 (3.2 examples/sec; 5.005 sec/batch)
2018-05-05 11:31:21.719956: step 68050, loss = 17.83 (3.3 examples/sec; 4.803 sec/batch)
2018-05-05 11:32:10.569667: step 68060, loss = 18.54 (3.2 examples/sec; 4.974 sec/batch)
2018-05-05 11:32:59.356035: step 68070, loss = 18.26 (3.3 examples/sec; 4.864 sec/batch)
2018-05-05 11:33:48.898307: step 68080, loss = 17.78 (3.2 examples/sec; 4.996 sec/batch)
2018-05-05 11:34:36.816708: step 68090, loss = 18.67 (3.3 examples/sec; 4.831 sec/batch)
2018-05-05 11:35:26.673672: step 68100, loss = 18.23 (3.0 examples/sec; 5.248 sec/batch)
2018-05-05 11:36:20.301908: step 68110, loss = 18.61 (3.2 examples/sec; 5.014 sec/batch)
2018-05-05 11:37:06.313923: step 68120, loss = 17.88 (3.3 examples/sec; 4.815 sec/batch)
2018-05-05 11:37:55.502344: step 68130, loss = 17.99 (3.2 examples/sec; 4.968 sec/batch)
2018-05-05 11:38:45.225169: step 68140, loss = 17.88 (3.2 examples/sec; 5.035 sec/batch)
2018-05-05 11:39:35.095031: step 68150, loss = 17.87 (3.3 examples/sec; 4.798 sec/batch)
2018-05-05 11:40:24.167811: step 68160, loss = 18.04 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 11:41:13.674469: step 68170, loss = 17.65 (3.3 examples/sec; 4.909 sec/batch)
2018-05-05 11:42:03.207657: step 68180, loss = 18.46 (3.2 examples/sec; 4.967 sec/batch)
2018-05-05 11:42:52.284646: step 68190, loss = 17.98 (3.2 examples/sec; 4.933 sec/batch)
2018-05-05 11:43:42.286940: step 68200, loss = 18.21 (3.3 examples/sec; 4.891 sec/batch)
2018-05-05 11:44:35.502732: step 68210, loss = 18.38 (3.1 examples/sec; 5.080 sec/batch)
2018-05-05 11:45:24.528457: step 68220, loss = 17.74 (3.3 examples/sec; 4.804 sec/batch)
2018-05-05 11:46:14.492018: step 68230, loss = 17.75 (3.2 examples/sec; 4.987 sec/batch)
2018-05-05 11:47:04.295721: step 68240, loss = 18.15 (3.2 examples/sec; 4.952 sec/batch)
2018-05-05 11:47:50.702656: step 68250, loss = 18.12 (3.3 examples/sec; 4.842 sec/batch)
2018-05-05 11:48:40.119911: step 68260, loss = 17.95 (3.3 examples/sec; 4.817 sec/batch)
2018-05-05 11:49:29.910506: step 68270, loss = 18.11 (3.3 examples/sec; 4.889 sec/batch)
2018-05-05 11:50:19.239818: step 68280, loss = 18.19 (3.2 examples/sec; 5.076 sec/batch)
2018-05-05 11:51:09.543765: step 68290, loss = 17.78 (3.2 examples/sec; 4.982 sec/batch)
2018-05-05 11:51:59.451578: step 68300, loss = 18.20 (3.3 examples/sec; 4.888 sec/batch)
2018-05-05 11:52:52.174927: step 68310, loss = 18.52 (3.4 examples/sec; 4.761 sec/batch)
2018-05-05 11:53:41.500057: step 68320, loss = 18.09 (3.2 examples/sec; 4.930 sec/batch)
2018-05-05 11:54:31.255055: step 68330, loss = 18.02 (3.2 examples/sec; 4.975 sec/batch)
2018-05-05 11:55:20.341514: step 68340, loss = 17.94 (3.3 examples/sec; 4.808 sec/batch)
2018-05-05 11:56:09.754649: step 68350, loss = 17.93 (3.2 examples/sec; 5.007 sec/batch)
2018-05-05 11:56:58.591043: step 68360, loss = 17.69 (3.3 examples/sec; 4.850 sec/batch)
2018-05-05 11:57:44.643906: step 68370, loss = 17.70 (3.2 examples/sec; 4.941 sec/batch)
2018-05-05 11:58:34.110483: step 68380, loss = 17.62 (3.2 examples/sec; 4.974 sec/batch)
2018-05-05 11:59:23.743647: step 68390, loss = 18.25 (3.3 examples/sec; 4.846 sec/batch)
2018-05-05 12:00:12.452634: step 68400, loss = 17.75 (3.4 examples/sec; 4.774 sec/batch)
2018-05-05 12:01:06.021687: step 68410, loss = 18.00 (3.2 examples/sec; 5.062 sec/batch)
2018-05-05 12:01:55.171294: step 68420, loss = 17.73 (3.2 examples/sec; 4.935 sec/batch)
2018-05-05 12:02:45.637294: step 68430, loss = 18.50 (3.2 examples/sec; 5.060 sec/batch)
2018-05-05 12:03:35.538446: step 68440, loss = 17.91 (3.3 examples/sec; 4.916 sec/batch)
2018-05-05 12:04:24.980114: step 68450, loss = 18.16 (3.2 examples/sec; 4.955 sec/batch)
2018-05-05 12:05:14.263292: step 68460, loss = 17.82 (3.4 examples/sec; 4.759 sec/batch)
2018-05-05 12:06:04.114560: step 68470, loss = 18.36 (3.2 examples/sec; 5.007 sec/batch)
2018-05-05 12:06:53.701262: step 68480, loss = 18.14 (3.2 examples/sec; 5.079 sec/batch)
2018-05-05 12:07:42.781319: step 68490, loss = 18.47 (3.2 examples/sec; 4.929 sec/batch)
2018-05-05 12:08:28.855633: step 68500, loss = 17.90 (3.2 examples/sec; 5.066 sec/batch)
2018-05-05 12:09:22.146866: step 68510, loss = 17.64 (3.2 examples/sec; 4.941 sec/batch)
2018-05-05 12:10:11.836167: step 68520, loss = 18.02 (3.2 examples/sec; 5.072 sec/batch)
2018-05-05 12:11:01.909427: step 68530, loss = 18.57 (3.2 examples/sec; 4.979 sec/batch)
2018-05-05 12:11:51.485854: step 68540, loss = 17.90 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 12:12:40.520131: step 68550, loss = 18.18 (3.2 examples/sec; 4.993 sec/batch)
2018-05-05 12:13:29.573119: step 68560, loss = 18.13 (3.3 examples/sec; 4.919 sec/batch)
2018-05-05 12:14:18.526284: step 68570, loss = 18.04 (3.3 examples/sec; 4.919 sec/batch)
2018-05-05 12:15:08.633421: step 68580, loss = 17.82 (3.3 examples/sec; 4.855 sec/batch)
2018-05-05 12:15:57.663991: step 68590, loss = 18.12 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 12:16:47.542132: step 68600, loss = 17.90 (3.2 examples/sec; 5.016 sec/batch)
2018-05-05 12:17:40.220394: step 68610, loss = 17.93 (3.2 examples/sec; 4.974 sec/batch)
2018-05-05 12:18:25.811244: step 68620, loss = 17.70 (3.2 examples/sec; 4.930 sec/batch)
2018-05-05 12:19:14.770020: step 68630, loss = 18.17 (3.3 examples/sec; 4.885 sec/batch)
2018-05-05 12:20:04.140248: step 68640, loss = 18.32 (3.3 examples/sec; 4.908 sec/batch)
2018-05-05 12:20:53.452833: step 68650, loss = 18.26 (3.3 examples/sec; 4.876 sec/batch)
2018-05-05 12:21:43.019862: step 68660, loss = 18.64 (3.2 examples/sec; 4.994 sec/batch)
2018-05-05 12:22:32.794778: step 68670, loss = 17.79 (3.2 examples/sec; 4.998 sec/batch)
2018-05-05 12:23:22.044641: step 68680, loss = 18.09 (3.3 examples/sec; 4.840 sec/batch)
2018-05-05 12:24:10.899793: step 68690, loss = 18.13 (3.3 examples/sec; 4.880 sec/batch)
2018-05-05 12:25:00.153602: step 68700, loss = 18.14 (3.0 examples/sec; 5.262 sec/batch)
2018-05-05 12:25:52.352348: step 68710, loss = 17.75 (3.3 examples/sec; 4.867 sec/batch)
2018-05-05 12:26:42.197398: step 68720, loss = 17.92 (3.2 examples/sec; 4.948 sec/batch)
2018-05-05 12:27:31.769542: step 68730, loss = 18.00 (3.2 examples/sec; 5.003 sec/batch)
2018-05-05 12:28:19.549460: step 68740, loss = 18.19 (4.2 examples/sec; 3.822 sec/batch)
2018-05-05 12:29:06.191033: step 68750, loss = 18.03 (3.2 examples/sec; 4.967 sec/batch)
2018-05-05 12:29:55.101623: step 68760, loss = 17.77 (3.2 examples/sec; 4.943 sec/batch)
2018-05-05 12:30:43.614682: step 68770, loss = 18.14 (3.2 examples/sec; 5.039 sec/batch)
2018-05-05 12:31:32.761857: step 68780, loss = 18.39 (3.2 examples/sec; 5.003 sec/batch)
2018-05-05 12:32:22.182840: step 68790, loss = 17.91 (3.3 examples/sec; 4.822 sec/batch)
2018-05-05 12:33:11.923896: step 68800, loss = 18.14 (3.1 examples/sec; 5.130 sec/batch)
2018-05-05 12:34:05.230547: step 68810, loss = 18.06 (3.2 examples/sec; 5.073 sec/batch)
2018-05-05 12:34:54.312024: step 68820, loss = 17.57 (3.2 examples/sec; 4.924 sec/batch)
2018-05-05 12:35:44.111432: step 68830, loss = 18.36 (3.3 examples/sec; 4.922 sec/batch)
2018-05-05 12:36:33.231052: step 68840, loss = 18.31 (3.3 examples/sec; 4.879 sec/batch)
2018-05-05 12:37:22.232928: step 68850, loss = 17.93 (3.2 examples/sec; 5.056 sec/batch)
2018-05-05 12:38:11.471637: step 68860, loss = 18.37 (3.3 examples/sec; 4.923 sec/batch)
2018-05-05 12:38:57.683035: step 68870, loss = 17.91 (3.2 examples/sec; 5.010 sec/batch)
2018-05-05 12:39:46.938680: step 68880, loss = 18.14 (3.3 examples/sec; 4.910 sec/batch)
2018-05-05 12:40:36.252590: step 68890, loss = 17.87 (3.3 examples/sec; 4.881 sec/batch)
2018-05-05 12:41:26.034470: step 68900, loss = 17.80 (3.3 examples/sec; 4.853 sec/batch)
2018-05-05 12:42:19.449403: step 68910, loss = 18.06 (3.1 examples/sec; 5.155 sec/batch)
2018-05-05 12:43:08.955932: step 68920, loss = 17.71 (3.2 examples/sec; 5.078 sec/batch)
2018-05-05 12:43:58.456653: step 68930, loss = 17.64 (3.3 examples/sec; 4.784 sec/batch)
2018-05-05 12:44:47.658053: step 68940, loss = 18.12 (3.3 examples/sec; 4.871 sec/batch)
2018-05-05 12:45:36.899111: step 68950, loss = 18.34 (3.3 examples/sec; 4.810 sec/batch)
2018-05-05 12:46:26.752822: step 68960, loss = 18.00 (3.3 examples/sec; 4.841 sec/batch)
2018-05-05 12:47:16.212727: step 68970, loss = 17.81 (3.3 examples/sec; 4.814 sec/batch)
2018-05-05 12:48:05.632714: step 68980, loss = 17.75 (3.2 examples/sec; 4.949 sec/batch)
2018-05-05 12:48:52.161782: step 68990, loss = 17.72 (4.1 examples/sec; 3.867 sec/batch)
2018-05-05 12:49:40.498134: step 69000, loss = 18.28 (3.3 examples/sec; 4.859 sec/batch)
2018-05-05 12:50:33.760978: step 69010, loss = 17.93 (3.3 examples/sec; 4.806 sec/batch)
2018-05-05 12:51:22.793871: step 69020, loss = 18.07 (3.2 examples/sec; 4.930 sec/batch)
2018-05-05 12:52:11.561704: step 69030, loss = 18.03 (3.3 examples/sec; 4.797 sec/batch)
2018-05-05 12:53:00.610884: step 69040, loss = 18.46 (3.2 examples/sec; 4.960 sec/batch)
2018-05-05 12:53:49.892033: step 69050, loss = 17.93 (3.2 examples/sec; 4.962 sec/batch)
2018-05-05 12:54:38.822703: step 69060, loss = 18.09 (3.3 examples/sec; 4.814 sec/batch)
2018-05-05 12:55:27.694816: step 69070, loss = 18.17 (3.3 examples/sec; 4.815 sec/batch)
2018-05-05 12:56:16.711509: step 69080, loss = 17.62 (3.3 examples/sec; 4.920 sec/batch)
2018-05-05 12:57:05.577861: step 69090, loss = 17.91 (3.4 examples/sec; 4.699 sec/batch)
2018-05-05 12:57:54.590225: step 69100, loss = 17.66 (3.3 examples/sec; 4.802 sec/batch)
2018-05-05 12:58:47.486392: step 69110, loss = 18.30 (3.3 examples/sec; 4.826 sec/batch)
2018-05-05 12:59:33.414065: step 69120, loss = 17.77 (3.2 examples/sec; 4.971 sec/batch)
2018-05-05 13:00:22.657902: step 69130, loss = 18.27 (3.2 examples/sec; 4.962 sec/batch)
2018-05-05 13:01:11.869182: step 69140, loss = 18.11 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 13:02:01.483629: step 69150, loss = 17.82 (3.2 examples/sec; 4.957 sec/batch)
2018-05-05 13:02:50.934212: step 69160, loss = 18.50 (3.3 examples/sec; 4.912 sec/batch)
2018-05-05 13:03:40.698358: step 69170, loss = 18.38 (3.3 examples/sec; 4.896 sec/batch)
2018-05-05 13:04:29.455711: step 69180, loss = 17.88 (3.3 examples/sec; 4.901 sec/batch)
2018-05-05 13:05:18.291073: step 69190, loss = 18.40 (3.3 examples/sec; 4.901 sec/batch)
2018-05-05 13:06:07.057579: step 69200, loss = 18.32 (3.2 examples/sec; 4.924 sec/batch)
2018-05-05 13:07:00.284956: step 69210, loss = 18.23 (3.3 examples/sec; 4.852 sec/batch)
2018-05-05 13:07:49.197264: step 69220, loss = 18.25 (3.3 examples/sec; 4.857 sec/batch)
2018-05-05 13:08:38.418018: step 69230, loss = 17.67 (3.2 examples/sec; 4.933 sec/batch)
2018-05-05 13:09:27.698806: step 69240, loss = 17.85 (3.3 examples/sec; 4.877 sec/batch)
2018-05-05 13:10:13.887731: step 69250, loss = 18.50 (3.2 examples/sec; 4.999 sec/batch)
2018-05-05 13:11:02.856674: step 69260, loss = 17.93 (3.2 examples/sec; 5.024 sec/batch)
2018-05-05 13:11:52.947782: step 69270, loss = 18.23 (3.2 examples/sec; 5.027 sec/batch)
2018-05-05 13:12:42.378711: step 69280, loss = 18.18 (3.2 examples/sec; 5.026 sec/batch)
2018-05-05 13:13:32.137950: step 69290, loss = 18.11 (3.2 examples/sec; 4.963 sec/batch)
2018-05-05 13:14:21.514276: step 69300, loss = 17.99 (3.1 examples/sec; 5.125 sec/batch)
2018-05-05 13:15:14.028776: step 69310, loss = 18.18 (3.2 examples/sec; 4.978 sec/batch)
2018-05-05 13:16:03.947839: step 69320, loss = 17.76 (3.1 examples/sec; 5.187 sec/batch)
2018-05-05 13:16:53.278597: step 69330, loss = 18.24 (3.3 examples/sec; 4.848 sec/batch)
2018-05-05 13:17:42.759974: step 69340, loss = 18.23 (3.3 examples/sec; 4.851 sec/batch)
2018-05-05 13:18:32.028153: step 69350, loss = 18.10 (3.3 examples/sec; 4.909 sec/batch)
2018-05-05 13:19:21.536549: step 69360, loss = 17.81 (3.2 examples/sec; 5.061 sec/batch)
2018-05-05 13:20:07.281066: step 69370, loss = 18.61 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 13:20:57.079005: step 69380, loss = 18.29 (3.3 examples/sec; 4.843 sec/batch)
2018-05-05 13:21:45.791038: step 69390, loss = 18.24 (3.3 examples/sec; 4.842 sec/batch)
2018-05-05 13:22:34.873132: step 69400, loss = 18.14 (3.2 examples/sec; 4.955 sec/batch)
2018-05-05 13:23:28.208868: step 69410, loss = 18.30 (3.2 examples/sec; 4.923 sec/batch)
2018-05-05 13:24:18.644037: step 69420, loss = 17.66 (3.2 examples/sec; 4.933 sec/batch)
2018-05-05 13:25:08.082796: step 69430, loss = 18.41 (3.2 examples/sec; 4.978 sec/batch)
2018-05-05 13:25:58.054904: step 69440, loss = 17.95 (3.2 examples/sec; 5.050 sec/batch)
2018-05-05 13:26:47.530688: step 69450, loss = 17.83 (3.1 examples/sec; 5.110 sec/batch)
2018-05-05 13:27:37.082097: step 69460, loss = 18.41 (3.1 examples/sec; 5.106 sec/batch)
2018-05-05 13:28:27.197584: step 69470, loss = 18.58 (3.3 examples/sec; 4.908 sec/batch)
2018-05-05 13:29:17.064924: step 69480, loss = 17.84 (3.2 examples/sec; 5.039 sec/batch)
2018-05-05 13:30:05.782282: step 69490, loss = 18.05 (4.2 examples/sec; 3.822 sec/batch)
2018-05-05 13:30:54.298440: step 69500, loss = 17.89 (3.2 examples/sec; 5.061 sec/batch)
2018-05-05 13:31:47.124897: step 69510, loss = 18.03 (3.3 examples/sec; 4.874 sec/batch)
2018-05-05 13:32:36.915100: step 69520, loss = 18.44 (3.3 examples/sec; 4.847 sec/batch)
2018-05-05 13:33:26.312356: step 69530, loss = 18.44 (3.3 examples/sec; 4.871 sec/batch)
2018-05-05 13:34:15.589896: step 69540, loss = 17.93 (3.3 examples/sec; 4.895 sec/batch)
2018-05-05 13:35:04.969512: step 69550, loss = 17.92 (3.3 examples/sec; 4.825 sec/batch)
2018-05-05 13:35:54.171061: step 69560, loss = 17.94 (3.3 examples/sec; 4.856 sec/batch)
2018-05-05 13:36:43.416277: step 69570, loss = 18.00 (3.3 examples/sec; 4.891 sec/batch)
2018-05-05 13:37:32.831856: step 69580, loss = 17.95 (3.1 examples/sec; 5.158 sec/batch)
2018-05-05 13:38:22.107681: step 69590, loss = 17.82 (3.3 examples/sec; 4.911 sec/batch)
2018-05-05 13:39:11.897986: step 69600, loss = 17.73 (3.3 examples/sec; 4.884 sec/batch)
2018-05-05 13:40:04.414432: step 69610, loss = 17.91 (3.4 examples/sec; 4.684 sec/batch)
2018-05-05 13:40:50.251586: step 69620, loss = 18.13 (3.2 examples/sec; 5.033 sec/batch)
2018-05-05 13:41:39.829479: step 69630, loss = 18.09 (3.2 examples/sec; 4.929 sec/batch)
2018-05-05 13:42:29.905049: step 69640, loss = 18.05 (3.3 examples/sec; 4.871 sec/batch)
2018-05-05 13:43:19.625179: step 69650, loss = 17.59 (3.3 examples/sec; 4.832 sec/batch)
2018-05-05 13:44:09.073575: step 69660, loss = 17.80 (3.2 examples/sec; 5.004 sec/batch)
2018-05-05 13:44:58.459188: step 69670, loss = 18.47 (3.1 examples/sec; 5.086 sec/batch)
2018-05-05 13:45:48.578081: step 69680, loss = 17.89 (3.3 examples/sec; 4.895 sec/batch)
2018-05-05 13:46:37.800114: step 69690, loss = 18.38 (3.3 examples/sec; 4.841 sec/batch)
2018-05-05 13:47:27.318601: step 69700, loss = 17.99 (3.3 examples/sec; 4.803 sec/batch)
2018-05-05 13:48:19.966873: step 69710, loss = 17.99 (3.3 examples/sec; 4.914 sec/batch)
2018-05-05 13:49:09.014040: step 69720, loss = 17.87 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 13:49:58.122520: step 69730, loss = 17.86 (3.3 examples/sec; 4.915 sec/batch)
2018-05-05 13:50:45.577800: step 69740, loss = 17.73 (4.2 examples/sec; 3.774 sec/batch)
2018-05-05 13:51:33.386337: step 69750, loss = 18.05 (3.2 examples/sec; 4.975 sec/batch)
2018-05-05 13:52:23.398504: step 69760, loss = 18.32 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 13:53:12.363141: step 69770, loss = 17.86 (3.2 examples/sec; 5.034 sec/batch)
2018-05-05 13:54:01.991777: step 69780, loss = 17.90 (3.3 examples/sec; 4.888 sec/batch)
2018-05-05 13:54:51.665210: step 69790, loss = 17.76 (3.1 examples/sec; 5.083 sec/batch)
2018-05-05 13:55:41.010174: step 69800, loss = 17.76 (3.2 examples/sec; 5.016 sec/batch)
2018-05-05 13:56:34.395399: step 69810, loss = 18.11 (3.1 examples/sec; 5.188 sec/batch)
2018-05-05 13:57:23.790520: step 69820, loss = 18.04 (3.3 examples/sec; 4.817 sec/batch)
2018-05-05 13:58:13.280311: step 69830, loss = 18.15 (3.2 examples/sec; 5.067 sec/batch)
2018-05-05 13:59:02.513382: step 69840, loss = 17.62 (3.2 examples/sec; 4.934 sec/batch)
2018-05-05 13:59:51.855482: step 69850, loss = 18.05 (3.3 examples/sec; 4.803 sec/batch)
2018-05-05 14:00:40.825301: step 69860, loss = 17.63 (3.1 examples/sec; 5.082 sec/batch)
2018-05-05 14:01:27.090309: step 69870, loss = 17.82 (3.2 examples/sec; 5.078 sec/batch)
2018-05-05 14:02:17.138693: step 69880, loss = 17.94 (3.3 examples/sec; 4.835 sec/batch)
2018-05-05 14:03:06.749079: step 69890, loss = 17.81 (3.2 examples/sec; 4.985 sec/batch)
2018-05-05 14:03:56.377069: step 69900, loss = 18.30 (3.1 examples/sec; 5.151 sec/batch)
2018-05-05 14:04:49.757939: step 69910, loss = 17.96 (3.2 examples/sec; 4.951 sec/batch)
2018-05-05 14:05:39.128496: step 69920, loss = 18.10 (3.3 examples/sec; 4.896 sec/batch)
2018-05-05 14:06:28.777274: step 69930, loss = 17.92 (3.3 examples/sec; 4.841 sec/batch)
2018-05-05 14:07:19.060313: step 69940, loss = 17.61 (3.2 examples/sec; 4.998 sec/batch)
2018-05-05 14:08:08.860837: step 69950, loss = 18.32 (3.2 examples/sec; 4.959 sec/batch)
2018-05-05 14:08:59.300362: step 69960, loss = 18.13 (3.2 examples/sec; 5.062 sec/batch)
2018-05-05 14:09:48.877873: step 69970, loss = 17.70 (3.2 examples/sec; 4.967 sec/batch)
2018-05-05 14:10:38.215388: step 69980, loss = 17.69 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 14:11:24.042430: step 69990, loss = 17.84 (3.3 examples/sec; 4.848 sec/batch)
2018-05-05 14:12:12.997084: step 70000, loss = 18.11 (3.4 examples/sec; 4.687 sec/batch)
2018-05-05 14:13:06.435936: step 70010, loss = 18.35 (3.1 examples/sec; 5.149 sec/batch)
2018-05-05 14:13:56.339511: step 70020, loss = 17.86 (3.2 examples/sec; 4.980 sec/batch)
2018-05-05 14:14:46.214042: step 70030, loss = 18.11 (3.1 examples/sec; 5.156 sec/batch)
2018-05-05 14:15:35.326553: step 70040, loss = 18.00 (3.3 examples/sec; 4.846 sec/batch)
2018-05-05 14:16:24.777873: step 70050, loss = 17.87 (3.3 examples/sec; 4.860 sec/batch)
2018-05-05 14:17:13.912857: step 70060, loss = 18.13 (3.3 examples/sec; 4.899 sec/batch)
2018-05-05 14:18:03.346387: step 70070, loss = 18.35 (3.3 examples/sec; 4.791 sec/batch)
2018-05-05 14:18:53.052693: step 70080, loss = 18.56 (3.2 examples/sec; 4.966 sec/batch)
2018-05-05 14:19:42.936997: step 70090, loss = 17.78 (3.2 examples/sec; 4.977 sec/batch)
2018-05-05 14:20:32.834348: step 70100, loss = 17.65 (3.2 examples/sec; 5.053 sec/batch)
2018-05-05 14:21:25.138647: step 70110, loss = 18.09 (4.2 examples/sec; 3.836 sec/batch)
2018-05-05 14:22:12.278212: step 70120, loss = 17.60 (3.2 examples/sec; 5.013 sec/batch)
2018-05-05 14:23:01.304825: step 70130, loss = 18.41 (3.3 examples/sec; 4.914 sec/batch)
2018-05-05 14:23:50.768746: step 70140, loss = 17.79 (3.3 examples/sec; 4.792 sec/batch)
2018-05-05 14:24:40.434293: step 70150, loss = 17.98 (3.1 examples/sec; 5.114 sec/batch)
2018-05-05 14:25:30.432159: step 70160, loss = 17.92 (3.1 examples/sec; 5.102 sec/batch)
2018-05-05 14:26:19.492063: step 70170, loss = 18.64 (3.3 examples/sec; 4.815 sec/batch)
2018-05-05 14:27:08.953385: step 70180, loss = 17.76 (3.3 examples/sec; 4.919 sec/batch)
2018-05-05 14:27:58.347513: step 70190, loss = 17.93 (3.2 examples/sec; 4.963 sec/batch)
2018-05-05 14:28:47.832558: step 70200, loss = 18.54 (3.2 examples/sec; 4.957 sec/batch)
2018-05-05 14:29:40.560528: step 70210, loss = 17.83 (3.2 examples/sec; 4.971 sec/batch)
2018-05-05 14:30:29.452994: step 70220, loss = 17.97 (3.4 examples/sec; 4.736 sec/batch)
2018-05-05 14:31:19.518418: step 70230, loss = 18.32 (3.2 examples/sec; 5.073 sec/batch)
2018-05-05 14:32:05.696652: step 70240, loss = 18.07 (3.2 examples/sec; 5.041 sec/batch)
2018-05-05 14:32:54.932712: step 70250, loss = 18.30 (3.3 examples/sec; 4.854 sec/batch)
2018-05-05 14:33:44.373169: step 70260, loss = 18.16 (3.2 examples/sec; 4.962 sec/batch)
2018-05-05 14:34:33.824602: step 70270, loss = 18.30 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 14:35:24.419788: step 70280, loss = 17.93 (3.2 examples/sec; 4.934 sec/batch)
2018-05-05 14:36:14.280906: step 70290, loss = 17.64 (3.3 examples/sec; 4.904 sec/batch)
2018-05-05 14:37:03.645676: step 70300, loss = 18.21 (3.1 examples/sec; 5.107 sec/batch)
2018-05-05 14:37:56.338733: step 70310, loss = 17.91 (3.2 examples/sec; 5.018 sec/batch)
2018-05-05 14:38:46.041915: step 70320, loss = 17.65 (3.3 examples/sec; 4.891 sec/batch)
2018-05-05 14:39:35.850336: step 70330, loss = 17.93 (3.2 examples/sec; 5.012 sec/batch)
2018-05-05 14:40:25.433018: step 70340, loss = 17.95 (3.3 examples/sec; 4.922 sec/batch)
2018-05-05 14:41:14.994279: step 70350, loss = 18.05 (3.3 examples/sec; 4.859 sec/batch)
2018-05-05 14:42:01.848043: step 70360, loss = 18.28 (4.2 examples/sec; 3.788 sec/batch)
2018-05-05 14:42:51.331675: step 70370, loss = 17.89 (3.3 examples/sec; 4.832 sec/batch)
2018-05-05 14:43:40.796030: step 70380, loss = 17.88 (3.3 examples/sec; 4.875 sec/batch)
2018-05-05 14:44:30.423974: step 70390, loss = 17.94 (3.2 examples/sec; 5.036 sec/batch)
2018-05-05 14:45:20.161047: step 70400, loss = 17.97 (3.1 examples/sec; 5.137 sec/batch)
2018-05-05 14:46:13.619037: step 70410, loss = 17.83 (3.2 examples/sec; 5.033 sec/batch)
2018-05-05 14:47:03.360355: step 70420, loss = 17.74 (3.3 examples/sec; 4.898 sec/batch)
2018-05-05 14:47:53.254544: step 70430, loss = 17.82 (3.2 examples/sec; 4.951 sec/batch)
2018-05-05 14:48:42.816248: step 70440, loss = 17.84 (3.2 examples/sec; 5.056 sec/batch)
2018-05-05 14:49:32.737415: step 70450, loss = 18.02 (3.1 examples/sec; 5.172 sec/batch)
2018-05-05 14:50:22.550242: step 70460, loss = 17.89 (3.3 examples/sec; 4.898 sec/batch)
2018-05-05 14:51:11.611248: step 70470, loss = 18.18 (3.3 examples/sec; 4.894 sec/batch)
2018-05-05 14:52:00.584136: step 70480, loss = 18.39 (3.3 examples/sec; 4.856 sec/batch)
2018-05-05 14:52:46.444552: step 70490, loss = 17.85 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 14:53:35.795213: step 70500, loss = 18.24 (3.2 examples/sec; 5.002 sec/batch)
2018-05-05 14:54:28.305543: step 70510, loss = 18.07 (3.2 examples/sec; 4.998 sec/batch)
2018-05-05 14:55:18.742138: step 70520, loss = 17.68 (3.2 examples/sec; 5.053 sec/batch)
2018-05-05 14:56:08.167738: step 70530, loss = 17.71 (3.3 examples/sec; 4.888 sec/batch)
2018-05-05 14:56:57.722484: step 70540, loss = 17.65 (3.3 examples/sec; 4.828 sec/batch)
2018-05-05 14:57:47.111297: step 70550, loss = 18.00 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 14:58:36.509475: step 70560, loss = 17.64 (3.2 examples/sec; 4.932 sec/batch)
2018-05-05 14:59:26.527681: step 70570, loss = 17.70 (3.2 examples/sec; 5.061 sec/batch)
2018-05-05 15:00:15.671062: step 70580, loss = 18.11 (3.2 examples/sec; 4.988 sec/batch)
2018-05-05 15:01:05.401920: step 70590, loss = 17.96 (3.2 examples/sec; 4.935 sec/batch)
2018-05-05 15:01:54.706545: step 70600, loss = 18.02 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 15:02:44.368698: step 70610, loss = 18.43 (3.2 examples/sec; 5.008 sec/batch)
2018-05-05 15:03:34.049070: step 70620, loss = 18.26 (3.2 examples/sec; 4.971 sec/batch)
2018-05-05 15:04:24.003688: step 70630, loss = 18.14 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 15:05:13.272655: step 70640, loss = 18.10 (3.3 examples/sec; 4.778 sec/batch)
2018-05-05 15:06:02.583423: step 70650, loss = 17.91 (3.3 examples/sec; 4.888 sec/batch)
2018-05-05 15:06:51.760591: step 70660, loss = 18.16 (3.3 examples/sec; 4.818 sec/batch)
2018-05-05 15:07:41.900386: step 70670, loss = 17.82 (3.2 examples/sec; 4.964 sec/batch)
2018-05-05 15:08:31.159189: step 70680, loss = 18.15 (3.2 examples/sec; 4.996 sec/batch)
2018-05-05 15:09:20.931892: step 70690, loss = 18.04 (3.1 examples/sec; 5.228 sec/batch)
2018-05-05 15:10:10.908072: step 70700, loss = 18.43 (3.3 examples/sec; 4.888 sec/batch)
2018-05-05 15:11:03.981439: step 70710, loss = 17.79 (3.2 examples/sec; 5.055 sec/batch)
2018-05-05 15:11:54.033827: step 70720, loss = 18.26 (3.2 examples/sec; 5.049 sec/batch)
2018-05-05 15:12:42.257128: step 70730, loss = 17.80 (4.3 examples/sec; 3.723 sec/batch)
2018-05-05 15:13:29.731745: step 70740, loss = 18.23 (3.3 examples/sec; 4.846 sec/batch)
2018-05-05 15:14:19.744630: step 70750, loss = 18.15 (3.2 examples/sec; 4.988 sec/batch)
2018-05-05 15:15:09.932728: step 70760, loss = 17.66 (3.2 examples/sec; 4.999 sec/batch)
2018-05-05 15:15:59.554145: step 70770, loss = 18.03 (3.2 examples/sec; 4.982 sec/batch)
2018-05-05 15:16:49.089403: step 70780, loss = 18.01 (3.2 examples/sec; 4.930 sec/batch)
2018-05-05 15:17:38.523556: step 70790, loss = 18.11 (3.2 examples/sec; 5.015 sec/batch)
2018-05-05 15:18:27.688201: step 70800, loss = 17.93 (3.4 examples/sec; 4.767 sec/batch)
2018-05-05 15:19:21.425136: step 70810, loss = 17.92 (3.3 examples/sec; 4.854 sec/batch)
2018-05-05 15:20:10.593237: step 70820, loss = 18.48 (3.2 examples/sec; 4.945 sec/batch)
2018-05-05 15:21:00.326073: step 70830, loss = 18.22 (3.2 examples/sec; 4.958 sec/batch)
2018-05-05 15:21:49.670606: step 70840, loss = 18.67 (3.3 examples/sec; 4.904 sec/batch)
2018-05-05 15:22:39.451874: step 70850, loss = 17.85 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 15:23:25.632292: step 70860, loss = 18.10 (3.1 examples/sec; 5.125 sec/batch)
2018-05-05 15:24:15.293564: step 70870, loss = 17.74 (3.2 examples/sec; 4.980 sec/batch)
2018-05-05 15:25:05.484473: step 70880, loss = 17.64 (3.0 examples/sec; 5.272 sec/batch)
2018-05-05 15:25:55.511180: step 70890, loss = 17.72 (3.1 examples/sec; 5.114 sec/batch)
2018-05-05 15:26:45.189957: step 70900, loss = 18.11 (3.3 examples/sec; 4.823 sec/batch)
2018-05-05 15:27:38.327013: step 70910, loss = 18.27 (3.2 examples/sec; 4.969 sec/batch)
2018-05-05 15:28:28.315316: step 70920, loss = 17.77 (3.2 examples/sec; 5.015 sec/batch)
2018-05-05 15:29:17.941884: step 70930, loss = 18.23 (3.3 examples/sec; 4.841 sec/batch)
2018-05-05 15:30:07.284980: step 70940, loss = 18.33 (3.2 examples/sec; 4.987 sec/batch)
2018-05-05 15:30:57.284825: step 70950, loss = 18.17 (3.1 examples/sec; 5.184 sec/batch)
2018-05-05 15:31:46.707067: step 70960, loss = 18.14 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 15:32:36.322347: step 70970, loss = 18.46 (3.3 examples/sec; 4.875 sec/batch)
2018-05-05 15:33:22.537222: step 70980, loss = 18.06 (3.8 examples/sec; 4.192 sec/batch)
2018-05-05 15:34:12.846409: step 70990, loss = 18.00 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 15:35:02.967539: step 71000, loss = 17.85 (3.2 examples/sec; 5.045 sec/batch)
2018-05-05 15:35:56.791312: step 71010, loss = 17.97 (3.1 examples/sec; 5.205 sec/batch)
2018-05-05 15:36:46.352917: step 71020, loss = 17.93 (3.2 examples/sec; 4.973 sec/batch)
2018-05-05 15:37:36.240968: step 71030, loss = 18.12 (3.3 examples/sec; 4.902 sec/batch)
2018-05-05 15:38:26.457484: step 71040, loss = 17.65 (3.3 examples/sec; 4.899 sec/batch)
2018-05-05 15:39:16.169777: step 71050, loss = 17.96 (3.3 examples/sec; 4.907 sec/batch)
2018-05-05 15:40:06.212492: step 71060, loss = 17.81 (3.2 examples/sec; 4.987 sec/batch)
2018-05-05 15:40:56.114233: step 71070, loss = 18.21 (3.3 examples/sec; 4.890 sec/batch)
2018-05-05 15:41:45.130793: step 71080, loss = 17.68 (3.3 examples/sec; 4.881 sec/batch)
2018-05-05 15:42:34.826200: step 71090, loss = 18.12 (3.3 examples/sec; 4.918 sec/batch)
2018-05-05 15:43:23.938219: step 71100, loss = 18.10 (3.9 examples/sec; 4.052 sec/batch)
2018-05-05 15:44:15.235016: step 71110, loss = 18.43 (3.3 examples/sec; 4.887 sec/batch)
2018-05-05 15:45:04.774851: step 71120, loss = 17.58 (3.3 examples/sec; 4.845 sec/batch)
2018-05-05 15:45:54.005655: step 71130, loss = 18.04 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 15:46:43.293069: step 71140, loss = 18.63 (3.3 examples/sec; 4.829 sec/batch)
2018-05-05 15:47:32.823211: step 71150, loss = 18.01 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 15:48:22.852215: step 71160, loss = 18.30 (3.2 examples/sec; 4.953 sec/batch)
2018-05-05 15:49:12.258996: step 71170, loss = 18.02 (3.4 examples/sec; 4.750 sec/batch)
2018-05-05 15:50:01.923376: step 71180, loss = 18.02 (3.3 examples/sec; 4.890 sec/batch)
2018-05-05 15:50:51.009879: step 71190, loss = 18.13 (3.3 examples/sec; 4.914 sec/batch)
2018-05-05 15:51:40.006356: step 71200, loss = 18.18 (3.2 examples/sec; 5.062 sec/batch)
2018-05-05 15:52:32.627625: step 71210, loss = 18.02 (3.2 examples/sec; 4.976 sec/batch)
2018-05-05 15:53:22.490121: step 71220, loss = 18.06 (3.2 examples/sec; 5.002 sec/batch)
2018-05-05 15:54:07.978061: step 71230, loss = 17.62 (3.3 examples/sec; 4.837 sec/batch)
2018-05-05 15:54:57.343731: step 71240, loss = 17.72 (3.2 examples/sec; 5.057 sec/batch)
2018-05-05 15:55:47.382630: step 71250, loss = 18.50 (3.2 examples/sec; 5.030 sec/batch)
2018-05-05 15:56:36.876653: step 71260, loss = 17.82 (3.2 examples/sec; 4.957 sec/batch)
2018-05-05 15:57:25.926881: step 71270, loss = 17.65 (3.2 examples/sec; 4.924 sec/batch)
2018-05-05 15:58:15.042268: step 71280, loss = 17.76 (3.3 examples/sec; 4.895 sec/batch)
2018-05-05 15:59:04.036292: step 71290, loss = 17.96 (3.2 examples/sec; 4.937 sec/batch)
2018-05-05 15:59:52.855707: step 71300, loss = 17.80 (3.2 examples/sec; 5.006 sec/batch)
2018-05-05 16:00:46.171809: step 71310, loss = 17.93 (3.2 examples/sec; 5.027 sec/batch)
2018-05-05 16:01:35.351478: step 71320, loss = 17.71 (3.2 examples/sec; 4.975 sec/batch)
2018-05-05 16:02:24.412381: step 71330, loss = 17.96 (3.3 examples/sec; 4.821 sec/batch)
2018-05-05 16:03:13.914821: step 71340, loss = 17.66 (3.1 examples/sec; 5.106 sec/batch)
2018-05-05 16:04:02.155348: step 71350, loss = 18.12 (4.2 examples/sec; 3.780 sec/batch)
2018-05-05 16:04:49.438969: step 71360, loss = 17.85 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 16:05:39.274802: step 71370, loss = 17.68 (3.2 examples/sec; 4.972 sec/batch)
2018-05-05 16:06:28.799205: step 71380, loss = 18.25 (3.3 examples/sec; 4.891 sec/batch)
2018-05-05 16:07:18.438171: step 71390, loss = 17.59 (3.1 examples/sec; 5.099 sec/batch)
2018-05-05 16:08:08.048644: step 71400, loss = 17.99 (3.3 examples/sec; 4.878 sec/batch)
2018-05-05 16:09:01.245131: step 71410, loss = 18.27 (3.2 examples/sec; 5.018 sec/batch)
2018-05-05 16:09:50.510470: step 71420, loss = 18.36 (3.3 examples/sec; 4.791 sec/batch)
2018-05-05 16:10:39.435951: step 71430, loss = 17.86 (3.3 examples/sec; 4.911 sec/batch)
2018-05-05 16:11:29.604539: step 71440, loss = 17.90 (3.3 examples/sec; 4.844 sec/batch)
2018-05-05 16:12:19.252713: step 71450, loss = 17.94 (3.2 examples/sec; 5.004 sec/batch)
2018-05-05 16:13:08.495361: step 71460, loss = 17.82 (3.3 examples/sec; 4.786 sec/batch)
2018-05-05 16:13:57.785902: step 71470, loss = 18.37 (3.2 examples/sec; 4.996 sec/batch)
2018-05-05 16:14:43.623261: step 71480, loss = 17.75 (3.2 examples/sec; 5.079 sec/batch)
2018-05-05 16:15:33.305058: step 71490, loss = 18.02 (3.3 examples/sec; 4.864 sec/batch)
2018-05-05 16:16:22.705413: step 71500, loss = 17.99 (3.2 examples/sec; 4.957 sec/batch)
2018-05-05 16:17:15.551064: step 71510, loss = 17.79 (3.2 examples/sec; 4.954 sec/batch)
2018-05-05 16:18:05.402172: step 71520, loss = 17.98 (3.2 examples/sec; 5.003 sec/batch)
2018-05-05 16:18:55.323195: step 71530, loss = 17.78 (3.2 examples/sec; 4.968 sec/batch)
2018-05-05 16:19:45.276151: step 71540, loss = 18.30 (3.1 examples/sec; 5.246 sec/batch)
2018-05-05 16:20:35.399365: step 71550, loss = 17.81 (3.3 examples/sec; 4.851 sec/batch)
2018-05-05 16:21:25.740930: step 71560, loss = 18.19 (3.1 examples/sec; 5.096 sec/batch)
2018-05-05 16:22:15.122652: step 71570, loss = 17.74 (3.3 examples/sec; 4.923 sec/batch)
2018-05-05 16:23:03.964471: step 71580, loss = 18.20 (3.3 examples/sec; 4.895 sec/batch)
2018-05-05 16:23:53.124009: step 71590, loss = 17.65 (3.1 examples/sec; 5.081 sec/batch)
2018-05-05 16:24:40.009701: step 71600, loss = 18.56 (4.2 examples/sec; 3.829 sec/batch)
2018-05-05 16:25:32.546028: step 71610, loss = 17.92 (3.2 examples/sec; 4.954 sec/batch)
2018-05-05 16:26:21.954126: step 71620, loss = 17.92 (3.3 examples/sec; 4.856 sec/batch)
2018-05-05 16:27:11.794028: step 71630, loss = 17.84 (3.2 examples/sec; 4.972 sec/batch)
2018-05-05 16:28:00.817634: step 71640, loss = 17.88 (3.2 examples/sec; 4.923 sec/batch)
2018-05-05 16:28:50.524974: step 71650, loss = 17.62 (3.3 examples/sec; 4.907 sec/batch)
2018-05-05 16:29:39.699352: step 71660, loss = 18.53 (3.2 examples/sec; 4.977 sec/batch)
2018-05-05 16:30:28.854573: step 71670, loss = 18.11 (3.2 examples/sec; 4.964 sec/batch)
2018-05-05 16:31:18.626294: step 71680, loss = 18.25 (3.1 examples/sec; 5.090 sec/batch)
2018-05-05 16:32:08.186939: step 71690, loss = 17.74 (3.4 examples/sec; 4.745 sec/batch)
2018-05-05 16:32:57.435816: step 71700, loss = 18.06 (3.2 examples/sec; 5.017 sec/batch)
2018-05-05 16:33:50.195434: step 71710, loss = 18.82 (3.2 examples/sec; 4.955 sec/batch)
2018-05-05 16:34:39.789757: step 71720, loss = 17.80 (3.2 examples/sec; 4.946 sec/batch)
2018-05-05 16:35:26.011806: step 71730, loss = 17.76 (3.2 examples/sec; 4.945 sec/batch)
2018-05-05 16:36:15.708371: step 71740, loss = 18.34 (3.2 examples/sec; 4.987 sec/batch)
2018-05-05 16:37:05.020723: step 71750, loss = 17.92 (3.2 examples/sec; 4.981 sec/batch)
2018-05-05 16:37:54.487078: step 71760, loss = 18.04 (3.3 examples/sec; 4.880 sec/batch)
2018-05-05 16:38:44.512286: step 71770, loss = 17.83 (3.1 examples/sec; 5.199 sec/batch)
2018-05-05 16:39:33.646840: step 71780, loss = 17.96 (3.2 examples/sec; 5.018 sec/batch)
2018-05-05 16:40:23.032376: step 71790, loss = 18.28 (3.3 examples/sec; 4.827 sec/batch)
2018-05-05 16:41:12.678321: step 71800, loss = 18.16 (3.2 examples/sec; 5.066 sec/batch)
2018-05-05 16:42:05.519550: step 71810, loss = 17.71 (3.2 examples/sec; 5.040 sec/batch)
2018-05-05 16:42:55.058467: step 71820, loss = 17.84 (3.4 examples/sec; 4.765 sec/batch)
2018-05-05 16:43:44.173703: step 71830, loss = 18.26 (3.3 examples/sec; 4.885 sec/batch)
2018-05-05 16:44:33.783071: step 71840, loss = 17.72 (3.2 examples/sec; 4.983 sec/batch)
2018-05-05 16:45:20.812906: step 71850, loss = 18.12 (4.2 examples/sec; 3.770 sec/batch)
2018-05-05 16:46:09.363218: step 71860, loss = 18.20 (3.2 examples/sec; 5.009 sec/batch)
2018-05-05 16:46:59.029196: step 71870, loss = 18.07 (3.3 examples/sec; 4.856 sec/batch)
2018-05-05 16:47:48.711050: step 71880, loss = 17.93 (3.3 examples/sec; 4.914 sec/batch)
2018-05-05 16:48:38.299843: step 71890, loss = 17.78 (3.1 examples/sec; 5.088 sec/batch)
2018-05-05 16:49:27.716798: step 71900, loss = 17.75 (3.1 examples/sec; 5.112 sec/batch)
2018-05-05 16:50:20.993051: step 71910, loss = 17.72 (3.3 examples/sec; 4.901 sec/batch)
2018-05-05 16:51:10.648289: step 71920, loss = 18.04 (3.3 examples/sec; 4.826 sec/batch)
2018-05-05 16:52:00.515266: step 71930, loss = 17.80 (3.1 examples/sec; 5.105 sec/batch)
2018-05-05 16:52:49.489035: step 71940, loss = 18.14 (3.3 examples/sec; 4.836 sec/batch)
2018-05-05 16:53:38.767598: step 71950, loss = 17.77 (3.2 examples/sec; 4.979 sec/batch)
2018-05-05 16:54:28.037531: step 71960, loss = 17.83 (3.3 examples/sec; 4.819 sec/batch)
2018-05-05 16:55:17.807815: step 71970, loss = 17.97 (3.2 examples/sec; 4.975 sec/batch)
2018-05-05 16:56:04.152927: step 71980, loss = 18.06 (3.3 examples/sec; 4.891 sec/batch)
2018-05-05 16:56:53.225562: step 71990, loss = 18.25 (3.3 examples/sec; 4.911 sec/batch)
2018-05-05 16:57:42.767629: step 72000, loss = 18.20 (3.2 examples/sec; 5.008 sec/batch)
2018-05-05 16:58:35.761378: step 72010, loss = 18.27 (3.1 examples/sec; 5.134 sec/batch)
2018-05-05 16:59:25.054812: step 72020, loss = 17.94 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 17:00:14.235962: step 72030, loss = 18.12 (3.3 examples/sec; 4.885 sec/batch)
2018-05-05 17:01:03.115867: step 72040, loss = 18.05 (3.4 examples/sec; 4.722 sec/batch)
2018-05-05 17:01:52.664973: step 72050, loss = 18.32 (3.2 examples/sec; 5.004 sec/batch)
2018-05-05 17:02:42.715680: step 72060, loss = 18.04 (3.2 examples/sec; 5.057 sec/batch)
2018-05-05 17:03:32.324207: step 72070, loss = 18.13 (3.2 examples/sec; 5.070 sec/batch)
2018-05-05 17:04:21.577854: step 72080, loss = 17.87 (3.3 examples/sec; 4.859 sec/batch)
2018-05-05 17:05:10.717843: step 72090, loss = 17.96 (3.3 examples/sec; 4.920 sec/batch)
2018-05-05 17:05:57.461102: step 72100, loss = 18.27 (4.2 examples/sec; 3.781 sec/batch)
2018-05-05 17:06:49.630072: step 72110, loss = 18.26 (3.3 examples/sec; 4.897 sec/batch)
2018-05-05 17:07:38.414112: step 72120, loss = 18.23 (3.3 examples/sec; 4.840 sec/batch)
2018-05-05 17:08:28.170406: step 72130, loss = 17.70 (3.1 examples/sec; 5.212 sec/batch)
2018-05-05 17:09:17.608823: step 72140, loss = 17.70 (3.3 examples/sec; 4.840 sec/batch)
2018-05-05 17:10:06.972273: step 72150, loss = 18.04 (3.2 examples/sec; 5.000 sec/batch)
2018-05-05 17:10:55.919873: step 72160, loss = 18.51 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 17:11:45.067213: step 72170, loss = 18.37 (3.1 examples/sec; 5.126 sec/batch)
2018-05-05 17:12:34.283297: step 72180, loss = 17.63 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 17:13:23.969685: step 72190, loss = 17.94 (3.2 examples/sec; 5.031 sec/batch)
2018-05-05 17:14:14.422492: step 72200, loss = 18.00 (3.1 examples/sec; 5.122 sec/batch)
2018-05-05 17:15:07.785383: step 72210, loss = 17.81 (3.2 examples/sec; 5.077 sec/batch)
2018-05-05 17:15:57.357918: step 72220, loss = 18.30 (3.3 examples/sec; 4.857 sec/batch)
2018-05-05 17:16:43.252879: step 72230, loss = 17.90 (3.2 examples/sec; 4.981 sec/batch)
2018-05-05 17:17:32.637730: step 72240, loss = 18.39 (3.2 examples/sec; 4.960 sec/batch)
2018-05-05 17:18:21.883944: step 72250, loss = 17.95 (3.3 examples/sec; 4.882 sec/batch)
2018-05-05 17:19:11.124520: step 72260, loss = 17.80 (3.1 examples/sec; 5.150 sec/batch)
2018-05-05 17:19:59.758823: step 72270, loss = 18.11 (3.3 examples/sec; 4.845 sec/batch)
2018-05-05 17:20:48.727821: step 72280, loss = 17.78 (3.4 examples/sec; 4.693 sec/batch)
2018-05-05 17:21:38.504595: step 72290, loss = 18.20 (3.2 examples/sec; 5.002 sec/batch)
2018-05-05 17:22:28.092491: step 72300, loss = 17.87 (3.3 examples/sec; 4.855 sec/batch)
2018-05-05 17:23:20.601708: step 72310, loss = 17.99 (3.3 examples/sec; 4.919 sec/batch)
2018-05-05 17:24:09.417302: step 72320, loss = 17.75 (3.2 examples/sec; 4.925 sec/batch)
2018-05-05 17:24:58.876936: step 72330, loss = 17.60 (3.3 examples/sec; 4.908 sec/batch)
2018-05-05 17:25:48.067201: step 72340, loss = 18.21 (3.2 examples/sec; 4.944 sec/batch)
2018-05-05 17:26:35.778696: step 72350, loss = 18.88 (4.2 examples/sec; 3.800 sec/batch)
2018-05-05 17:27:24.238614: step 72360, loss = 17.70 (3.2 examples/sec; 5.022 sec/batch)
2018-05-05 17:28:14.062110: step 72370, loss = 18.01 (3.2 examples/sec; 4.926 sec/batch)
2018-05-05 17:29:04.045161: step 72380, loss = 17.91 (3.2 examples/sec; 4.970 sec/batch)
2018-05-05 17:29:53.942018: step 72390, loss = 18.39 (3.2 examples/sec; 4.924 sec/batch)
2018-05-05 17:30:43.288629: step 72400, loss = 18.48 (3.2 examples/sec; 4.997 sec/batch)
2018-05-05 17:31:36.523419: step 72410, loss = 18.01 (3.2 examples/sec; 4.928 sec/batch)
2018-05-05 17:32:26.741168: step 72420, loss = 18.69 (3.2 examples/sec; 4.996 sec/batch)
2018-05-05 17:33:16.695464: step 72430, loss = 17.61 (3.3 examples/sec; 4.850 sec/batch)
2018-05-05 17:34:06.735081: step 72440, loss = 18.29 (3.2 examples/sec; 5.045 sec/batch)
2018-05-05 17:34:56.941843: step 72450, loss = 18.14 (3.2 examples/sec; 4.979 sec/batch)
2018-05-05 17:35:46.625341: step 72460, loss = 18.15 (3.1 examples/sec; 5.094 sec/batch)
2018-05-05 17:36:36.810203: step 72470, loss = 18.11 (3.1 examples/sec; 5.098 sec/batch)
2018-05-05 17:37:23.276337: step 72480, loss = 18.26 (3.2 examples/sec; 4.931 sec/batch)
2018-05-05 17:38:13.103122: step 72490, loss = 17.84 (3.3 examples/sec; 4.842 sec/batch)
2018-05-05 17:39:02.584964: step 72500, loss = 18.05 (3.1 examples/sec; 5.151 sec/batch)
2018-05-05 17:39:55.986253: step 72510, loss = 17.78 (3.3 examples/sec; 4.876 sec/batch)
2018-05-05 17:40:45.747632: step 72520, loss = 18.40 (3.2 examples/sec; 4.931 sec/batch)
2018-05-05 17:41:35.719936: step 72530, loss = 18.42 (3.3 examples/sec; 4.870 sec/batch)
2018-05-05 17:42:25.757888: step 72540, loss = 17.89 (3.2 examples/sec; 5.049 sec/batch)
2018-05-05 17:43:15.810055: step 72550, loss = 17.94 (3.1 examples/sec; 5.170 sec/batch)
2018-05-05 17:44:05.657747: step 72560, loss = 18.19 (3.1 examples/sec; 5.154 sec/batch)
2018-05-05 17:44:55.778724: step 72570, loss = 17.78 (3.2 examples/sec; 4.942 sec/batch)
2018-05-05 17:45:45.720946: step 72580, loss = 17.96 (3.3 examples/sec; 4.906 sec/batch)
2018-05-05 17:46:35.616884: step 72590, loss = 18.05 (3.2 examples/sec; 5.053 sec/batch)
2018-05-05 17:47:22.162652: step 72600, loss = 18.25 (3.2 examples/sec; 4.971 sec/batch)
2018-05-05 17:48:15.241020: step 72610, loss = 18.95 (3.2 examples/sec; 4.968 sec/batch)
2018-05-05 17:49:05.138169: step 72620, loss = 17.85 (3.2 examples/sec; 4.989 sec/batch)
2018-05-05 17:49:54.434304: step 72630, loss = 18.52 (3.3 examples/sec; 4.894 sec/batch)
2018-05-05 17:50:43.334606: step 72640, loss = 18.18 (3.3 examples/sec; 4.827 sec/batch)
2018-05-05 17:51:33.092036: step 72650, loss = 17.55 (3.2 examples/sec; 4.957 sec/batch)
2018-05-05 17:52:22.170207: step 72660, loss = 17.72 (3.2 examples/sec; 4.968 sec/batch)
2018-05-05 17:53:11.869111: step 72670, loss = 17.77 (3.3 examples/sec; 4.909 sec/batch)
2018-05-05 17:54:01.730177: step 72680, loss = 18.00 (3.3 examples/sec; 4.859 sec/batch)
2018-05-05 17:54:50.918325: step 72690, loss = 17.69 (3.3 examples/sec; 4.822 sec/batch)
2018-05-05 17:55:39.899721: step 72700, loss = 18.19 (3.2 examples/sec; 5.025 sec/batch)
2018-05-05 17:56:33.026328: step 72710, loss = 18.15 (3.2 examples/sec; 4.946 sec/batch)
2018-05-05 17:57:22.022369: step 72720, loss = 18.05 (3.3 examples/sec; 4.788 sec/batch)
2018-05-05 17:58:08.319809: step 72730, loss = 17.81 (3.1 examples/sec; 5.130 sec/batch)
2018-05-05 17:58:57.096717: step 72740, loss = 17.85 (3.3 examples/sec; 4.829 sec/batch)
2018-05-05 17:59:47.191246: step 72750, loss = 17.64 (3.2 examples/sec; 4.935 sec/batch)
2018-05-05 18:00:36.697824: step 72760, loss = 17.93 (3.2 examples/sec; 4.964 sec/batch)
2018-05-05 18:01:25.963807: step 72770, loss = 17.68 (3.3 examples/sec; 4.892 sec/batch)
2018-05-05 18:02:15.728920: step 72780, loss = 17.98 (3.4 examples/sec; 4.771 sec/batch)
2018-05-05 18:03:04.763757: step 72790, loss = 17.82 (3.3 examples/sec; 4.848 sec/batch)
2018-05-05 18:03:54.744063: step 72800, loss = 18.35 (3.2 examples/sec; 4.980 sec/batch)
2018-05-05 18:04:47.736509: step 72810, loss = 18.17 (3.2 examples/sec; 5.012 sec/batch)
2018-05-05 18:05:36.706473: step 72820, loss = 18.39 (3.2 examples/sec; 4.996 sec/batch)
2018-05-05 18:06:26.265275: step 72830, loss = 17.94 (3.2 examples/sec; 5.078 sec/batch)
2018-05-05 18:07:15.668985: step 72840, loss = 17.74 (3.2 examples/sec; 5.002 sec/batch)
2018-05-05 18:08:01.775098: step 72850, loss = 18.11 (3.2 examples/sec; 5.012 sec/batch)
2018-05-05 18:08:51.046872: step 72860, loss = 17.88 (3.2 examples/sec; 4.930 sec/batch)
2018-05-05 18:09:40.112841: step 72870, loss = 17.89 (3.3 examples/sec; 4.870 sec/batch)
2018-05-05 18:10:30.321937: step 72880, loss = 18.21 (3.1 examples/sec; 5.130 sec/batch)
2018-05-05 18:11:20.341305: step 72890, loss = 17.94 (3.1 examples/sec; 5.157 sec/batch)
2018-05-05 18:12:10.644499: step 72900, loss = 18.36 (3.2 examples/sec; 5.021 sec/batch)
2018-05-05 18:13:03.941674: step 72910, loss = 17.89 (3.2 examples/sec; 4.993 sec/batch)
2018-05-05 18:13:52.924994: step 72920, loss = 18.15 (3.2 examples/sec; 5.021 sec/batch)
2018-05-05 18:14:41.684547: step 72930, loss = 18.16 (3.1 examples/sec; 5.100 sec/batch)
2018-05-05 18:15:31.863725: step 72940, loss = 18.13 (3.3 examples/sec; 4.922 sec/batch)
2018-05-05 18:16:21.881728: step 72950, loss = 17.71 (3.1 examples/sec; 5.127 sec/batch)
2018-05-05 18:17:10.841296: step 72960, loss = 17.98 (3.3 examples/sec; 4.902 sec/batch)
2018-05-05 18:17:57.749740: step 72970, loss = 17.85 (4.2 examples/sec; 3.807 sec/batch)
2018-05-05 18:18:45.893021: step 72980, loss = 18.62 (3.3 examples/sec; 4.889 sec/batch)
2018-05-05 18:19:35.888098: step 72990, loss = 17.71 (3.1 examples/sec; 5.227 sec/batch)
2018-05-05 18:20:25.432910: step 73000, loss = 17.66 (3.2 examples/sec; 5.010 sec/batch)
2018-05-05 18:21:18.707610: step 73010, loss = 18.07 (3.2 examples/sec; 4.996 sec/batch)
2018-05-05 18:22:07.828742: step 73020, loss = 17.88 (3.3 examples/sec; 4.881 sec/batch)
2018-05-05 18:22:56.755631: step 73030, loss = 18.04 (3.2 examples/sec; 4.962 sec/batch)
2018-05-05 18:23:46.865808: step 73040, loss = 17.96 (3.1 examples/sec; 5.082 sec/batch)
2018-05-05 18:24:36.573821: step 73050, loss = 18.36 (3.2 examples/sec; 4.936 sec/batch)
2018-05-05 18:25:26.566592: step 73060, loss = 17.97 (3.1 examples/sec; 5.100 sec/batch)
2018-05-05 18:26:15.655093: step 73070, loss = 17.80 (3.2 examples/sec; 5.033 sec/batch)
2018-05-05 18:27:05.922396: step 73080, loss = 17.61 (3.2 examples/sec; 4.973 sec/batch)
2018-05-05 18:27:55.840905: step 73090, loss = 18.92 (3.2 examples/sec; 4.935 sec/batch)
2018-05-05 18:28:41.970839: step 73100, loss = 17.77 (3.3 examples/sec; 4.821 sec/batch)
2018-05-05 18:29:35.434088: step 73110, loss = 18.68 (3.1 examples/sec; 5.131 sec/batch)
2018-05-05 18:30:24.188984: step 73120, loss = 17.80 (3.2 examples/sec; 4.940 sec/batch)
2018-05-05 18:31:14.333767: step 73130, loss = 18.09 (3.3 examples/sec; 4.897 sec/batch)
2018-05-05 18:32:04.169719: step 73140, loss = 17.97 (3.3 examples/sec; 4.902 sec/batch)
2018-05-05 18:32:53.061334: step 73150, loss = 18.31 (3.2 examples/sec; 4.973 sec/batch)
2018-05-05 18:33:42.755497: step 73160, loss = 17.82 (3.3 examples/sec; 4.856 sec/batch)
2018-05-05 18:34:32.145106: step 73170, loss = 17.91 (3.3 examples/sec; 4.819 sec/batch)
2018-05-05 18:35:21.950554: step 73180, loss = 17.86 (3.3 examples/sec; 4.919 sec/batch)
2018-05-05 18:36:12.250227: step 73190, loss = 17.84 (3.2 examples/sec; 5.001 sec/batch)
2018-05-05 18:37:01.837105: step 73200, loss = 18.25 (3.4 examples/sec; 4.730 sec/batch)
2018-05-05 18:37:54.857722: step 73210, loss = 18.08 (3.3 examples/sec; 4.863 sec/batch)
2018-05-05 18:38:41.035298: step 73220, loss = 17.62 (3.3 examples/sec; 4.843 sec/batch)
2018-05-05 18:39:31.365166: step 73230, loss = 17.78 (3.2 examples/sec; 4.943 sec/batch)
2018-05-05 18:40:20.846083: step 73240, loss = 17.73 (3.2 examples/sec; 5.071 sec/batch)
2018-05-05 18:41:09.968401: step 73250, loss = 17.57 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 18:41:59.496269: step 73260, loss = 18.26 (3.2 examples/sec; 4.988 sec/batch)
2018-05-05 18:42:49.151236: step 73270, loss = 18.04 (3.2 examples/sec; 4.966 sec/batch)
2018-05-05 18:43:39.178641: step 73280, loss = 17.86 (3.2 examples/sec; 4.964 sec/batch)
2018-05-05 18:44:28.863318: step 73290, loss = 18.25 (3.3 examples/sec; 4.841 sec/batch)
2018-05-05 18:45:18.221052: step 73300, loss = 18.07 (3.3 examples/sec; 4.912 sec/batch)
2018-05-05 18:46:10.840685: step 73310, loss = 18.06 (3.3 examples/sec; 4.778 sec/batch)
2018-05-05 18:46:59.937118: step 73320, loss = 18.04 (3.2 examples/sec; 4.927 sec/batch)
2018-05-05 18:47:49.338962: step 73330, loss = 17.93 (3.1 examples/sec; 5.096 sec/batch)
2018-05-05 18:48:39.135256: step 73340, loss = 18.58 (3.3 examples/sec; 4.893 sec/batch)
2018-05-05 18:49:25.221084: step 73350, loss = 18.52 (3.2 examples/sec; 4.965 sec/batch)
2018-05-05 18:50:14.604780: step 73360, loss = 18.16 (3.2 examples/sec; 4.975 sec/batch)
2018-05-05 18:51:04.078192: step 73370, loss = 18.06 (3.3 examples/sec; 4.901 sec/batch)
2018-05-05 18:51:53.506527: step 73380, loss = 18.41 (3.2 examples/sec; 5.068 sec/batch)
2018-05-05 18:52:43.086354: step 73390, loss = 17.68 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 18:53:32.962558: step 73400, loss = 17.97 (3.2 examples/sec; 5.059 sec/batch)
2018-05-05 18:54:26.712650: step 73410, loss = 17.69 (3.3 examples/sec; 4.862 sec/batch)
2018-05-05 18:55:15.904602: step 73420, loss = 18.06 (3.2 examples/sec; 4.988 sec/batch)
2018-05-05 18:56:05.226602: step 73430, loss = 18.27 (3.2 examples/sec; 4.972 sec/batch)
2018-05-05 18:56:55.128411: step 73440, loss = 18.32 (3.3 examples/sec; 4.850 sec/batch)
2018-05-05 18:57:44.888658: step 73450, loss = 17.92 (3.2 examples/sec; 4.981 sec/batch)
2018-05-05 18:58:34.306982: step 73460, loss = 17.71 (3.2 examples/sec; 5.040 sec/batch)
2018-05-05 18:59:20.165623: step 73470, loss = 17.77 (3.3 examples/sec; 4.880 sec/batch)
2018-05-05 19:00:10.044669: step 73480, loss = 18.08 (3.1 examples/sec; 5.098 sec/batch)
2018-05-05 19:00:59.913902: step 73490, loss = 18.01 (3.2 examples/sec; 4.985 sec/batch)
2018-05-05 19:01:49.759632: step 73500, loss = 17.95 (3.2 examples/sec; 4.985 sec/batch)
2018-05-05 19:02:42.148112: step 73510, loss = 17.76 (3.3 examples/sec; 4.901 sec/batch)
2018-05-05 19:03:31.458979: step 73520, loss = 17.94 (3.2 examples/sec; 4.995 sec/batch)
2018-05-05 19:04:20.537050: step 73530, loss = 17.74 (3.3 examples/sec; 4.889 sec/batch)
2018-05-05 19:05:10.123159: step 73540, loss = 17.80 (3.2 examples/sec; 4.983 sec/batch)
2018-05-05 19:05:59.836826: step 73550, loss = 18.11 (3.2 examples/sec; 4.968 sec/batch)
2018-05-05 19:06:49.334967: step 73560, loss = 17.92 (3.2 examples/sec; 5.030 sec/batch)
2018-05-05 19:07:38.999638: step 73570, loss = 17.61 (3.3 examples/sec; 4.813 sec/batch)
2018-05-05 19:08:28.567999: step 73580, loss = 17.72 (3.3 examples/sec; 4.918 sec/batch)
2018-05-05 19:09:18.194528: step 73590, loss = 18.42 (3.2 examples/sec; 5.033 sec/batch)
2018-05-05 19:10:04.753292: step 73600, loss = 18.19 (3.2 examples/sec; 4.970 sec/batch)
2018-05-05 19:10:57.538539: step 73610, loss = 18.11 (3.1 examples/sec; 5.084 sec/batch)
2018-05-05 19:11:47.272134: step 73620, loss = 18.14 (3.2 examples/sec; 5.019 sec/batch)
2018-05-05 19:12:37.006812: step 73630, loss = 17.93 (3.1 examples/sec; 5.149 sec/batch)
2018-05-05 19:13:26.705457: step 73640, loss = 17.74 (3.3 examples/sec; 4.880 sec/batch)
2018-05-05 19:14:16.813012: step 73650, loss = 18.07 (3.2 examples/sec; 4.996 sec/batch)
2018-05-05 19:15:06.752289: step 73660, loss = 17.86 (3.3 examples/sec; 4.882 sec/batch)
2018-05-05 19:15:56.396269: step 73670, loss = 18.18 (3.3 examples/sec; 4.907 sec/batch)
2018-05-05 19:16:46.212207: step 73680, loss = 18.54 (3.2 examples/sec; 5.026 sec/batch)
2018-05-05 19:17:36.133707: step 73690, loss = 17.93 (3.2 examples/sec; 4.994 sec/batch)
2018-05-05 19:18:25.725342: step 73700, loss = 17.70 (3.2 examples/sec; 5.041 sec/batch)
2018-05-05 19:19:18.662462: step 73710, loss = 17.95 (3.3 examples/sec; 4.809 sec/batch)
2018-05-05 19:20:04.902260: step 73720, loss = 17.85 (3.2 examples/sec; 4.964 sec/batch)
2018-05-05 19:20:54.162691: step 73730, loss = 17.83 (3.2 examples/sec; 4.931 sec/batch)
2018-05-05 19:21:43.358215: step 73740, loss = 17.63 (3.3 examples/sec; 4.853 sec/batch)
2018-05-05 19:22:33.067837: step 73750, loss = 18.41 (3.2 examples/sec; 4.964 sec/batch)
2018-05-05 19:23:22.085313: step 73760, loss = 17.94 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 19:24:11.540322: step 73770, loss = 17.68 (3.1 examples/sec; 5.135 sec/batch)
2018-05-05 19:25:00.968683: step 73780, loss = 18.21 (3.2 examples/sec; 4.934 sec/batch)
2018-05-05 19:25:50.627746: step 73790, loss = 18.02 (3.1 examples/sec; 5.149 sec/batch)
2018-05-05 19:26:40.388459: step 73800, loss = 17.62 (3.2 examples/sec; 5.035 sec/batch)
2018-05-05 19:27:33.303766: step 73810, loss = 17.68 (3.2 examples/sec; 5.036 sec/batch)
2018-05-05 19:28:23.430111: step 73820, loss = 17.99 (3.2 examples/sec; 5.016 sec/batch)
2018-05-05 19:29:12.846820: step 73830, loss = 17.88 (3.2 examples/sec; 4.940 sec/batch)
2018-05-05 19:29:59.591116: step 73840, loss = 18.16 (4.2 examples/sec; 3.820 sec/batch)
2018-05-05 19:30:47.356071: step 73850, loss = 18.31 (3.2 examples/sec; 5.016 sec/batch)
2018-05-05 19:31:36.113350: step 73860, loss = 17.70 (3.4 examples/sec; 4.767 sec/batch)
2018-05-05 19:32:24.959674: step 73870, loss = 17.93 (3.3 examples/sec; 4.883 sec/batch)
2018-05-05 19:33:15.013009: step 73880, loss = 17.63 (3.4 examples/sec; 4.739 sec/batch)
2018-05-05 19:34:04.669565: step 73890, loss = 17.88 (3.2 examples/sec; 4.928 sec/batch)
2018-05-05 19:34:54.612886: step 73900, loss = 18.00 (3.1 examples/sec; 5.131 sec/batch)
2018-05-05 19:35:48.853607: step 73910, loss = 17.76 (3.2 examples/sec; 5.049 sec/batch)
2018-05-05 19:36:38.682881: step 73920, loss = 18.69 (3.4 examples/sec; 4.765 sec/batch)
2018-05-05 19:37:27.899866: step 73930, loss = 17.74 (3.2 examples/sec; 5.048 sec/batch)
2018-05-05 19:38:17.698015: step 73940, loss = 17.97 (3.3 examples/sec; 4.855 sec/batch)
2018-05-05 19:39:07.044612: step 73950, loss = 18.57 (3.4 examples/sec; 4.760 sec/batch)
2018-05-05 19:39:56.478069: step 73960, loss = 17.88 (3.2 examples/sec; 5.002 sec/batch)
2018-05-05 19:40:43.008337: step 73970, loss = 18.71 (3.3 examples/sec; 4.884 sec/batch)
2018-05-05 19:41:32.490181: step 73980, loss = 18.18 (3.2 examples/sec; 5.034 sec/batch)
2018-05-05 19:42:22.470875: step 73990, loss = 17.80 (3.2 examples/sec; 5.045 sec/batch)
2018-05-05 19:43:12.552715: step 74000, loss = 17.79 (3.2 examples/sec; 5.040 sec/batch)
2018-05-05 19:44:05.859613: step 74010, loss = 17.74 (3.2 examples/sec; 4.963 sec/batch)
2018-05-05 19:44:55.055964: step 74020, loss = 18.06 (3.3 examples/sec; 4.915 sec/batch)
2018-05-05 19:45:44.388668: step 74030, loss = 18.05 (3.2 examples/sec; 4.953 sec/batch)
2018-05-05 19:46:34.691556: step 74040, loss = 18.02 (3.2 examples/sec; 4.963 sec/batch)
2018-05-05 19:47:23.796876: step 74050, loss = 17.93 (3.3 examples/sec; 4.813 sec/batch)
2018-05-05 19:48:13.295225: step 74060, loss = 18.28 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 19:49:02.628542: step 74070, loss = 18.06 (3.3 examples/sec; 4.779 sec/batch)
2018-05-05 19:49:52.006197: step 74080, loss = 17.84 (3.2 examples/sec; 4.977 sec/batch)
2018-05-05 19:50:37.954680: step 74090, loss = 17.86 (4.1 examples/sec; 3.928 sec/batch)
2018-05-05 19:51:27.048281: step 74100, loss = 18.16 (3.3 examples/sec; 4.872 sec/batch)
2018-05-05 19:52:20.189695: step 74110, loss = 17.77 (3.2 examples/sec; 4.933 sec/batch)
2018-05-05 19:53:08.826287: step 74120, loss = 18.03 (3.3 examples/sec; 4.785 sec/batch)
2018-05-05 19:53:58.459439: step 74130, loss = 18.00 (3.2 examples/sec; 5.079 sec/batch)
2018-05-05 19:54:47.268044: step 74140, loss = 18.20 (3.3 examples/sec; 4.849 sec/batch)
2018-05-05 19:55:36.234816: step 74150, loss = 18.15 (3.2 examples/sec; 4.934 sec/batch)
2018-05-05 19:56:25.475639: step 74160, loss = 17.65 (3.2 examples/sec; 4.963 sec/batch)
2018-05-05 19:57:14.696183: step 74170, loss = 17.78 (3.2 examples/sec; 5.009 sec/batch)
2018-05-05 19:58:03.792958: step 74180, loss = 17.93 (3.3 examples/sec; 4.790 sec/batch)
2018-05-05 19:58:53.517139: step 74190, loss = 17.95 (3.3 examples/sec; 4.869 sec/batch)
2018-05-05 19:59:42.900481: step 74200, loss = 17.98 (3.2 examples/sec; 5.017 sec/batch)
2018-05-05 20:00:36.213021: step 74210, loss = 17.94 (3.2 examples/sec; 4.925 sec/batch)
2018-05-05 20:01:22.113798: step 74220, loss = 18.06 (3.2 examples/sec; 4.993 sec/batch)
2018-05-05 20:02:11.466027: step 74230, loss = 18.37 (3.2 examples/sec; 5.014 sec/batch)
2018-05-05 20:03:00.828759: step 74240, loss = 17.59 (3.3 examples/sec; 4.848 sec/batch)
2018-05-05 20:03:50.066755: step 74250, loss = 18.26 (3.3 examples/sec; 4.876 sec/batch)
2018-05-05 20:04:39.246576: step 74260, loss = 18.04 (3.2 examples/sec; 4.994 sec/batch)
2018-05-05 20:05:28.387450: step 74270, loss = 18.24 (3.2 examples/sec; 4.958 sec/batch)
2018-05-05 20:06:17.293513: step 74280, loss = 18.09 (3.1 examples/sec; 5.118 sec/batch)
2018-05-05 20:07:07.179039: step 74290, loss = 17.78 (3.3 examples/sec; 4.920 sec/batch)
2018-05-05 20:07:57.182029: step 74300, loss = 18.52 (3.2 examples/sec; 4.945 sec/batch)
2018-05-05 20:08:50.093278: step 74310, loss = 18.21 (3.1 examples/sec; 5.095 sec/batch)
2018-05-05 20:09:39.995590: step 74320, loss = 18.08 (3.3 examples/sec; 4.919 sec/batch)
2018-05-05 20:10:29.260426: step 74330, loss = 18.13 (3.3 examples/sec; 4.819 sec/batch)
2018-05-05 20:11:17.101217: step 74340, loss = 18.12 (4.4 examples/sec; 3.601 sec/batch)
2018-05-05 20:12:04.351335: step 74350, loss = 17.97 (3.2 examples/sec; 4.983 sec/batch)
2018-05-05 20:12:53.271218: step 74360, loss = 18.15 (3.3 examples/sec; 4.805 sec/batch)
2018-05-05 20:13:42.465249: step 74370, loss = 17.61 (3.2 examples/sec; 4.981 sec/batch)
2018-05-05 20:14:32.218490: step 74380, loss = 17.95 (3.1 examples/sec; 5.118 sec/batch)
2018-05-05 20:15:21.828948: step 74390, loss = 17.64 (3.2 examples/sec; 4.924 sec/batch)
2018-05-05 20:16:10.992788: step 74400, loss = 18.19 (3.3 examples/sec; 4.840 sec/batch)
2018-05-05 20:17:04.396639: step 74410, loss = 18.51 (3.2 examples/sec; 5.048 sec/batch)
2018-05-05 20:17:54.511926: step 74420, loss = 18.08 (3.2 examples/sec; 5.007 sec/batch)
2018-05-05 20:18:44.011339: step 74430, loss = 18.09 (3.2 examples/sec; 5.052 sec/batch)
2018-05-05 20:19:34.066001: step 74440, loss = 18.30 (3.2 examples/sec; 4.934 sec/batch)
2018-05-05 20:20:24.339467: step 74450, loss = 18.55 (3.3 examples/sec; 4.906 sec/batch)
2018-05-05 20:21:13.832727: step 74460, loss = 17.71 (3.3 examples/sec; 4.784 sec/batch)
2018-05-05 20:22:00.288234: step 74470, loss = 18.30 (3.2 examples/sec; 4.924 sec/batch)
2018-05-05 20:22:50.483756: step 74480, loss = 17.94 (3.2 examples/sec; 5.042 sec/batch)
2018-05-05 20:23:40.420079: step 74490, loss = 17.82 (3.2 examples/sec; 4.944 sec/batch)
2018-05-05 20:24:29.935991: step 74500, loss = 17.78 (3.2 examples/sec; 4.981 sec/batch)
2018-05-05 20:25:23.105253: step 74510, loss = 17.75 (3.2 examples/sec; 4.960 sec/batch)
2018-05-05 20:26:12.528559: step 74520, loss = 18.35 (3.2 examples/sec; 4.969 sec/batch)
2018-05-05 20:27:01.542486: step 74530, loss = 17.87 (3.3 examples/sec; 4.904 sec/batch)
2018-05-05 20:27:50.446302: step 74540, loss = 17.78 (3.4 examples/sec; 4.728 sec/batch)
2018-05-05 20:28:39.916568: step 74550, loss = 18.25 (3.3 examples/sec; 4.915 sec/batch)
2018-05-05 20:29:29.678215: step 74560, loss = 18.47 (3.2 examples/sec; 4.991 sec/batch)
2018-05-05 20:30:19.179554: step 74570, loss = 18.45 (3.2 examples/sec; 5.052 sec/batch)
2018-05-05 20:31:08.877389: step 74580, loss = 18.00 (3.2 examples/sec; 5.018 sec/batch)
2018-05-05 20:31:55.061485: step 74590, loss = 18.31 (4.2 examples/sec; 3.775 sec/batch)
2018-05-05 20:32:44.145011: step 74600, loss = 17.87 (3.3 examples/sec; 4.797 sec/batch)
2018-05-05 20:33:36.798320: step 74610, loss = 17.88 (3.3 examples/sec; 4.911 sec/batch)
2018-05-05 20:34:26.378609: step 74620, loss = 18.14 (3.2 examples/sec; 4.978 sec/batch)
2018-05-05 20:35:15.573310: step 74630, loss = 17.65 (3.4 examples/sec; 4.770 sec/batch)
2018-05-05 20:36:04.851953: step 74640, loss = 18.13 (3.2 examples/sec; 5.018 sec/batch)
2018-05-05 20:36:54.189856: step 74650, loss = 18.21 (3.2 examples/sec; 4.926 sec/batch)
2018-05-05 20:37:43.413410: step 74660, loss = 18.16 (3.2 examples/sec; 4.975 sec/batch)
2018-05-05 20:38:32.849277: step 74670, loss = 17.99 (3.2 examples/sec; 4.929 sec/batch)
2018-05-05 20:39:22.107134: step 74680, loss = 17.95 (3.3 examples/sec; 4.785 sec/batch)
2018-05-05 20:40:11.033895: step 74690, loss = 18.10 (3.3 examples/sec; 4.888 sec/batch)
2018-05-05 20:41:00.234370: step 74700, loss = 18.22 (3.2 examples/sec; 5.000 sec/batch)
2018-05-05 20:41:52.680624: step 74710, loss = 18.55 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 20:42:38.414038: step 74720, loss = 18.21 (3.2 examples/sec; 4.941 sec/batch)
2018-05-05 20:43:27.778036: step 74730, loss = 18.18 (3.2 examples/sec; 5.000 sec/batch)
2018-05-05 20:44:16.913850: step 74740, loss = 17.97 (3.3 examples/sec; 4.920 sec/batch)
2018-05-05 20:45:06.443027: step 74750, loss = 17.88 (3.2 examples/sec; 4.949 sec/batch)
2018-05-05 20:45:55.737007: step 74760, loss = 18.17 (3.3 examples/sec; 4.893 sec/batch)
2018-05-05 20:46:44.906069: step 74770, loss = 18.31 (3.3 examples/sec; 4.909 sec/batch)
2018-05-05 20:47:34.620051: step 74780, loss = 18.17 (3.1 examples/sec; 5.099 sec/batch)
2018-05-05 20:48:24.405037: step 74790, loss = 17.95 (3.1 examples/sec; 5.089 sec/batch)
2018-05-05 20:49:13.755020: step 74800, loss = 17.77 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 20:50:06.453147: step 74810, loss = 18.20 (3.2 examples/sec; 4.948 sec/batch)
2018-05-05 20:50:55.470687: step 74820, loss = 17.81 (3.3 examples/sec; 4.866 sec/batch)
2018-05-05 20:51:45.220927: step 74830, loss = 17.87 (3.1 examples/sec; 5.081 sec/batch)
2018-05-05 20:52:32.793055: step 74840, loss = 18.00 (4.2 examples/sec; 3.822 sec/batch)
2018-05-05 20:53:20.366171: step 74850, loss = 17.70 (3.0 examples/sec; 5.374 sec/batch)
2018-05-05 20:54:09.779550: step 74860, loss = 17.60 (3.2 examples/sec; 5.015 sec/batch)
2018-05-05 20:54:59.398949: step 74870, loss = 17.88 (3.4 examples/sec; 4.774 sec/batch)
2018-05-05 20:55:48.580358: step 74880, loss = 18.67 (3.3 examples/sec; 4.782 sec/batch)
2018-05-05 20:56:38.345980: step 74890, loss = 17.92 (3.3 examples/sec; 4.898 sec/batch)
2018-05-05 20:57:28.097375: step 74900, loss = 17.86 (3.2 examples/sec; 4.988 sec/batch)
2018-05-05 20:58:21.312445: step 74910, loss = 18.58 (3.2 examples/sec; 4.939 sec/batch)
2018-05-05 20:59:10.853421: step 74920, loss = 17.88 (3.3 examples/sec; 4.839 sec/batch)
2018-05-05 21:00:00.781267: step 74930, loss = 18.40 (3.1 examples/sec; 5.155 sec/batch)
2018-05-05 21:00:50.482112: step 74940, loss = 17.66 (3.1 examples/sec; 5.103 sec/batch)
2018-05-05 21:01:40.202614: step 74950, loss = 17.94 (3.1 examples/sec; 5.118 sec/batch)
2018-05-05 21:02:29.295844: step 74960, loss = 18.78 (3.3 examples/sec; 4.839 sec/batch)
2018-05-05 21:03:15.468824: step 74970, loss = 18.27 (3.2 examples/sec; 4.942 sec/batch)
2018-05-05 21:04:04.678780: step 74980, loss = 17.69 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 21:04:54.230600: step 74990, loss = 17.68 (3.3 examples/sec; 4.908 sec/batch)
2018-05-05 21:05:43.794305: step 75000, loss = 17.92 (3.3 examples/sec; 4.901 sec/batch)
2018-05-05 21:06:36.933071: step 75010, loss = 18.58 (3.3 examples/sec; 4.812 sec/batch)
2018-05-05 21:07:26.342778: step 75020, loss = 17.77 (3.2 examples/sec; 4.953 sec/batch)
2018-05-05 21:08:15.850711: step 75030, loss = 18.01 (3.0 examples/sec; 5.402 sec/batch)
2018-05-05 21:09:05.874938: step 75040, loss = 17.98 (3.2 examples/sec; 4.938 sec/batch)
2018-05-05 21:09:55.339032: step 75050, loss = 18.45 (3.2 examples/sec; 5.000 sec/batch)
2018-05-05 21:10:44.993688: step 75060, loss = 17.67 (3.2 examples/sec; 5.038 sec/batch)
2018-05-05 21:11:34.978091: step 75070, loss = 18.01 (3.1 examples/sec; 5.109 sec/batch)
2018-05-05 21:12:24.291467: step 75080, loss = 17.98 (3.3 examples/sec; 4.874 sec/batch)
2018-05-05 21:13:12.008456: step 75090, loss = 18.53 (4.3 examples/sec; 3.763 sec/batch)
2018-05-05 21:13:59.870045: step 75100, loss = 18.37 (3.2 examples/sec; 5.023 sec/batch)
2018-05-05 21:14:52.227732: step 75110, loss = 18.23 (3.3 examples/sec; 4.863 sec/batch)
2018-05-05 21:15:42.227289: step 75120, loss = 17.90 (3.2 examples/sec; 4.932 sec/batch)
2018-05-05 21:16:31.915099: step 75130, loss = 18.46 (3.3 examples/sec; 4.906 sec/batch)
2018-05-05 21:17:21.104385: step 75140, loss = 18.44 (3.3 examples/sec; 4.861 sec/batch)
2018-05-05 21:18:10.657031: step 75150, loss = 18.10 (3.3 examples/sec; 4.856 sec/batch)
2018-05-05 21:18:59.761315: step 75160, loss = 18.04 (3.2 examples/sec; 4.952 sec/batch)
2018-05-05 21:19:48.848226: step 75170, loss = 18.27 (3.2 examples/sec; 4.947 sec/batch)
2018-05-05 21:20:37.447990: step 75180, loss = 17.66 (3.2 examples/sec; 4.987 sec/batch)
2018-05-05 21:21:26.569894: step 75190, loss = 17.95 (3.3 examples/sec; 4.851 sec/batch)
2018-05-05 21:22:15.857593: step 75200, loss = 17.95 (3.2 examples/sec; 4.974 sec/batch)
2018-05-05 21:23:08.912791: step 75210, loss = 17.62 (3.3 examples/sec; 4.808 sec/batch)
2018-05-05 21:23:54.391083: step 75220, loss = 17.81 (3.2 examples/sec; 5.009 sec/batch)
2018-05-05 21:24:44.075120: step 75230, loss = 17.61 (3.2 examples/sec; 4.959 sec/batch)
2018-05-05 21:25:32.824069: step 75240, loss = 18.68 (3.2 examples/sec; 4.931 sec/batch)
2018-05-05 21:26:21.951808: step 75250, loss = 18.50 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 21:27:12.376851: step 75260, loss = 17.93 (3.2 examples/sec; 5.009 sec/batch)
2018-05-05 21:28:01.866639: step 75270, loss = 18.18 (3.2 examples/sec; 4.974 sec/batch)
2018-05-05 21:28:51.408118: step 75280, loss = 18.28 (3.1 examples/sec; 5.106 sec/batch)
2018-05-05 21:29:40.452253: step 75290, loss = 18.00 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 21:30:30.206609: step 75300, loss = 18.15 (3.2 examples/sec; 5.058 sec/batch)
2018-05-05 21:31:23.728560: step 75310, loss = 18.09 (3.1 examples/sec; 5.105 sec/batch)
2018-05-05 21:32:13.542665: step 75320, loss = 17.80 (3.2 examples/sec; 5.039 sec/batch)
2018-05-05 21:33:03.017775: step 75330, loss = 17.75 (3.2 examples/sec; 5.066 sec/batch)
2018-05-05 21:33:50.993966: step 75340, loss = 17.91 (4.1 examples/sec; 3.897 sec/batch)
2018-05-05 21:34:38.008520: step 75350, loss = 17.67 (3.2 examples/sec; 5.016 sec/batch)
2018-05-05 21:35:27.351551: step 75360, loss = 17.62 (3.4 examples/sec; 4.688 sec/batch)
2018-05-05 21:36:16.134942: step 75370, loss = 17.96 (3.3 examples/sec; 4.810 sec/batch)
2018-05-05 21:37:05.573078: step 75380, loss = 17.95 (3.2 examples/sec; 5.016 sec/batch)
2018-05-05 21:37:54.502913: step 75390, loss = 18.05 (3.3 examples/sec; 4.839 sec/batch)
2018-05-05 21:38:43.375151: step 75400, loss = 18.06 (3.3 examples/sec; 4.897 sec/batch)
2018-05-05 21:39:36.417435: step 75410, loss = 18.29 (3.2 examples/sec; 4.989 sec/batch)
2018-05-05 21:40:25.572099: step 75420, loss = 18.32 (3.3 examples/sec; 4.912 sec/batch)
2018-05-05 21:41:14.683201: step 75430, loss = 19.09 (3.1 examples/sec; 5.086 sec/batch)
2018-05-05 21:42:03.995989: step 75440, loss = 17.81 (3.3 examples/sec; 4.856 sec/batch)
2018-05-05 21:42:53.326127: step 75450, loss = 18.06 (3.3 examples/sec; 4.903 sec/batch)
2018-05-05 21:43:42.609002: step 75460, loss = 17.84 (3.3 examples/sec; 4.822 sec/batch)
2018-05-05 21:44:28.230775: step 75470, loss = 17.88 (3.3 examples/sec; 4.911 sec/batch)
2018-05-05 21:45:17.922785: step 75480, loss = 17.88 (3.2 examples/sec; 5.057 sec/batch)
2018-05-05 21:46:07.243029: step 75490, loss = 18.04 (3.4 examples/sec; 4.773 sec/batch)
2018-05-05 21:46:56.497588: step 75500, loss = 18.15 (3.3 examples/sec; 4.905 sec/batch)
2018-05-05 21:47:49.156727: step 75510, loss = 18.35 (3.3 examples/sec; 4.850 sec/batch)
2018-05-05 21:48:38.411021: step 75520, loss = 18.35 (3.3 examples/sec; 4.867 sec/batch)
2018-05-05 21:49:28.026350: step 75530, loss = 18.02 (3.3 examples/sec; 4.913 sec/batch)
2018-05-05 21:50:18.209300: step 75540, loss = 17.85 (3.3 examples/sec; 4.815 sec/batch)
2018-05-05 21:51:07.558236: step 75550, loss = 18.11 (3.2 examples/sec; 4.978 sec/batch)
2018-05-05 21:51:56.984451: step 75560, loss = 17.90 (3.4 examples/sec; 4.753 sec/batch)
2018-05-05 21:52:46.134718: step 75570, loss = 18.20 (3.3 examples/sec; 4.793 sec/batch)
2018-05-05 21:53:35.513895: step 75580, loss = 17.91 (3.3 examples/sec; 4.871 sec/batch)
2018-05-05 21:54:24.770826: step 75590, loss = 17.88 (3.3 examples/sec; 4.844 sec/batch)
2018-05-05 21:55:10.801211: step 75600, loss = 18.56 (3.4 examples/sec; 4.732 sec/batch)
2018-05-05 21:56:03.395070: step 75610, loss = 17.63 (3.2 examples/sec; 5.037 sec/batch)
2018-05-05 21:56:53.136382: step 75620, loss = 18.33 (3.3 examples/sec; 4.917 sec/batch)
2018-05-05 21:57:42.572311: step 75630, loss = 17.68 (3.3 examples/sec; 4.896 sec/batch)
2018-05-05 21:58:32.409061: step 75640, loss = 17.89 (3.3 examples/sec; 4.916 sec/batch)
2018-05-05 21:59:21.774128: step 75650, loss = 18.12 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 22:00:11.127830: step 75660, loss = 17.89 (3.2 examples/sec; 5.053 sec/batch)
2018-05-05 22:01:00.005265: step 75670, loss = 18.71 (3.2 examples/sec; 4.924 sec/batch)
2018-05-05 22:01:49.635501: step 75680, loss = 17.71 (3.2 examples/sec; 5.037 sec/batch)
2018-05-05 22:02:38.849463: step 75690, loss = 18.47 (3.2 examples/sec; 4.940 sec/batch)
2018-05-05 22:03:28.187069: step 75700, loss = 17.74 (3.2 examples/sec; 5.062 sec/batch)
2018-05-05 22:04:22.044903: step 75710, loss = 18.11 (3.2 examples/sec; 4.951 sec/batch)
2018-05-05 22:05:08.578570: step 75720, loss = 18.30 (3.2 examples/sec; 4.974 sec/batch)
2018-05-05 22:05:57.775005: step 75730, loss = 17.61 (3.2 examples/sec; 4.974 sec/batch)
2018-05-05 22:06:47.230748: step 75740, loss = 17.94 (3.2 examples/sec; 4.984 sec/batch)
2018-05-05 22:07:36.999435: step 75750, loss = 17.66 (3.3 examples/sec; 4.877 sec/batch)
2018-05-05 22:08:26.433535: step 75760, loss = 17.82 (3.3 examples/sec; 4.842 sec/batch)
2018-05-05 22:09:15.961331: step 75770, loss = 17.97 (3.3 examples/sec; 4.829 sec/batch)
2018-05-05 22:10:06.211552: step 75780, loss = 18.13 (3.3 examples/sec; 4.811 sec/batch)
2018-05-05 22:10:56.272604: step 75790, loss = 17.96 (3.2 examples/sec; 5.071 sec/batch)
2018-05-05 22:11:45.340815: step 75800, loss = 18.34 (3.2 examples/sec; 4.980 sec/batch)
2018-05-05 22:12:38.073121: step 75810, loss = 17.62 (3.3 examples/sec; 4.794 sec/batch)
2018-05-05 22:13:27.081134: step 75820, loss = 18.17 (3.3 examples/sec; 4.795 sec/batch)
2018-05-05 22:14:16.467971: step 75830, loss = 17.61 (3.3 examples/sec; 4.900 sec/batch)
2018-05-05 22:15:05.672787: step 75840, loss = 18.01 (3.3 examples/sec; 4.783 sec/batch)
2018-05-05 22:15:51.897387: step 75850, loss = 17.64 (3.2 examples/sec; 5.065 sec/batch)
2018-05-05 22:16:41.071371: step 75860, loss = 18.17 (3.2 examples/sec; 5.019 sec/batch)
2018-05-05 22:17:30.313985: step 75870, loss = 18.04 (3.3 examples/sec; 4.849 sec/batch)
2018-05-05 22:18:20.014344: step 75880, loss = 17.88 (3.2 examples/sec; 5.007 sec/batch)
2018-05-05 22:19:09.581641: step 75890, loss = 17.89 (3.2 examples/sec; 4.993 sec/batch)
2018-05-05 22:19:59.563021: step 75900, loss = 17.77 (3.1 examples/sec; 5.112 sec/batch)
2018-05-05 22:20:52.292447: step 75910, loss = 18.70 (3.3 examples/sec; 4.914 sec/batch)
2018-05-05 22:21:41.799765: step 75920, loss = 17.91 (3.2 examples/sec; 5.035 sec/batch)
2018-05-05 22:22:31.511736: step 75930, loss = 17.85 (3.3 examples/sec; 4.912 sec/batch)
2018-05-05 22:23:21.055471: step 75940, loss = 18.16 (3.3 examples/sec; 4.883 sec/batch)
2018-05-05 22:24:10.434500: step 75950, loss = 17.62 (3.2 examples/sec; 4.945 sec/batch)
2018-05-05 22:25:00.113554: step 75960, loss = 17.94 (3.2 examples/sec; 4.936 sec/batch)
2018-05-05 22:25:46.131992: step 75970, loss = 18.44 (3.3 examples/sec; 4.881 sec/batch)
2018-05-05 22:26:34.933469: step 75980, loss = 18.18 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 22:27:24.489774: step 75990, loss = 18.44 (3.3 examples/sec; 4.818 sec/batch)
2018-05-05 22:28:13.451112: step 76000, loss = 18.10 (3.3 examples/sec; 4.842 sec/batch)
2018-05-05 22:29:06.617112: step 76010, loss = 18.13 (3.2 examples/sec; 5.012 sec/batch)
2018-05-05 22:29:55.905368: step 76020, loss = 18.10 (3.1 examples/sec; 5.093 sec/batch)
2018-05-05 22:30:45.582705: step 76030, loss = 18.06 (3.2 examples/sec; 5.017 sec/batch)
2018-05-05 22:31:35.019013: step 76040, loss = 18.18 (3.2 examples/sec; 4.946 sec/batch)
2018-05-05 22:32:24.131885: step 76050, loss = 17.93 (3.3 examples/sec; 4.842 sec/batch)
2018-05-05 22:33:13.910522: step 76060, loss = 17.88 (3.3 examples/sec; 4.898 sec/batch)
2018-05-05 22:34:03.146779: step 76070, loss = 17.82 (3.3 examples/sec; 4.867 sec/batch)
2018-05-05 22:34:52.419535: step 76080, loss = 17.79 (3.1 examples/sec; 5.095 sec/batch)
2018-05-05 22:35:41.692686: step 76090, loss = 17.90 (3.3 examples/sec; 4.910 sec/batch)
2018-05-05 22:36:27.583779: step 76100, loss = 18.27 (3.3 examples/sec; 4.876 sec/batch)
2018-05-05 22:37:20.738723: step 76110, loss = 17.73 (3.1 examples/sec; 5.106 sec/batch)
2018-05-05 22:38:10.223491: step 76120, loss = 17.97 (3.3 examples/sec; 4.871 sec/batch)
2018-05-05 22:38:59.845595: step 76130, loss = 17.90 (3.1 examples/sec; 5.115 sec/batch)
2018-05-05 22:39:48.830868: step 76140, loss = 18.30 (3.2 examples/sec; 4.950 sec/batch)
2018-05-05 22:40:37.432085: step 76150, loss = 18.03 (3.3 examples/sec; 4.796 sec/batch)
2018-05-05 22:41:27.221639: step 76160, loss = 18.25 (3.2 examples/sec; 4.997 sec/batch)
2018-05-05 22:42:16.586960: step 76170, loss = 17.90 (3.3 examples/sec; 4.864 sec/batch)
2018-05-05 22:43:05.683279: step 76180, loss = 17.61 (3.3 examples/sec; 4.902 sec/batch)
2018-05-05 22:43:55.521881: step 76190, loss = 17.96 (3.1 examples/sec; 5.172 sec/batch)
2018-05-05 22:44:44.926612: step 76200, loss = 18.53 (3.2 examples/sec; 4.946 sec/batch)
2018-05-05 22:45:37.331108: step 76210, loss = 17.93 (3.2 examples/sec; 4.938 sec/batch)
2018-05-05 22:46:23.640187: step 76220, loss = 17.86 (3.2 examples/sec; 5.070 sec/batch)
2018-05-05 22:47:13.128505: step 76230, loss = 17.88 (3.2 examples/sec; 5.029 sec/batch)
2018-05-05 22:48:01.681630: step 76240, loss = 18.33 (3.2 examples/sec; 5.027 sec/batch)
2018-05-05 22:48:51.624537: step 76250, loss = 17.80 (3.3 examples/sec; 4.854 sec/batch)
2018-05-05 22:49:40.607858: step 76260, loss = 18.49 (3.2 examples/sec; 4.961 sec/batch)
2018-05-05 22:50:30.402124: step 76270, loss = 18.29 (3.3 examples/sec; 4.865 sec/batch)
2018-05-05 22:51:19.579972: step 76280, loss = 18.43 (3.2 examples/sec; 4.978 sec/batch)
2018-05-05 22:52:08.683113: step 76290, loss = 18.36 (3.2 examples/sec; 5.055 sec/batch)
2018-05-05 22:52:58.129530: step 76300, loss = 18.21 (3.2 examples/sec; 5.044 sec/batch)
2018-05-05 22:53:51.459265: step 76310, loss = 18.79 (3.1 examples/sec; 5.242 sec/batch)
2018-05-05 22:54:41.331055: step 76320, loss = 17.79 (3.2 examples/sec; 5.001 sec/batch)
2018-05-05 22:55:30.489724: step 76330, loss = 17.80 (3.2 examples/sec; 4.948 sec/batch)
2018-05-05 22:56:19.502895: step 76340, loss = 18.09 (3.3 examples/sec; 4.845 sec/batch)
2018-05-05 22:57:05.030420: step 76350, loss = 18.92 (3.3 examples/sec; 4.851 sec/batch)
2018-05-05 22:57:54.408683: step 76360, loss = 18.49 (3.2 examples/sec; 5.009 sec/batch)
2018-05-05 22:58:43.804559: step 76370, loss = 17.82 (3.2 examples/sec; 4.998 sec/batch)
2018-05-05 22:59:33.927702: step 76380, loss = 17.96 (3.2 examples/sec; 4.951 sec/batch)
2018-05-05 23:00:23.763221: step 76390, loss = 17.82 (3.2 examples/sec; 4.992 sec/batch)
2018-05-05 23:01:13.342356: step 76400, loss = 18.02 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 23:02:06.815437: step 76410, loss = 18.39 (3.2 examples/sec; 4.975 sec/batch)
2018-05-05 23:02:56.535199: step 76420, loss = 17.94 (3.2 examples/sec; 4.997 sec/batch)
2018-05-05 23:03:46.790775: step 76430, loss = 18.47 (3.2 examples/sec; 4.955 sec/batch)
2018-05-05 23:04:36.384457: step 76440, loss = 18.29 (3.2 examples/sec; 5.032 sec/batch)
2018-05-05 23:05:25.857701: step 76450, loss = 17.68 (3.1 examples/sec; 5.165 sec/batch)
2018-05-05 23:06:14.432213: step 76460, loss = 17.77 (3.3 examples/sec; 4.887 sec/batch)
2018-05-05 23:07:00.029599: step 76470, loss = 17.99 (3.3 examples/sec; 4.835 sec/batch)
2018-05-05 23:07:49.239974: step 76480, loss = 17.97 (3.2 examples/sec; 4.973 sec/batch)
2018-05-05 23:08:38.353504: step 76490, loss = 17.76 (3.1 examples/sec; 5.140 sec/batch)
2018-05-05 23:09:27.118352: step 76500, loss = 17.98 (3.3 examples/sec; 4.835 sec/batch)
2018-05-05 23:10:20.060090: step 76510, loss = 17.86 (3.3 examples/sec; 4.827 sec/batch)
2018-05-05 23:11:09.554955: step 76520, loss = 17.82 (3.2 examples/sec; 5.004 sec/batch)
2018-05-05 23:11:58.908431: step 76530, loss = 18.03 (3.3 examples/sec; 4.820 sec/batch)
2018-05-05 23:12:48.701949: step 76540, loss = 17.58 (3.2 examples/sec; 4.936 sec/batch)
2018-05-05 23:13:37.631987: step 76550, loss = 18.20 (3.3 examples/sec; 4.875 sec/batch)
2018-05-05 23:14:27.204137: step 76560, loss = 17.94 (3.1 examples/sec; 5.194 sec/batch)
2018-05-05 23:15:16.612387: step 76570, loss = 17.81 (3.3 examples/sec; 4.827 sec/batch)
2018-05-05 23:16:06.545054: step 76580, loss = 17.94 (3.1 examples/sec; 5.085 sec/batch)
2018-05-05 23:16:55.643149: step 76590, loss = 17.67 (3.3 examples/sec; 4.836 sec/batch)
2018-05-05 23:17:41.471114: step 76600, loss = 18.08 (3.3 examples/sec; 4.806 sec/batch)
2018-05-05 23:18:35.373248: step 76610, loss = 17.89 (3.2 examples/sec; 5.034 sec/batch)
2018-05-05 23:19:24.334066: step 76620, loss = 18.10 (3.3 examples/sec; 4.918 sec/batch)
2018-05-05 23:20:13.446676: step 76630, loss = 17.76 (3.2 examples/sec; 4.946 sec/batch)
2018-05-05 23:21:03.349434: step 76640, loss = 17.79 (3.1 examples/sec; 5.092 sec/batch)
2018-05-05 23:21:52.539721: step 76650, loss = 17.82 (3.2 examples/sec; 4.956 sec/batch)
2018-05-05 23:22:41.941906: step 76660, loss = 18.22 (3.2 examples/sec; 4.979 sec/batch)
2018-05-05 23:23:30.884264: step 76670, loss = 17.94 (3.2 examples/sec; 4.925 sec/batch)
2018-05-05 23:24:20.508622: step 76680, loss = 18.29 (3.3 examples/sec; 4.896 sec/batch)
2018-05-05 23:25:09.626311: step 76690, loss = 17.88 (3.3 examples/sec; 4.825 sec/batch)
2018-05-05 23:25:59.194882: step 76700, loss = 18.01 (3.3 examples/sec; 4.821 sec/batch)
2018-05-05 23:26:52.432422: step 76710, loss = 18.62 (3.3 examples/sec; 4.887 sec/batch)
2018-05-05 23:27:38.641653: step 76720, loss = 18.84 (3.2 examples/sec; 5.042 sec/batch)
2018-05-05 23:28:28.145380: step 76730, loss = 18.46 (3.4 examples/sec; 4.742 sec/batch)
2018-05-05 23:29:17.407903: step 76740, loss = 17.86 (3.3 examples/sec; 4.788 sec/batch)
2018-05-05 23:30:06.359269: step 76750, loss = 18.12 (3.2 examples/sec; 5.063 sec/batch)
2018-05-05 23:30:56.390001: step 76760, loss = 17.75 (3.2 examples/sec; 4.959 sec/batch)
2018-05-05 23:31:45.787185: step 76770, loss = 18.04 (3.2 examples/sec; 4.984 sec/batch)
2018-05-05 23:32:35.028148: step 76780, loss = 18.51 (3.3 examples/sec; 4.891 sec/batch)
2018-05-05 23:33:24.157636: step 76790, loss = 18.01 (3.2 examples/sec; 4.941 sec/batch)
2018-05-05 23:34:12.833590: step 76800, loss = 18.26 (3.3 examples/sec; 4.841 sec/batch)
2018-05-05 23:35:05.706854: step 76810, loss = 17.80 (3.3 examples/sec; 4.874 sec/batch)
2018-05-05 23:35:55.278896: step 76820, loss = 18.04 (3.3 examples/sec; 4.833 sec/batch)
2018-05-05 23:36:44.609623: step 76830, loss = 17.80 (3.2 examples/sec; 5.013 sec/batch)
2018-05-05 23:37:33.912123: step 76840, loss = 18.06 (3.5 examples/sec; 4.511 sec/batch)
2018-05-05 23:38:19.385785: step 76850, loss = 18.33 (3.2 examples/sec; 4.991 sec/batch)
2018-05-05 23:39:08.907792: step 76860, loss = 17.97 (3.2 examples/sec; 5.075 sec/batch)
2018-05-05 23:39:58.829397: step 76870, loss = 18.04 (3.2 examples/sec; 4.941 sec/batch)
2018-05-05 23:40:48.922066: step 76880, loss = 18.16 (3.2 examples/sec; 5.010 sec/batch)
2018-05-05 23:41:38.465575: step 76890, loss = 17.91 (3.1 examples/sec; 5.100 sec/batch)
2018-05-05 23:42:27.316600: step 76900, loss = 17.71 (3.3 examples/sec; 4.822 sec/batch)
2018-05-05 23:43:20.323493: step 76910, loss = 18.08 (3.3 examples/sec; 4.880 sec/batch)
2018-05-05 23:44:09.389953: step 76920, loss = 18.19 (3.2 examples/sec; 5.009 sec/batch)
2018-05-05 23:44:59.642099: step 76930, loss = 17.99 (3.0 examples/sec; 5.250 sec/batch)
2018-05-05 23:45:49.358250: step 76940, loss = 18.08 (3.2 examples/sec; 5.057 sec/batch)
2018-05-05 23:46:38.917553: step 76950, loss = 17.87 (3.2 examples/sec; 4.971 sec/batch)
2018-05-05 23:47:29.449643: step 76960, loss = 17.79 (3.1 examples/sec; 5.186 sec/batch)
2018-05-05 23:48:15.750286: step 76970, loss = 17.86 (3.2 examples/sec; 4.964 sec/batch)
2018-05-05 23:49:05.619814: step 76980, loss = 17.65 (3.3 examples/sec; 4.860 sec/batch)
2018-05-05 23:49:54.939074: step 76990, loss = 18.17 (3.3 examples/sec; 4.913 sec/batch)
2018-05-05 23:50:44.355337: step 77000, loss = 18.06 (3.2 examples/sec; 5.037 sec/batch)
2018-05-05 23:51:37.499505: step 77010, loss = 18.21 (3.0 examples/sec; 5.297 sec/batch)
2018-05-05 23:52:26.314508: step 77020, loss = 17.75 (3.2 examples/sec; 5.040 sec/batch)
2018-05-05 23:53:15.973871: step 77030, loss = 17.75 (3.2 examples/sec; 5.044 sec/batch)
2018-05-05 23:54:05.808627: step 77040, loss = 18.13 (3.2 examples/sec; 4.955 sec/batch)
2018-05-05 23:54:55.557230: step 77050, loss = 18.27 (3.1 examples/sec; 5.114 sec/batch)
2018-05-05 23:55:44.950198: step 77060, loss = 17.90 (3.2 examples/sec; 5.047 sec/batch)
2018-05-05 23:56:34.674468: step 77070, loss = 17.92 (3.2 examples/sec; 5.072 sec/batch)
2018-05-05 23:57:24.208912: step 77080, loss = 17.75 (3.3 examples/sec; 4.777 sec/batch)
2018-05-05 23:58:12.845008: step 77090, loss = 17.90 (4.0 examples/sec; 4.049 sec/batch)
2018-05-05 23:58:59.688678: step 77100, loss = 18.23 (3.3 examples/sec; 4.896 sec/batch)
2018-05-05 23:59:52.361371: step 77110, loss = 18.05 (3.2 examples/sec; 5.007 sec/batch)
2018-05-06 00:00:41.688792: step 77120, loss = 17.84 (3.3 examples/sec; 4.912 sec/batch)
2018-05-06 00:01:31.075540: step 77130, loss = 17.66 (3.3 examples/sec; 4.901 sec/batch)
2018-05-06 00:02:20.555754: step 77140, loss = 17.78 (3.2 examples/sec; 4.948 sec/batch)
2018-05-06 00:03:10.230582: step 77150, loss = 17.57 (3.2 examples/sec; 4.964 sec/batch)
2018-05-06 00:03:59.398265: step 77160, loss = 17.87 (3.2 examples/sec; 5.045 sec/batch)
2018-05-06 00:04:48.991808: step 77170, loss = 17.59 (3.2 examples/sec; 5.054 sec/batch)
2018-05-06 00:05:38.830555: step 77180, loss = 17.87 (3.2 examples/sec; 5.002 sec/batch)
2018-05-06 00:06:28.532601: step 77190, loss = 18.13 (3.2 examples/sec; 4.944 sec/batch)
2018-05-06 00:07:18.091933: step 77200, loss = 18.03 (3.1 examples/sec; 5.187 sec/batch)
2018-05-06 00:08:11.123097: step 77210, loss = 18.31 (3.2 examples/sec; 5.009 sec/batch)
2018-05-06 00:08:56.941102: step 77220, loss = 18.01 (3.2 examples/sec; 4.956 sec/batch)
2018-05-06 00:09:46.317252: step 77230, loss = 18.86 (3.4 examples/sec; 4.773 sec/batch)
2018-05-06 00:10:35.576126: step 77240, loss = 18.28 (3.2 examples/sec; 4.955 sec/batch)
2018-05-06 00:11:24.501956: step 77250, loss = 18.15 (3.2 examples/sec; 4.976 sec/batch)
2018-05-06 00:12:14.047084: step 77260, loss = 17.96 (3.2 examples/sec; 4.981 sec/batch)
2018-05-06 00:13:03.889309: step 77270, loss = 18.21 (3.2 examples/sec; 4.975 sec/batch)
2018-05-06 00:13:52.656340: step 77280, loss = 17.61 (3.4 examples/sec; 4.755 sec/batch)
2018-05-06 00:14:42.130314: step 77290, loss = 17.68 (3.0 examples/sec; 5.256 sec/batch)
2018-05-06 00:15:31.605595: step 77300, loss = 18.08 (3.1 examples/sec; 5.087 sec/batch)
2018-05-06 00:16:24.499176: step 77310, loss = 17.94 (3.2 examples/sec; 4.956 sec/batch)
2018-05-06 00:17:13.770711: step 77320, loss = 17.86 (3.3 examples/sec; 4.864 sec/batch)
2018-05-06 00:18:02.710005: step 77330, loss = 17.93 (3.3 examples/sec; 4.868 sec/batch)
2018-05-06 00:18:51.735149: step 77340, loss = 17.89 (4.1 examples/sec; 3.949 sec/batch)
2018-05-06 00:19:38.598287: step 77350, loss = 17.80 (3.1 examples/sec; 5.124 sec/batch)
2018-05-06 00:20:28.205006: step 77360, loss = 17.73 (3.3 examples/sec; 4.799 sec/batch)
2018-05-06 00:21:17.894233: step 77370, loss = 18.45 (3.3 examples/sec; 4.909 sec/batch)
2018-05-06 00:22:07.485466: step 77380, loss = 18.01 (3.2 examples/sec; 5.024 sec/batch)
2018-05-06 00:22:56.406661: step 77390, loss = 18.54 (3.3 examples/sec; 4.866 sec/batch)
2018-05-06 00:23:45.812099: step 77400, loss = 18.16 (3.2 examples/sec; 4.992 sec/batch)
2018-05-06 00:24:38.548323: step 77410, loss = 18.37 (3.2 examples/sec; 4.971 sec/batch)
2018-05-06 00:25:28.163214: step 77420, loss = 18.21 (3.3 examples/sec; 4.917 sec/batch)
2018-05-06 00:26:18.052029: step 77430, loss = 18.04 (3.3 examples/sec; 4.909 sec/batch)
2018-05-06 00:27:07.350623: step 77440, loss = 18.32 (3.3 examples/sec; 4.869 sec/batch)
2018-05-06 00:27:56.719724: step 77450, loss = 18.44 (3.2 examples/sec; 4.967 sec/batch)
2018-05-06 00:28:45.897963: step 77460, loss = 18.27 (3.3 examples/sec; 4.854 sec/batch)
2018-05-06 00:29:32.203472: step 77470, loss = 18.00 (3.2 examples/sec; 5.052 sec/batch)
2018-05-06 00:30:21.950758: step 77480, loss = 17.82 (3.1 examples/sec; 5.142 sec/batch)
2018-05-06 00:31:11.742285: step 77490, loss = 17.72 (3.2 examples/sec; 5.001 sec/batch)
2018-05-06 00:32:01.011777: step 77500, loss = 17.87 (3.3 examples/sec; 4.907 sec/batch)
2018-05-06 00:32:54.241675: step 77510, loss = 17.88 (3.1 examples/sec; 5.100 sec/batch)
2018-05-06 00:33:43.242238: step 77520, loss = 17.65 (3.2 examples/sec; 4.925 sec/batch)
2018-05-06 00:34:32.747337: step 77530, loss = 17.95 (3.3 examples/sec; 4.890 sec/batch)
2018-05-06 00:35:21.911654: step 77540, loss = 17.88 (3.3 examples/sec; 4.914 sec/batch)
2018-05-06 00:36:10.871779: step 77550, loss = 17.85 (3.4 examples/sec; 4.655 sec/batch)
2018-05-06 00:37:00.849453: step 77560, loss = 17.75 (3.2 examples/sec; 4.958 sec/batch)
2018-05-06 00:37:49.994354: step 77570, loss = 18.24 (3.4 examples/sec; 4.689 sec/batch)
2018-05-06 00:38:39.587062: step 77580, loss = 18.32 (3.2 examples/sec; 5.032 sec/batch)
2018-05-06 00:39:29.359620: step 77590, loss = 18.75 (3.1 examples/sec; 5.082 sec/batch)
2018-05-06 00:40:15.272680: step 77600, loss = 17.93 (3.3 examples/sec; 4.794 sec/batch)
2018-05-06 00:41:08.303716: step 77610, loss = 17.88 (3.2 examples/sec; 5.009 sec/batch)
2018-05-06 00:41:58.091581: step 77620, loss = 17.73 (3.2 examples/sec; 5.046 sec/batch)
2018-05-06 00:42:47.545603: step 77630, loss = 17.82 (3.2 examples/sec; 4.983 sec/batch)
2018-05-06 00:43:36.428999: step 77640, loss = 18.15 (3.3 examples/sec; 4.882 sec/batch)
2018-05-06 00:44:25.866254: step 77650, loss = 17.93 (3.1 examples/sec; 5.119 sec/batch)
2018-05-06 00:45:15.196917: step 77660, loss = 18.38 (3.3 examples/sec; 4.921 sec/batch)
2018-05-06 00:46:04.669933: step 77670, loss = 17.69 (3.3 examples/sec; 4.877 sec/batch)
2018-05-06 00:46:53.371970: step 77680, loss = 18.72 (3.3 examples/sec; 4.871 sec/batch)
2018-05-06 00:47:42.378477: step 77690, loss = 18.29 (3.2 examples/sec; 4.991 sec/batch)
2018-05-06 00:48:31.470979: step 77700, loss = 17.71 (3.2 examples/sec; 4.924 sec/batch)
2018-05-06 00:49:24.065009: step 77710, loss = 17.63 (3.2 examples/sec; 5.047 sec/batch)
2018-05-06 00:50:09.787662: step 77720, loss = 17.88 (3.3 examples/sec; 4.819 sec/batch)
2018-05-06 00:50:59.451457: step 77730, loss = 18.68 (3.2 examples/sec; 4.932 sec/batch)
2018-05-06 00:51:48.756293: step 77740, loss = 17.99 (3.3 examples/sec; 4.835 sec/batch)
2018-05-06 00:52:37.911690: step 77750, loss = 18.14 (3.2 examples/sec; 5.054 sec/batch)
2018-05-06 00:53:27.359746: step 77760, loss = 17.99 (3.3 examples/sec; 4.890 sec/batch)
2018-05-06 00:54:16.435643: step 77770, loss = 18.82 (3.2 examples/sec; 4.962 sec/batch)
2018-05-06 00:55:06.044781: step 77780, loss = 17.71 (3.1 examples/sec; 5.184 sec/batch)
2018-05-06 00:55:56.039060: step 77790, loss = 18.25 (3.2 examples/sec; 5.030 sec/batch)
2018-05-06 00:56:45.495881: step 77800, loss = 18.16 (3.2 examples/sec; 5.017 sec/batch)
2018-05-06 00:57:39.061923: step 77810, loss = 17.57 (3.2 examples/sec; 4.953 sec/batch)
2018-05-06 00:58:28.839786: step 77820, loss = 18.08 (3.2 examples/sec; 5.034 sec/batch)
2018-05-06 00:59:19.008812: step 77830, loss = 18.19 (3.1 examples/sec; 5.144 sec/batch)
2018-05-06 01:00:08.502641: step 77840, loss = 17.93 (3.2 examples/sec; 4.954 sec/batch)
2018-05-06 01:00:54.403190: step 77850, loss = 17.63 (3.2 examples/sec; 5.061 sec/batch)
2018-05-06 01:01:44.310502: step 77860, loss = 17.91 (3.3 examples/sec; 4.923 sec/batch)
2018-05-06 01:02:33.868422: step 77870, loss = 17.83 (3.2 examples/sec; 4.927 sec/batch)
2018-05-06 01:03:23.437017: step 77880, loss = 18.02 (3.3 examples/sec; 4.883 sec/batch)
2018-05-06 01:04:12.065499: step 77890, loss = 18.04 (3.3 examples/sec; 4.864 sec/batch)
2018-05-06 01:05:01.477753: step 77900, loss = 17.61 (3.2 examples/sec; 5.045 sec/batch)
2018-05-06 01:05:54.422939: step 77910, loss = 17.89 (3.4 examples/sec; 4.773 sec/batch)
2018-05-06 01:06:44.048166: step 77920, loss = 18.43 (3.2 examples/sec; 4.946 sec/batch)
2018-05-06 01:07:33.463857: step 77930, loss = 17.87 (3.2 examples/sec; 4.962 sec/batch)
2018-05-06 01:08:23.282834: step 77940, loss = 18.78 (3.4 examples/sec; 4.731 sec/batch)
2018-05-06 01:09:13.037360: step 77950, loss = 17.90 (3.2 examples/sec; 5.075 sec/batch)
2018-05-06 01:10:02.432919: step 77960, loss = 17.63 (3.2 examples/sec; 4.940 sec/batch)
2018-05-06 01:10:48.185763: step 77970, loss = 18.03 (3.3 examples/sec; 4.874 sec/batch)
2018-05-06 01:11:37.834511: step 77980, loss = 17.98 (3.2 examples/sec; 5.003 sec/batch)
2018-05-06 01:12:27.679136: step 77990, loss = 18.55 (3.2 examples/sec; 5.031 sec/batch)
2018-05-06 01:13:17.579143: step 78000, loss = 17.61 (3.2 examples/sec; 4.958 sec/batch)
2018-05-06 01:14:11.198474: step 78010, loss = 18.31 (3.1 examples/sec; 5.099 sec/batch)
2018-05-06 01:15:00.294560: step 78020, loss = 17.64 (3.2 examples/sec; 4.926 sec/batch)
2018-05-06 01:15:50.094444: step 78030, loss = 18.40 (3.2 examples/sec; 4.960 sec/batch)
2018-05-06 01:16:39.025944: step 78040, loss = 17.91 (3.2 examples/sec; 5.017 sec/batch)
2018-05-06 01:17:28.974302: step 78050, loss = 18.46 (3.2 examples/sec; 5.043 sec/batch)
2018-05-06 01:18:18.583157: step 78060, loss = 18.18 (3.3 examples/sec; 4.901 sec/batch)
2018-05-06 01:19:08.023730: step 78070, loss = 18.08 (3.3 examples/sec; 4.899 sec/batch)
2018-05-06 01:19:57.954344: step 78080, loss = 18.36 (3.1 examples/sec; 5.213 sec/batch)
2018-05-06 01:20:45.457021: step 78090, loss = 17.80 (4.2 examples/sec; 3.789 sec/batch)
2018-05-06 01:21:33.251619: step 78100, loss = 18.86 (3.2 examples/sec; 4.943 sec/batch)
2018-05-06 01:22:25.663858: step 78110, loss = 18.01 (3.3 examples/sec; 4.848 sec/batch)
2018-05-06 01:23:14.551964: step 78120, loss = 17.77 (3.2 examples/sec; 4.926 sec/batch)
2018-05-06 01:24:03.832239: step 78130, loss = 18.16 (3.2 examples/sec; 5.027 sec/batch)
2018-05-06 01:24:52.954903: step 78140, loss = 17.89 (3.3 examples/sec; 4.863 sec/batch)
2018-05-06 01:25:42.730012: step 78150, loss = 18.17 (3.2 examples/sec; 4.948 sec/batch)
2018-05-06 01:26:31.687392: step 78160, loss = 18.19 (3.3 examples/sec; 4.826 sec/batch)
2018-05-06 01:27:21.014211: step 78170, loss = 17.91 (3.3 examples/sec; 4.796 sec/batch)
2018-05-06 01:28:10.069479: step 78180, loss = 18.08 (3.2 examples/sec; 4.972 sec/batch)
2018-05-06 01:28:59.045539: step 78190, loss = 17.88 (3.3 examples/sec; 4.848 sec/batch)
2018-05-06 01:29:48.279342: step 78200, loss = 18.36 (3.2 examples/sec; 4.971 sec/batch)
2018-05-06 01:30:41.416664: step 78210, loss = 17.88 (3.2 examples/sec; 5.027 sec/batch)
2018-05-06 01:31:27.043751: step 78220, loss = 17.92 (3.2 examples/sec; 5.035 sec/batch)
2018-05-06 01:32:16.279087: step 78230, loss = 17.82 (3.3 examples/sec; 4.917 sec/batch)
2018-05-06 01:33:05.654749: step 78240, loss = 18.18 (3.2 examples/sec; 4.959 sec/batch)
2018-05-06 01:33:54.744122: step 78250, loss = 17.92 (3.3 examples/sec; 4.891 sec/batch)
2018-05-06 01:34:43.795286: step 78260, loss = 18.40 (3.5 examples/sec; 4.618 sec/batch)
2018-05-06 01:35:33.499996: step 78270, loss = 17.90 (3.3 examples/sec; 4.811 sec/batch)
2018-05-06 01:36:22.511621: step 78280, loss = 17.90 (3.3 examples/sec; 4.849 sec/batch)
2018-05-06 01:37:11.690245: step 78290, loss = 18.12 (3.2 examples/sec; 4.935 sec/batch)
2018-05-06 01:38:01.539717: step 78300, loss = 17.99 (3.1 examples/sec; 5.170 sec/batch)
2018-05-06 01:38:54.582398: step 78310, loss = 17.96 (3.2 examples/sec; 4.930 sec/batch)
2018-05-06 01:39:43.242379: step 78320, loss = 18.52 (3.2 examples/sec; 4.985 sec/batch)
2018-05-06 01:40:32.325236: step 78330, loss = 18.26 (3.2 examples/sec; 4.937 sec/batch)
2018-05-06 01:41:21.530474: step 78340, loss = 17.94 (3.3 examples/sec; 4.855 sec/batch)
2018-05-06 01:42:07.682023: step 78350, loss = 18.05 (3.2 examples/sec; 5.047 sec/batch)
2018-05-06 01:42:56.500940: step 78360, loss = 18.09 (3.3 examples/sec; 4.856 sec/batch)
2018-05-06 01:43:45.604621: step 78370, loss = 17.87 (3.3 examples/sec; 4.778 sec/batch)
2018-05-06 01:44:35.513982: step 78380, loss = 17.88 (3.1 examples/sec; 5.175 sec/batch)
2018-05-06 01:45:25.486183: step 78390, loss = 18.38 (3.2 examples/sec; 4.980 sec/batch)
2018-05-06 01:46:14.850983: step 78400, loss = 17.63 (3.4 examples/sec; 4.695 sec/batch)
2018-05-06 01:47:08.785923: step 78410, loss = 17.79 (3.1 examples/sec; 5.112 sec/batch)
2018-05-06 01:47:58.381960: step 78420, loss = 17.96 (3.2 examples/sec; 5.015 sec/batch)
2018-05-06 01:48:48.293782: step 78430, loss = 18.36 (3.0 examples/sec; 5.362 sec/batch)
2018-05-06 01:49:37.744288: step 78440, loss = 17.70 (3.3 examples/sec; 4.877 sec/batch)
2018-05-06 01:50:27.168583: step 78450, loss = 18.11 (3.2 examples/sec; 5.058 sec/batch)
2018-05-06 01:51:16.612370: step 78460, loss = 17.88 (3.3 examples/sec; 4.906 sec/batch)
2018-05-06 01:52:02.308538: step 78470, loss = 17.68 (3.2 examples/sec; 4.930 sec/batch)
2018-05-06 01:52:51.471359: step 78480, loss = 17.74 (3.4 examples/sec; 4.775 sec/batch)
2018-05-06 01:53:40.897444: step 78490, loss = 18.12 (3.3 examples/sec; 4.898 sec/batch)
2018-05-06 01:54:30.393988: step 78500, loss = 17.97 (3.3 examples/sec; 4.912 sec/batch)
2018-05-06 01:55:23.015795: step 78510, loss = 18.43 (3.2 examples/sec; 5.017 sec/batch)
2018-05-06 01:56:12.584441: step 78520, loss = 18.08 (3.3 examples/sec; 4.834 sec/batch)
2018-05-06 01:57:01.580211: step 78530, loss = 18.81 (3.3 examples/sec; 4.805 sec/batch)
2018-05-06 01:57:50.748757: step 78540, loss = 17.97 (3.3 examples/sec; 4.827 sec/batch)
2018-05-06 01:58:40.389879: step 78550, loss = 17.69 (3.2 examples/sec; 4.957 sec/batch)
2018-05-06 01:59:30.453349: step 78560, loss = 17.66 (3.2 examples/sec; 4.927 sec/batch)
2018-05-06 02:00:20.245063: step 78570, loss = 18.08 (3.2 examples/sec; 4.961 sec/batch)
2018-05-06 02:01:09.271517: step 78580, loss = 17.60 (3.3 examples/sec; 4.831 sec/batch)
2018-05-06 02:01:57.179205: step 78590, loss = 18.03 (4.1 examples/sec; 3.884 sec/batch)
2018-05-06 02:02:44.090592: step 78600, loss = 17.85 (3.2 examples/sec; 4.995 sec/batch)
2018-05-06 02:03:37.098121: step 78610, loss = 17.79 (3.2 examples/sec; 5.004 sec/batch)
2018-05-06 02:04:26.422854: step 78620, loss = 17.93 (3.2 examples/sec; 4.958 sec/batch)
2018-05-06 02:05:16.206649: step 78630, loss = 17.92 (3.3 examples/sec; 4.905 sec/batch)
2018-05-06 02:06:05.449442: step 78640, loss = 18.07 (3.3 examples/sec; 4.887 sec/batch)
2018-05-06 02:06:54.328555: step 78650, loss = 18.03 (3.3 examples/sec; 4.800 sec/batch)
2018-05-06 02:07:43.667706: step 78660, loss = 18.40 (3.2 examples/sec; 4.953 sec/batch)
2018-05-06 02:08:32.004930: step 78670, loss = 17.98 (3.3 examples/sec; 4.874 sec/batch)
2018-05-06 02:09:21.634198: step 78680, loss = 17.80 (3.2 examples/sec; 4.997 sec/batch)
2018-05-06 02:10:10.599905: step 78690, loss = 17.88 (3.3 examples/sec; 4.802 sec/batch)
2018-05-06 02:10:59.982464: step 78700, loss = 17.84 (3.2 examples/sec; 5.071 sec/batch)
2018-05-06 02:11:52.808724: step 78710, loss = 17.98 (3.3 examples/sec; 4.821 sec/batch)
2018-05-06 02:12:38.833548: step 78720, loss = 17.69 (3.2 examples/sec; 4.944 sec/batch)
2018-05-06 02:13:27.998628: step 78730, loss = 18.06 (3.4 examples/sec; 4.647 sec/batch)
2018-05-06 02:14:17.278309: step 78740, loss = 17.78 (3.2 examples/sec; 4.947 sec/batch)
2018-05-06 02:15:07.081842: step 78750, loss = 17.75 (3.2 examples/sec; 4.941 sec/batch)
2018-05-06 02:15:57.182634: step 78760, loss = 18.03 (3.2 examples/sec; 5.063 sec/batch)
2018-05-06 02:16:46.511962: step 78770, loss = 17.64 (3.1 examples/sec; 5.100 sec/batch)
2018-05-06 02:17:36.013931: step 78780, loss = 18.03 (3.3 examples/sec; 4.869 sec/batch)
2018-05-06 02:18:25.484523: step 78790, loss = 17.93 (3.3 examples/sec; 4.796 sec/batch)
2018-05-06 02:19:14.688871: step 78800, loss = 17.78 (3.2 examples/sec; 4.951 sec/batch)
2018-05-06 02:20:08.400527: step 78810, loss = 18.78 (3.3 examples/sec; 4.920 sec/batch)
2018-05-06 02:20:58.025830: step 78820, loss = 17.68 (3.3 examples/sec; 4.872 sec/batch)
2018-05-06 02:21:47.452980: step 78830, loss = 18.21 (3.3 examples/sec; 4.833 sec/batch)
2018-05-06 02:22:36.596921: step 78840, loss = 18.24 (3.2 examples/sec; 5.053 sec/batch)
2018-05-06 02:23:22.544485: step 78850, loss = 17.78 (3.2 examples/sec; 4.941 sec/batch)
2018-05-06 02:24:11.671797: step 78860, loss = 18.22 (3.3 examples/sec; 4.807 sec/batch)
2018-05-06 02:25:00.324953: step 78870, loss = 17.72 (3.3 examples/sec; 4.918 sec/batch)
2018-05-06 02:25:49.729389: step 78880, loss = 17.91 (3.2 examples/sec; 4.932 sec/batch)
2018-05-06 02:26:39.419756: step 78890, loss = 17.87 (3.3 examples/sec; 4.821 sec/batch)
2018-05-06 02:27:29.300984: step 78900, loss = 18.29 (3.3 examples/sec; 4.906 sec/batch)
2018-05-06 02:28:22.004718: step 78910, loss = 17.84 (3.2 examples/sec; 5.011 sec/batch)
2018-05-06 02:29:11.446396: step 78920, loss = 18.50 (3.3 examples/sec; 4.835 sec/batch)
2018-05-06 02:30:00.591543: step 78930, loss = 18.52 (3.2 examples/sec; 4.930 sec/batch)
2018-05-06 02:30:50.204440: step 78940, loss = 18.03 (3.2 examples/sec; 4.986 sec/batch)
2018-05-06 02:31:38.979047: step 78950, loss = 17.73 (3.3 examples/sec; 4.885 sec/batch)
2018-05-06 02:32:28.380716: step 78960, loss = 18.17 (3.1 examples/sec; 5.082 sec/batch)
2018-05-06 02:33:14.639070: step 78970, loss = 18.02 (3.2 examples/sec; 5.004 sec/batch)
2018-05-06 02:34:04.198369: step 78980, loss = 18.11 (3.1 examples/sec; 5.178 sec/batch)
2018-05-06 02:34:54.075667: step 78990, loss = 17.94 (3.3 examples/sec; 4.898 sec/batch)
2018-05-06 02:35:43.539791: step 79000, loss = 17.75 (3.2 examples/sec; 4.978 sec/batch)
2018-05-06 02:36:36.523530: step 79010, loss = 17.72 (3.2 examples/sec; 4.967 sec/batch)
2018-05-06 02:37:26.219336: step 79020, loss = 17.84 (3.2 examples/sec; 4.963 sec/batch)
2018-05-06 02:38:15.697621: step 79030, loss = 17.68 (3.2 examples/sec; 4.948 sec/batch)
2018-05-06 02:39:05.592262: step 79040, loss = 17.89 (3.2 examples/sec; 5.031 sec/batch)
2018-05-06 02:39:55.160360: step 79050, loss = 18.30 (3.2 examples/sec; 4.937 sec/batch)
2018-05-06 02:40:44.378570: step 79060, loss = 18.21 (3.2 examples/sec; 4.973 sec/batch)
2018-05-06 02:41:33.969622: step 79070, loss = 18.03 (3.3 examples/sec; 4.918 sec/batch)
2018-05-06 02:42:23.469179: step 79080, loss = 17.97 (3.2 examples/sec; 4.980 sec/batch)
2018-05-06 02:43:12.944846: step 79090, loss = 17.68 (3.5 examples/sec; 4.511 sec/batch)
2018-05-06 02:44:00.048316: step 79100, loss = 17.99 (3.3 examples/sec; 4.862 sec/batch)
2018-05-06 02:44:53.329596: step 79110, loss = 18.09 (3.2 examples/sec; 5.058 sec/batch)
2018-05-06 02:45:42.685664: step 79120, loss = 18.45 (3.3 examples/sec; 4.777 sec/batch)
2018-05-06 02:46:31.541972: step 79130, loss = 18.12 (3.3 examples/sec; 4.888 sec/batch)
2018-05-06 02:47:21.131952: step 79140, loss = 18.18 (3.2 examples/sec; 4.982 sec/batch)
2018-05-06 02:48:10.960558: step 79150, loss = 17.70 (3.1 examples/sec; 5.116 sec/batch)
2018-05-06 02:49:00.190477: step 79160, loss = 18.15 (3.3 examples/sec; 4.886 sec/batch)
2018-05-06 02:49:49.465196: step 79170, loss = 17.63 (3.2 examples/sec; 4.949 sec/batch)
2018-05-06 02:50:39.207800: step 79180, loss = 18.73 (3.2 examples/sec; 5.049 sec/batch)
2018-05-06 02:51:28.920336: step 79190, loss = 17.95 (3.2 examples/sec; 5.035 sec/batch)
2018-05-06 02:52:18.293100: step 79200, loss = 18.26 (3.2 examples/sec; 4.982 sec/batch)
2018-05-06 02:53:10.740234: step 79210, loss = 17.86 (3.3 examples/sec; 4.912 sec/batch)
2018-05-06 02:53:56.322962: step 79220, loss = 18.04 (3.3 examples/sec; 4.879 sec/batch)
2018-05-06 02:54:45.746929: step 79230, loss = 17.62 (3.2 examples/sec; 4.964 sec/batch)
2018-05-06 02:55:35.440362: step 79240, loss = 17.82 (3.3 examples/sec; 4.921 sec/batch)
2018-05-06 02:56:24.804307: step 79250, loss = 17.96 (3.3 examples/sec; 4.899 sec/batch)
2018-05-06 02:57:14.055253: step 79260, loss = 17.91 (3.2 examples/sec; 5.000 sec/batch)
2018-05-06 02:58:02.797907: step 79270, loss = 18.00 (3.4 examples/sec; 4.760 sec/batch)
2018-05-06 02:58:51.876468: step 79280, loss = 17.93 (3.3 examples/sec; 4.865 sec/batch)
2018-05-06 02:59:41.118509: step 79290, loss = 17.65 (3.3 examples/sec; 4.835 sec/batch)
2018-05-06 03:00:30.443799: step 79300, loss = 17.76 (3.1 examples/sec; 5.095 sec/batch)
2018-05-06 03:01:23.987295: step 79310, loss = 18.14 (3.2 examples/sec; 4.974 sec/batch)
2018-05-06 03:02:13.601586: step 79320, loss = 18.30 (3.2 examples/sec; 5.053 sec/batch)
2018-05-06 03:03:03.161622: step 79330, loss = 18.39 (3.4 examples/sec; 4.736 sec/batch)
2018-05-06 03:03:50.998495: step 79340, loss = 17.72 (4.3 examples/sec; 3.694 sec/batch)
2018-05-06 03:04:38.301384: step 79350, loss = 18.06 (3.2 examples/sec; 5.039 sec/batch)
2018-05-06 03:05:27.565053: step 79360, loss = 17.81 (3.2 examples/sec; 4.966 sec/batch)
2018-05-06 03:06:17.315757: step 79370, loss = 17.72 (3.1 examples/sec; 5.165 sec/batch)
2018-05-06 03:07:07.245784: step 79380, loss = 17.89 (3.2 examples/sec; 4.997 sec/batch)
2018-05-06 03:07:56.610369: step 79390, loss = 18.26 (3.3 examples/sec; 4.820 sec/batch)
2018-05-06 03:08:45.620247: step 79400, loss = 17.89 (3.2 examples/sec; 4.926 sec/batch)
2018-05-06 03:09:38.542088: step 79410, loss = 17.91 (3.3 examples/sec; 4.853 sec/batch)
2018-05-06 03:10:28.086338: step 79420, loss = 18.20 (3.2 examples/sec; 5.013 sec/batch)
2018-05-06 03:11:17.697964: step 79430, loss = 17.77 (3.2 examples/sec; 4.981 sec/batch)
2018-05-06 03:12:06.654954: step 79440, loss = 17.95 (3.2 examples/sec; 4.935 sec/batch)
2018-05-06 03:12:56.322351: step 79450, loss = 17.64 (3.3 examples/sec; 4.905 sec/batch)
2018-05-06 03:13:45.315823: step 79460, loss = 17.84 (3.3 examples/sec; 4.779 sec/batch)
2018-05-06 03:14:31.456411: step 79470, loss = 17.89 (3.1 examples/sec; 5.108 sec/batch)
2018-05-06 03:15:20.397302: step 79480, loss = 17.67 (3.2 examples/sec; 4.984 sec/batch)
2018-05-06 03:16:09.980934: step 79490, loss = 17.95 (3.2 examples/sec; 4.985 sec/batch)
2018-05-06 03:16:59.765397: step 79500, loss = 18.06 (3.1 examples/sec; 5.138 sec/batch)
2018-05-06 03:17:52.971397: step 79510, loss = 18.05 (3.2 examples/sec; 5.009 sec/batch)
2018-05-06 03:18:42.703759: step 79520, loss = 17.84 (3.1 examples/sec; 5.130 sec/batch)
2018-05-06 03:19:32.439544: step 79530, loss = 17.88 (3.3 examples/sec; 4.841 sec/batch)
2018-05-06 03:20:22.418988: step 79540, loss = 17.70 (3.2 examples/sec; 4.953 sec/batch)
2018-05-06 03:21:11.930874: step 79550, loss = 18.36 (3.2 examples/sec; 5.008 sec/batch)
2018-05-06 03:22:01.375360: step 79560, loss = 18.10 (3.2 examples/sec; 4.957 sec/batch)
2018-05-06 03:22:50.414573: step 79570, loss = 17.67 (3.3 examples/sec; 4.886 sec/batch)
2018-05-06 03:23:39.540586: step 79580, loss = 17.57 (3.3 examples/sec; 4.839 sec/batch)
2018-05-06 03:24:27.434005: step 79590, loss = 18.12 (4.2 examples/sec; 3.809 sec/batch)
2018-05-06 03:25:14.744765: step 79600, loss = 18.14 (3.3 examples/sec; 4.899 sec/batch)
2018-05-06 03:26:07.098767: step 79610, loss = 17.92 (3.4 examples/sec; 4.752 sec/batch)
2018-05-06 03:26:56.295460: step 79620, loss = 17.61 (3.1 examples/sec; 5.168 sec/batch)
2018-05-06 03:27:45.681831: step 79630, loss = 18.06 (3.2 examples/sec; 4.940 sec/batch)
2018-05-06 03:28:34.961376: step 79640, loss = 18.05 (3.3 examples/sec; 4.891 sec/batch)
2018-05-06 03:29:24.540873: step 79650, loss = 18.23 (3.2 examples/sec; 4.950 sec/batch)
2018-05-06 03:30:14.192652: step 79660, loss = 17.69 (3.2 examples/sec; 4.957 sec/batch)
2018-05-06 03:31:03.425009: step 79670, loss = 18.00 (3.4 examples/sec; 4.640 sec/batch)
2018-05-06 03:31:52.492856: step 79680, loss = 17.71 (3.2 examples/sec; 4.941 sec/batch)
2018-05-06 03:32:42.038367: step 79690, loss = 17.88 (3.3 examples/sec; 4.860 sec/batch)
2018-05-06 03:33:31.813896: step 79700, loss = 18.02 (3.3 examples/sec; 4.911 sec/batch)
2018-05-06 03:34:24.538743: step 79710, loss = 18.17 (3.3 examples/sec; 4.881 sec/batch)
2018-05-06 03:35:11.079344: step 79720, loss = 18.22 (3.2 examples/sec; 4.995 sec/batch)
2018-05-06 03:36:00.076772: step 79730, loss = 18.23 (3.3 examples/sec; 4.829 sec/batch)
2018-05-06 03:36:49.422152: step 79740, loss = 17.96 (3.3 examples/sec; 4.876 sec/batch)
2018-05-06 03:37:38.601886: step 79750, loss = 18.36 (3.2 examples/sec; 5.005 sec/batch)
2018-05-06 03:38:28.022262: step 79760, loss = 18.10 (3.3 examples/sec; 4.853 sec/batch)
2018-05-06 03:39:17.866989: step 79770, loss = 17.54 (3.2 examples/sec; 5.066 sec/batch)
2018-05-06 03:40:07.335972: step 79780, loss = 17.91 (3.2 examples/sec; 5.055 sec/batch)
2018-05-06 03:40:57.052861: step 79790, loss = 17.67 (3.0 examples/sec; 5.249 sec/batch)
2018-05-06 03:41:46.176993: step 79800, loss = 17.64 (3.2 examples/sec; 4.934 sec/batch)
2018-05-06 03:42:39.215886: step 79810, loss = 18.29 (3.2 examples/sec; 5.002 sec/batch)
2018-05-06 03:43:28.833857: step 79820, loss = 18.14 (3.2 examples/sec; 4.931 sec/batch)
2018-05-06 03:44:18.891978: step 79830, loss = 17.65 (3.2 examples/sec; 4.995 sec/batch)
2018-05-06 03:45:06.221673: step 79840, loss = 18.14 (4.3 examples/sec; 3.697 sec/batch)
2018-05-06 03:45:54.527049: step 79850, loss = 17.94 (3.2 examples/sec; 5.011 sec/batch)
2018-05-06 03:46:43.774153: step 79860, loss = 18.91 (3.2 examples/sec; 4.937 sec/batch)
2018-05-06 03:47:33.102364: step 79870, loss = 18.27 (3.3 examples/sec; 4.892 sec/batch)
2018-05-06 03:48:22.485891: step 79880, loss = 17.92 (3.2 examples/sec; 4.985 sec/batch)
2018-05-06 03:49:11.942936: step 79890, loss = 17.88 (3.3 examples/sec; 4.877 sec/batch)
2018-05-06 03:50:01.139625: step 79900, loss = 18.10 (3.4 examples/sec; 4.678 sec/batch)
2018-05-06 03:50:53.753621: step 79910, loss = 17.86 (3.3 examples/sec; 4.829 sec/batch)
2018-05-06 03:51:43.302066: step 79920, loss = 17.71 (3.3 examples/sec; 4.920 sec/batch)
2018-05-06 03:52:32.862148: step 79930, loss = 17.73 (3.2 examples/sec; 4.967 sec/batch)
2018-05-06 03:53:22.111035: step 79940, loss = 18.14 (3.2 examples/sec; 5.013 sec/batch)
2018-05-06 03:54:11.158831: step 79950, loss = 17.84 (3.3 examples/sec; 4.909 sec/batch)
2018-05-06 03:55:00.558161: step 79960, loss = 18.29 (3.1 examples/sec; 5.117 sec/batch)
2018-05-06 03:55:46.719899: step 79970, loss = 17.86 (3.4 examples/sec; 4.760 sec/batch)
2018-05-06 03:56:36.024265: step 79980, loss = 17.84 (3.2 examples/sec; 4.944 sec/batch)
2018-05-06 03:57:25.206235: step 79990, loss = 18.05 (3.3 examples/sec; 4.836 sec/batch)
2018-05-06 03:58:14.690802: step 80000, loss = 17.93 (3.3 examples/sec; 4.911 sec/batch)
2018-05-06 03:59:06.996610: step 80010, loss = 17.93 (3.2 examples/sec; 5.056 sec/batch)
2018-05-06 03:59:55.591583: step 80020, loss = 18.64 (3.3 examples/sec; 4.878 sec/batch)
2018-05-06 04:00:44.479603: step 80030, loss = 17.68 (3.2 examples/sec; 4.960 sec/batch)
2018-05-06 04:01:33.473132: step 80040, loss = 17.86 (3.3 examples/sec; 4.852 sec/batch)
2018-05-06 04:02:22.244326: step 80050, loss = 17.94 (3.3 examples/sec; 4.883 sec/batch)
2018-05-06 04:03:11.660412: step 80060, loss = 17.59 (3.2 examples/sec; 5.028 sec/batch)
2018-05-06 04:04:01.355437: step 80070, loss = 17.99 (3.2 examples/sec; 4.926 sec/batch)
2018-05-06 04:04:50.577222: step 80080, loss = 18.01 (3.3 examples/sec; 4.899 sec/batch)
2018-05-06 04:05:40.104070: step 80090, loss = 18.15 (3.2 examples/sec; 4.937 sec/batch)
2018-05-06 04:06:25.539089: step 80100, loss = 17.59 (3.4 examples/sec; 4.767 sec/batch)
2018-05-06 04:07:18.426274: step 80110, loss = 18.26 (3.4 examples/sec; 4.742 sec/batch)
2018-05-06 04:08:07.421025: step 80120, loss = 18.12 (3.3 examples/sec; 4.786 sec/batch)
2018-05-06 04:08:56.379117: step 80130, loss = 17.96 (3.3 examples/sec; 4.900 sec/batch)
2018-05-06 04:09:45.939437: step 80140, loss = 18.20 (3.3 examples/sec; 4.906 sec/batch)
2018-05-06 04:10:35.218202: step 80150, loss = 18.20 (3.3 examples/sec; 4.826 sec/batch)
2018-05-06 04:11:24.099586: step 80160, loss = 17.66 (3.3 examples/sec; 4.849 sec/batch)
2018-05-06 04:12:13.405006: step 80170, loss = 18.15 (3.2 examples/sec; 4.999 sec/batch)
2018-05-06 04:13:02.194480: step 80180, loss = 17.82 (3.3 examples/sec; 4.886 sec/batch)
2018-05-06 04:13:50.421019: step 80190, loss = 17.93 (3.5 examples/sec; 4.595 sec/batch)
2018-05-06 04:14:39.459456: step 80200, loss = 17.87 (3.3 examples/sec; 4.869 sec/batch)
2018-05-06 04:15:32.418688: step 80210, loss = 18.17 (3.3 examples/sec; 4.880 sec/batch)
2018-05-06 04:16:18.256904: step 80220, loss = 17.98 (3.7 examples/sec; 4.295 sec/batch)
2018-05-06 04:17:08.047847: step 80230, loss = 17.96 (3.2 examples/sec; 5.045 sec/batch)
2018-05-06 04:17:57.586017: step 80240, loss = 17.73 (3.2 examples/sec; 4.965 sec/batch)
2018-05-06 04:18:47.671243: step 80250, loss = 18.11 (3.2 examples/sec; 5.002 sec/batch)
2018-05-06 04:19:37.981001: step 80260, loss = 17.86 (3.1 examples/sec; 5.127 sec/batch)
2018-05-06 04:20:27.321544: step 80270, loss = 18.05 (3.3 examples/sec; 4.802 sec/batch)
2018-05-06 04:21:16.975415: step 80280, loss = 18.24 (3.3 examples/sec; 4.897 sec/batch)
2018-05-06 04:22:06.649255: step 80290, loss = 18.10 (3.2 examples/sec; 5.012 sec/batch)
2018-05-06 04:22:56.575372: step 80300, loss = 17.87 (3.1 examples/sec; 5.098 sec/batch)
2018-05-06 04:23:49.689536: step 80310, loss = 18.09 (3.2 examples/sec; 5.034 sec/batch)
2018-05-06 04:24:39.160691: step 80320, loss = 17.58 (3.2 examples/sec; 4.987 sec/batch)
2018-05-06 04:25:29.098815: step 80330, loss = 17.88 (3.2 examples/sec; 5.032 sec/batch)
2018-05-06 04:26:18.802034: step 80340, loss = 17.89 (3.2 examples/sec; 4.990 sec/batch)
2018-05-06 04:27:04.218501: step 80350, loss = 18.07 (3.2 examples/sec; 4.929 sec/batch)
2018-05-06 04:27:53.757922: step 80360, loss = 17.82 (3.2 examples/sec; 4.953 sec/batch)
2018-05-06 04:28:42.574774: step 80370, loss = 18.15 (3.3 examples/sec; 4.804 sec/batch)
2018-05-06 04:29:31.978532: step 80380, loss = 18.01 (3.4 examples/sec; 4.738 sec/batch)
2018-05-06 04:30:21.941942: step 80390, loss = 18.38 (3.2 examples/sec; 4.939 sec/batch)
2018-05-06 04:31:11.882290: step 80400, loss = 17.72 (3.1 examples/sec; 5.141 sec/batch)
2018-05-06 04:32:04.713014: step 80410, loss = 18.04 (3.3 examples/sec; 4.859 sec/batch)
2018-05-06 04:32:53.906585: step 80420, loss = 18.45 (3.2 examples/sec; 5.022 sec/batch)
2018-05-06 04:33:43.960532: step 80430, loss = 17.92 (3.2 examples/sec; 5.020 sec/batch)
2018-05-06 04:34:34.101812: step 80440, loss = 18.34 (3.1 examples/sec; 5.096 sec/batch)
2018-05-06 04:35:23.394405: step 80450, loss = 18.13 (3.2 examples/sec; 4.963 sec/batch)
2018-05-06 04:36:12.289663: step 80460, loss = 17.60 (3.3 examples/sec; 4.878 sec/batch)
2018-05-06 04:36:58.498483: step 80470, loss = 17.84 (3.1 examples/sec; 5.084 sec/batch)
2018-05-06 04:37:48.558235: step 80480, loss = 17.88 (3.2 examples/sec; 4.987 sec/batch)
2018-05-06 04:38:38.374646: step 80490, loss = 17.69 (3.3 examples/sec; 4.874 sec/batch)
2018-05-06 04:39:27.425412: step 80500, loss = 18.25 (3.4 examples/sec; 4.753 sec/batch)
2018-05-06 04:40:19.928611: step 80510, loss = 17.95 (3.3 examples/sec; 4.840 sec/batch)
2018-05-06 04:41:09.840803: step 80520, loss = 17.74 (3.1 examples/sec; 5.121 sec/batch)
2018-05-06 04:41:59.842373: step 80530, loss = 18.30 (3.2 examples/sec; 5.011 sec/batch)
2018-05-06 04:42:49.614952: step 80540, loss = 17.60 (3.2 examples/sec; 4.956 sec/batch)
2018-05-06 04:43:38.475168: step 80550, loss = 17.86 (3.2 examples/sec; 4.938 sec/batch)
2018-05-06 04:44:27.560315: step 80560, loss = 17.64 (3.2 examples/sec; 4.960 sec/batch)
2018-05-06 04:45:16.750359: step 80570, loss = 18.19 (3.3 examples/sec; 4.905 sec/batch)
2018-05-06 04:46:05.747449: step 80580, loss = 18.09 (3.2 examples/sec; 4.969 sec/batch)
2018-05-06 04:46:55.573590: step 80590, loss = 18.03 (3.3 examples/sec; 4.890 sec/batch)
2018-05-06 04:47:41.695195: step 80600, loss = 18.16 (3.2 examples/sec; 4.947 sec/batch)
2018-05-06 04:48:34.710405: step 80610, loss = 17.93 (3.2 examples/sec; 5.029 sec/batch)
2018-05-06 04:49:24.452254: step 80620, loss = 17.88 (3.2 examples/sec; 5.004 sec/batch)
2018-05-06 04:50:14.175928: step 80630, loss = 17.65 (3.2 examples/sec; 4.962 sec/batch)
2018-05-06 04:51:03.678820: step 80640, loss = 18.18 (3.2 examples/sec; 5.059 sec/batch)
2018-05-06 04:51:53.051695: step 80650, loss = 17.80 (3.2 examples/sec; 4.960 sec/batch)
2018-05-06 04:52:42.837017: step 80660, loss = 18.29 (3.4 examples/sec; 4.740 sec/batch)
2018-05-06 04:53:32.717422: step 80670, loss = 17.72 (3.1 examples/sec; 5.107 sec/batch)
2018-05-06 04:54:22.876098: step 80680, loss = 17.62 (3.2 examples/sec; 5.025 sec/batch)
2018-05-06 04:55:11.985510: step 80690, loss = 18.31 (3.3 examples/sec; 4.921 sec/batch)
2018-05-06 04:56:01.845809: step 80700, loss = 17.64 (3.2 examples/sec; 5.000 sec/batch)
2018-05-06 04:56:54.582843: step 80710, loss = 18.14 (3.2 examples/sec; 5.059 sec/batch)
2018-05-06 04:57:40.394694: step 80720, loss = 18.59 (3.2 examples/sec; 4.962 sec/batch)
2018-05-06 04:58:30.626577: step 80730, loss = 18.23 (3.1 examples/sec; 5.124 sec/batch)
2018-05-06 04:59:20.264661: step 80740, loss = 18.01 (3.2 examples/sec; 5.053 sec/batch)
2018-05-06 05:00:09.128725: step 80750, loss = 18.06 (3.3 examples/sec; 4.864 sec/batch)
2018-05-06 05:00:58.487339: step 80760, loss = 17.63 (3.3 examples/sec; 4.847 sec/batch)
2018-05-06 05:01:47.931338: step 80770, loss = 18.27 (3.2 examples/sec; 4.924 sec/batch)
2018-05-06 05:02:36.984143: step 80780, loss = 17.75 (3.3 examples/sec; 4.887 sec/batch)
2018-05-06 05:03:25.941902: step 80790, loss = 18.06 (3.3 examples/sec; 4.837 sec/batch)
2018-05-06 05:04:15.398227: step 80800, loss = 18.16 (3.2 examples/sec; 5.030 sec/batch)
2018-05-06 05:05:09.079776: step 80810, loss = 17.96 (3.1 examples/sec; 5.153 sec/batch)
2018-05-06 05:05:58.826082: step 80820, loss = 17.60 (3.2 examples/sec; 5.013 sec/batch)
2018-05-06 05:06:47.714523: step 80830, loss = 18.13 (3.2 examples/sec; 4.977 sec/batch)
2018-05-06 05:07:37.801602: step 80840, loss = 17.85 (3.2 examples/sec; 5.061 sec/batch)
2018-05-06 05:08:24.160904: step 80850, loss = 17.99 (3.2 examples/sec; 4.976 sec/batch)
2018-05-06 05:09:13.260685: step 80860, loss = 18.21 (3.3 examples/sec; 4.878 sec/batch)
2018-05-06 05:10:03.144432: step 80870, loss = 18.24 (3.2 examples/sec; 4.961 sec/batch)
2018-05-06 05:10:52.831273: step 80880, loss = 18.14 (3.3 examples/sec; 4.863 sec/batch)
2018-05-06 05:11:42.232551: step 80890, loss = 18.06 (3.3 examples/sec; 4.845 sec/batch)
2018-05-06 05:12:31.697694: step 80900, loss = 17.92 (3.3 examples/sec; 4.842 sec/batch)
2018-05-06 05:13:24.819958: step 80910, loss = 17.86 (3.3 examples/sec; 4.868 sec/batch)
2018-05-06 05:14:14.013950: step 80920, loss = 18.41 (3.1 examples/sec; 5.093 sec/batch)
2018-05-06 05:15:03.490615: step 80930, loss = 17.85 (3.2 examples/sec; 4.965 sec/batch)
2018-05-06 05:15:52.465206: step 80940, loss = 18.15 (3.3 examples/sec; 4.844 sec/batch)
2018-05-06 05:16:41.711464: step 80950, loss = 18.23 (3.2 examples/sec; 4.929 sec/batch)
2018-05-06 05:17:31.128047: step 80960, loss = 17.96 (3.2 examples/sec; 4.943 sec/batch)
2018-05-06 05:18:16.870802: step 80970, loss = 17.85 (3.3 examples/sec; 4.879 sec/batch)
2018-05-06 05:19:06.552222: step 80980, loss = 17.60 (3.2 examples/sec; 4.932 sec/batch)
2018-05-06 05:19:56.045627: step 80990, loss = 18.17 (3.2 examples/sec; 4.977 sec/batch)
2018-05-06 05:20:45.237122: step 81000, loss = 17.67 (3.2 examples/sec; 4.992 sec/batch)
2018-05-06 05:21:38.485280: step 81010, loss = 18.36 (3.3 examples/sec; 4.911 sec/batch)
2018-05-06 05:22:27.891017: step 81020, loss = 18.16 (3.2 examples/sec; 4.939 sec/batch)
2018-05-06 05:23:17.432437: step 81030, loss = 18.07 (3.3 examples/sec; 4.873 sec/batch)
2018-05-06 05:24:06.725453: step 81040, loss = 18.15 (3.2 examples/sec; 4.978 sec/batch)
2018-05-06 05:24:56.419146: step 81050, loss = 17.99 (3.2 examples/sec; 4.967 sec/batch)
2018-05-06 05:25:45.743462: step 81060, loss = 17.72 (3.3 examples/sec; 4.884 sec/batch)
2018-05-06 05:26:34.820972: step 81070, loss = 18.34 (3.3 examples/sec; 4.911 sec/batch)
2018-05-06 05:27:24.294046: step 81080, loss = 18.38 (3.2 examples/sec; 4.962 sec/batch)
2018-05-06 05:28:13.439494: step 81090, loss = 17.69 (3.3 examples/sec; 4.916 sec/batch)
2018-05-06 05:28:59.414255: step 81100, loss = 18.40 (3.2 examples/sec; 5.015 sec/batch)
2018-05-06 05:29:51.769212: step 81110, loss = 17.75 (3.2 examples/sec; 4.963 sec/batch)
2018-05-06 05:30:41.355030: step 81120, loss = 18.29 (3.2 examples/sec; 5.035 sec/batch)
2018-05-06 05:31:30.544790: step 81130, loss = 17.99 (3.3 examples/sec; 4.812 sec/batch)
2018-05-06 05:32:19.794162: step 81140, loss = 18.00 (3.3 examples/sec; 4.786 sec/batch)
2018-05-06 05:33:08.769928: step 81150, loss = 17.89 (3.2 examples/sec; 4.998 sec/batch)
2018-05-06 05:33:58.869493: step 81160, loss = 17.88 (3.3 examples/sec; 4.864 sec/batch)
2018-05-06 05:34:48.087294: step 81170, loss = 17.92 (3.3 examples/sec; 4.883 sec/batch)
2018-05-06 05:35:37.091545: step 81180, loss = 18.10 (3.3 examples/sec; 4.850 sec/batch)
2018-05-06 05:36:26.747912: step 81190, loss = 17.60 (3.2 examples/sec; 4.978 sec/batch)
2018-05-06 05:37:16.436921: step 81200, loss = 17.86 (3.2 examples/sec; 5.048 sec/batch)
2018-05-06 05:38:09.496775: step 81210, loss = 17.65 (3.3 examples/sec; 4.878 sec/batch)
2018-05-06 05:38:56.257024: step 81220, loss = 17.63 (4.3 examples/sec; 3.718 sec/batch)
2018-05-06 05:39:44.603870: step 81230, loss = 18.28 (3.2 examples/sec; 4.995 sec/batch)
2018-05-06 05:40:34.179457: step 81240, loss = 17.80 (3.3 examples/sec; 4.899 sec/batch)
2018-05-06 05:41:23.873082: step 81250, loss = 17.77 (3.3 examples/sec; 4.920 sec/batch)
2018-05-06 05:42:13.482941: step 81260, loss = 17.66 (3.3 examples/sec; 4.848 sec/batch)
2018-05-06 05:43:03.009924: step 81270, loss = 17.91 (3.2 examples/sec; 4.955 sec/batch)
2018-05-06 05:43:52.153229: step 81280, loss = 17.96 (3.2 examples/sec; 4.956 sec/batch)
2018-05-06 05:44:41.583462: step 81290, loss = 18.12 (3.2 examples/sec; 4.963 sec/batch)
2018-05-06 05:45:31.146149: step 81300, loss = 17.86 (3.2 examples/sec; 4.987 sec/batch)
2018-05-06 05:46:24.240672: step 81310, loss = 17.81 (3.2 examples/sec; 4.966 sec/batch)
2018-05-06 05:47:13.695681: step 81320, loss = 17.93 (3.2 examples/sec; 5.078 sec/batch)
2018-05-06 05:48:03.531860: step 81330, loss = 18.06 (3.1 examples/sec; 5.106 sec/batch)
2018-05-06 05:48:53.018678: step 81340, loss = 18.69 (3.2 examples/sec; 4.956 sec/batch)
2018-05-06 05:49:39.167814: step 81350, loss = 18.67 (3.2 examples/sec; 4.963 sec/batch)
2018-05-06 05:50:28.524886: step 81360, loss = 18.45 (3.2 examples/sec; 4.955 sec/batch)
2018-05-06 05:51:17.469629: step 81370, loss = 18.07 (3.3 examples/sec; 4.859 sec/batch)
2018-05-06 05:52:06.531729: step 81380, loss = 18.11 (3.2 examples/sec; 5.055 sec/batch)
2018-05-06 05:52:56.419975: step 81390, loss = 17.70 (3.1 examples/sec; 5.092 sec/batch)
2018-05-06 05:53:45.995870: step 81400, loss = 17.95 (3.2 examples/sec; 5.066 sec/batch)
2018-05-06 05:54:38.738615: step 81410, loss = 18.04 (3.3 examples/sec; 4.796 sec/batch)
2018-05-06 05:55:27.978072: step 81420, loss = 18.06 (3.2 examples/sec; 4.963 sec/batch)
2018-05-06 05:56:17.590532: step 81430, loss = 18.08 (3.2 examples/sec; 5.022 sec/batch)
2018-05-06 05:57:07.325150: step 81440, loss = 18.21 (3.2 examples/sec; 5.000 sec/batch)
2018-05-06 05:57:56.329776: step 81450, loss = 17.88 (3.3 examples/sec; 4.917 sec/batch)
2018-05-06 05:58:45.285014: step 81460, loss = 18.20 (3.4 examples/sec; 4.700 sec/batch)
2018-05-06 05:59:33.019827: step 81470, loss = 18.47 (4.2 examples/sec; 3.809 sec/batch)
2018-05-06 06:00:19.962809: step 81480, loss = 17.78 (3.3 examples/sec; 4.817 sec/batch)
2018-05-06 06:01:09.143595: step 81490, loss = 17.88 (3.2 examples/sec; 4.986 sec/batch)
2018-05-06 06:01:58.186331: step 81500, loss = 18.04 (3.2 examples/sec; 4.998 sec/batch)
2018-05-06 06:02:50.918196: step 81510, loss = 18.14 (3.4 examples/sec; 4.718 sec/batch)
2018-05-06 06:03:40.918236: step 81520, loss = 17.71 (3.2 examples/sec; 4.933 sec/batch)
2018-05-06 06:04:30.743146: step 81530, loss = 17.96 (3.3 examples/sec; 4.918 sec/batch)
2018-05-06 06:05:20.793211: step 81540, loss = 17.70 (3.1 examples/sec; 5.175 sec/batch)
2018-05-06 06:06:10.830678: step 81550, loss = 18.03 (3.2 examples/sec; 4.978 sec/batch)
2018-05-06 06:07:00.186845: step 81560, loss = 17.57 (3.2 examples/sec; 5.009 sec/batch)
2018-05-06 06:07:49.771611: step 81570, loss = 17.99 (3.2 examples/sec; 4.980 sec/batch)
2018-05-06 06:08:39.647984: step 81580, loss = 18.28 (3.2 examples/sec; 4.971 sec/batch)
2018-05-06 06:09:28.799720: step 81590, loss = 17.76 (3.2 examples/sec; 4.926 sec/batch)
2018-05-06 06:10:14.859740: step 81600, loss = 17.90 (3.3 examples/sec; 4.900 sec/batch)
2018-05-06 06:11:07.436160: step 81610, loss = 17.86 (3.3 examples/sec; 4.826 sec/batch)
2018-05-06 06:11:56.921696: step 81620, loss = 17.64 (3.1 examples/sec; 5.085 sec/batch)
2018-05-06 06:12:45.917215: step 81630, loss = 18.13 (3.3 examples/sec; 4.800 sec/batch)
2018-05-06 06:13:35.868188: step 81640, loss = 17.62 (3.3 examples/sec; 4.910 sec/batch)
2018-05-06 06:14:25.667263: step 81650, loss = 17.97 (3.2 examples/sec; 4.935 sec/batch)
2018-05-06 06:15:15.075029: step 81660, loss = 17.60 (3.2 examples/sec; 5.062 sec/batch)
2018-05-06 06:16:04.369822: step 81670, loss = 17.70 (3.2 examples/sec; 5.001 sec/batch)
2018-05-06 06:16:53.351157: step 81680, loss = 17.86 (3.1 examples/sec; 5.126 sec/batch)
2018-05-06 06:17:42.899042: step 81690, loss = 17.90 (3.2 examples/sec; 4.974 sec/batch)
2018-05-06 06:18:31.899354: step 81700, loss = 17.73 (3.2 examples/sec; 4.950 sec/batch)
2018-05-06 06:19:24.728835: step 81710, loss = 17.62 (3.3 examples/sec; 4.847 sec/batch)
2018-05-06 06:20:12.676940: step 81720, loss = 18.57 (4.3 examples/sec; 3.707 sec/batch)
2018-05-06 06:20:59.843619: step 81730, loss = 18.13 (3.2 examples/sec; 5.037 sec/batch)
2018-05-06 06:21:50.747061: step 81740, loss = 18.18 (3.2 examples/sec; 4.957 sec/batch)
2018-05-06 06:22:40.359042: step 81750, loss = 17.82 (3.2 examples/sec; 4.923 sec/batch)
2018-05-06 06:23:29.995438: step 81760, loss = 17.90 (3.2 examples/sec; 4.944 sec/batch)
2018-05-06 06:24:19.044073: step 81770, loss = 17.85 (3.2 examples/sec; 4.937 sec/batch)
2018-05-06 06:25:08.460630: step 81780, loss = 17.90 (3.1 examples/sec; 5.089 sec/batch)
2018-05-06 06:25:57.250295: step 81790, loss = 18.11 (3.3 examples/sec; 4.895 sec/batch)
2018-05-06 06:26:46.509115: step 81800, loss = 17.68 (3.2 examples/sec; 4.940 sec/batch)
2018-05-06 06:27:39.436019: step 81810, loss = 17.89 (3.2 examples/sec; 4.948 sec/batch)
2018-05-06 06:28:29.245928: step 81820, loss = 17.83 (3.2 examples/sec; 4.947 sec/batch)
2018-05-06 06:29:18.975101: step 81830, loss = 18.02 (3.1 examples/sec; 5.097 sec/batch)
2018-05-06 06:30:08.303822: step 81840, loss = 17.75 (3.2 examples/sec; 5.034 sec/batch)
2018-05-06 06:30:54.161838: step 81850, loss = 17.70 (3.3 examples/sec; 4.912 sec/batch)
2018-05-06 06:31:43.624166: step 81860, loss = 17.96 (3.3 examples/sec; 4.825 sec/batch)
2018-05-06 06:32:32.923777: step 81870, loss = 17.57 (3.2 examples/sec; 4.973 sec/batch)
2018-05-06 06:33:21.926633: step 81880, loss = 17.86 (3.2 examples/sec; 5.014 sec/batch)
2018-05-06 06:34:11.535826: step 81890, loss = 17.98 (3.2 examples/sec; 5.008 sec/batch)
2018-05-06 06:35:01.897443: step 81900, loss = 17.64 (3.2 examples/sec; 5.030 sec/batch)
2018-05-06 06:35:54.865747: step 81910, loss = 18.02 (3.3 examples/sec; 4.873 sec/batch)
2018-05-06 06:36:44.190556: step 81920, loss = 18.18 (3.3 examples/sec; 4.919 sec/batch)
2018-05-06 06:37:33.664559: step 81930, loss = 17.82 (3.3 examples/sec; 4.815 sec/batch)
2018-05-06 06:38:22.353689: step 81940, loss = 18.08 (3.3 examples/sec; 4.903 sec/batch)
2018-05-06 06:39:11.688995: step 81950, loss = 17.78 (3.3 examples/sec; 4.856 sec/batch)
2018-05-06 06:40:01.434143: step 81960, loss = 18.20 (3.0 examples/sec; 5.250 sec/batch)
2018-05-06 06:40:50.919576: step 81970, loss = 18.21 (3.2 examples/sec; 4.981 sec/batch)
2018-05-06 06:41:39.691808: step 81980, loss = 17.88 (3.3 examples/sec; 4.848 sec/batch)
2018-05-06 06:42:28.958179: step 81990, loss = 17.88 (3.4 examples/sec; 4.772 sec/batch)
2018-05-06 06:43:18.891877: step 82000, loss = 17.66 (3.3 examples/sec; 4.875 sec/batch)
2018-05-06 06:44:12.185030: step 82010, loss = 18.25 (3.2 examples/sec; 4.980 sec/batch)
2018-05-06 06:45:02.025935: step 82020, loss = 18.08 (3.1 examples/sec; 5.094 sec/batch)
2018-05-06 06:45:52.041884: step 82030, loss = 17.55 (3.1 examples/sec; 5.244 sec/batch)
2018-05-06 06:46:41.826406: step 82040, loss = 18.03 (3.2 examples/sec; 5.034 sec/batch)
2018-05-06 06:47:31.634539: step 82050, loss = 18.05 (3.3 examples/sec; 4.886 sec/batch)
2018-05-06 06:48:21.676846: step 82060, loss = 18.49 (3.2 examples/sec; 4.955 sec/batch)
2018-05-06 06:49:10.690100: step 82070, loss = 18.17 (3.2 examples/sec; 5.004 sec/batch)
2018-05-06 06:50:00.380752: step 82080, loss = 17.88 (3.2 examples/sec; 5.062 sec/batch)
2018-05-06 06:50:49.265139: step 82090, loss = 17.58 (3.2 examples/sec; 4.933 sec/batch)
2018-05-06 06:51:34.999031: step 82100, loss = 17.79 (3.3 examples/sec; 4.777 sec/batch)
2018-05-06 06:52:28.392298: step 82110, loss = 17.54 (3.2 examples/sec; 5.008 sec/batch)
2018-05-06 06:53:17.209018: step 82120, loss = 18.42 (3.4 examples/sec; 4.671 sec/batch)
2018-05-06 06:54:06.821750: step 82130, loss = 18.03 (3.2 examples/sec; 4.992 sec/batch)
2018-05-06 06:54:56.355967: step 82140, loss = 18.03 (3.2 examples/sec; 5.031 sec/batch)
2018-05-06 06:55:45.671906: step 82150, loss = 18.16 (3.2 examples/sec; 4.995 sec/batch)
2018-05-06 06:56:34.656714: step 82160, loss = 17.84 (3.3 examples/sec; 4.870 sec/batch)
2018-05-06 06:57:24.193018: step 82170, loss = 18.04 (3.3 examples/sec; 4.904 sec/batch)
2018-05-06 06:58:13.038233: step 82180, loss = 18.80 (3.3 examples/sec; 4.861 sec/batch)
2018-05-06 06:59:02.361669: step 82190, loss = 17.71 (3.4 examples/sec; 4.748 sec/batch)
2018-05-06 06:59:51.377907: step 82200, loss = 17.92 (3.2 examples/sec; 4.951 sec/batch)
2018-05-06 07:00:44.737175: step 82210, loss = 17.80 (3.2 examples/sec; 5.076 sec/batch)
2018-05-06 07:01:33.665919: step 82220, loss = 17.86 (3.3 examples/sec; 4.894 sec/batch)
2018-05-06 07:02:19.559302: step 82230, loss = 18.15 (3.1 examples/sec; 5.088 sec/batch)
2018-05-06 07:03:09.379923: step 82240, loss = 17.75 (3.2 examples/sec; 4.945 sec/batch)
2018-05-06 07:03:58.399592: step 82250, loss = 18.40 (3.2 examples/sec; 4.998 sec/batch)
2018-05-06 07:04:47.884673: step 82260, loss = 17.98 (3.2 examples/sec; 5.003 sec/batch)
2018-05-06 07:05:37.349155: step 82270, loss = 17.77 (3.2 examples/sec; 5.020 sec/batch)
2018-05-06 07:06:26.082394: step 82280, loss = 18.84 (3.2 examples/sec; 4.929 sec/batch)
2018-05-06 07:07:14.786331: step 82290, loss = 18.14 (3.3 examples/sec; 4.806 sec/batch)
2018-05-06 07:08:03.990937: step 82300, loss = 17.62 (3.3 examples/sec; 4.826 sec/batch)
2018-05-06 07:08:57.377295: step 82310, loss = 17.59 (3.1 examples/sec; 5.126 sec/batch)
2018-05-06 07:09:47.309067: step 82320, loss = 17.96 (3.2 examples/sec; 5.011 sec/batch)
2018-05-06 07:10:36.461721: step 82330, loss = 17.79 (3.2 examples/sec; 5.007 sec/batch)
2018-05-06 07:11:25.756870: step 82340, loss = 17.72 (3.3 examples/sec; 4.871 sec/batch)
2018-05-06 07:12:11.067045: step 82350, loss = 17.81 (3.5 examples/sec; 4.585 sec/batch)
2018-05-06 07:13:00.233305: step 82360, loss = 17.94 (3.4 examples/sec; 4.696 sec/batch)
2018-05-06 07:13:49.326686: step 82370, loss = 17.74 (3.2 examples/sec; 5.018 sec/batch)
2018-05-06 07:14:39.002084: step 82380, loss = 18.39 (3.2 examples/sec; 4.948 sec/batch)
2018-05-06 07:15:29.560652: step 82390, loss = 18.12 (3.3 examples/sec; 4.911 sec/batch)
2018-05-06 07:16:19.029833: step 82400, loss = 18.34 (3.2 examples/sec; 4.950 sec/batch)
2018-05-06 07:17:11.942869: step 82410, loss = 18.54 (3.3 examples/sec; 4.899 sec/batch)
2018-05-06 07:18:01.336757: step 82420, loss = 19.07 (3.2 examples/sec; 5.041 sec/batch)
2018-05-06 07:18:50.847534: step 82430, loss = 17.62 (3.3 examples/sec; 4.883 sec/batch)
2018-05-06 07:19:40.420487: step 82440, loss = 18.33 (3.2 examples/sec; 5.020 sec/batch)
2018-05-06 07:20:28.915186: step 82450, loss = 17.88 (3.2 examples/sec; 5.026 sec/batch)
2018-05-06 07:21:18.623740: step 82460, loss = 17.66 (3.2 examples/sec; 4.963 sec/batch)
2018-05-06 07:22:08.475522: step 82470, loss = 18.78 (3.2 examples/sec; 4.971 sec/batch)
2018-05-06 07:22:54.501655: step 82480, loss = 18.09 (3.2 examples/sec; 4.989 sec/batch)
2018-05-06 07:23:43.659745: step 82490, loss = 17.89 (3.3 examples/sec; 4.867 sec/batch)
2018-05-06 07:24:32.462480: step 82500, loss = 17.82 (3.3 examples/sec; 4.902 sec/batch)
2018-05-06 07:25:25.187089: step 82510, loss = 17.77 (3.2 examples/sec; 4.955 sec/batch)
2018-05-06 07:26:14.165560: step 82520, loss = 18.18 (3.2 examples/sec; 4.982 sec/batch)
2018-05-06 07:27:02.948727: step 82530, loss = 17.97 (3.2 examples/sec; 4.935 sec/batch)
2018-05-06 07:27:52.699978: step 82540, loss = 18.16 (3.2 examples/sec; 4.936 sec/batch)
2018-05-06 07:28:42.331675: step 82550, loss = 17.91 (3.2 examples/sec; 4.928 sec/batch)
2018-05-06 07:29:31.455826: step 82560, loss = 17.87 (3.2 examples/sec; 5.005 sec/batch)
2018-05-06 07:30:20.605543: step 82570, loss = 18.33 (3.3 examples/sec; 4.905 sec/batch)
2018-05-06 07:31:09.917686: step 82580, loss = 18.59 (3.1 examples/sec; 5.150 sec/batch)
2018-05-06 07:31:59.660989: step 82590, loss = 17.62 (3.3 examples/sec; 4.894 sec/batch)
2018-05-06 07:32:46.466283: step 82600, loss = 18.03 (4.1 examples/sec; 3.924 sec/batch)
2018-05-06 07:33:39.462828: step 82610, loss = 17.69 (3.2 examples/sec; 4.954 sec/batch)
2018-05-06 07:34:28.585107: step 82620, loss = 18.13 (3.2 examples/sec; 5.054 sec/batch)
2018-05-06 07:35:18.393480: step 82630, loss = 18.46 (3.2 examples/sec; 4.974 sec/batch)
2018-05-06 07:36:07.893443: step 82640, loss = 18.49 (3.2 examples/sec; 5.018 sec/batch)
2018-05-06 07:36:56.991457: step 82650, loss = 18.50 (3.2 examples/sec; 5.047 sec/batch)
2018-05-06 07:37:46.211367: step 82660, loss = 18.23 (3.2 examples/sec; 4.980 sec/batch)
2018-05-06 07:38:35.500118: step 82670, loss = 17.78 (3.2 examples/sec; 5.032 sec/batch)
2018-05-06 07:39:24.613322: step 82680, loss = 17.59 (3.3 examples/sec; 4.908 sec/batch)
2018-05-06 07:40:13.863311: step 82690, loss = 17.70 (3.3 examples/sec; 4.864 sec/batch)
2018-05-06 07:41:03.549949: step 82700, loss = 18.46 (3.2 examples/sec; 5.021 sec/batch)
2018-05-06 07:41:56.046251: step 82710, loss = 18.10 (3.3 examples/sec; 4.831 sec/batch)
2018-05-06 07:42:45.282985: step 82720, loss = 18.56 (3.2 examples/sec; 4.933 sec/batch)
2018-05-06 07:43:31.345851: step 82730, loss = 18.36 (3.1 examples/sec; 5.086 sec/batch)
2018-05-06 07:44:20.593464: step 82740, loss = 17.90 (3.2 examples/sec; 4.975 sec/batch)
2018-05-06 07:45:10.249378: step 82750, loss = 18.52 (3.2 examples/sec; 5.021 sec/batch)
2018-05-06 07:45:59.274522: step 82760, loss = 17.66 (3.1 examples/sec; 5.167 sec/batch)
2018-05-06 07:46:49.282067: step 82770, loss = 17.72 (3.2 examples/sec; 4.986 sec/batch)
2018-05-06 07:47:39.146123: step 82780, loss = 17.97 (3.1 examples/sec; 5.102 sec/batch)
2018-05-06 07:48:28.572369: step 82790, loss = 18.26 (3.2 examples/sec; 5.056 sec/batch)
2018-05-06 07:49:17.479982: step 82800, loss = 17.98 (3.2 examples/sec; 5.003 sec/batch)
2018-05-06 07:50:10.012871: step 82810, loss = 18.11 (3.2 examples/sec; 5.008 sec/batch)
2018-05-06 07:50:58.687787: step 82820, loss = 17.87 (3.2 examples/sec; 4.960 sec/batch)
2018-05-06 07:51:47.743551: step 82830, loss = 18.53 (3.3 examples/sec; 4.824 sec/batch)
2018-05-06 07:52:37.044240: step 82840, loss = 17.75 (3.3 examples/sec; 4.891 sec/batch)
2018-05-06 07:53:24.075107: step 82850, loss = 18.10 (4.2 examples/sec; 3.776 sec/batch)
2018-05-06 07:54:13.519939: step 82860, loss = 18.02 (3.2 examples/sec; 4.929 sec/batch)
2018-05-06 07:55:03.486103: step 82870, loss = 17.98 (3.2 examples/sec; 4.960 sec/batch)
2018-05-06 07:55:53.418449: step 82880, loss = 18.16 (2.9 examples/sec; 5.502 sec/batch)
2018-05-06 07:56:42.563341: step 82890, loss = 17.80 (3.3 examples/sec; 4.854 sec/batch)
2018-05-06 07:57:31.245803: step 82900, loss = 18.06 (3.3 examples/sec; 4.861 sec/batch)
2018-05-06 07:58:24.261901: step 82910, loss = 18.24 (3.2 examples/sec; 4.934 sec/batch)
2018-05-06 07:59:13.030701: step 82920, loss = 17.96 (3.3 examples/sec; 4.874 sec/batch)
2018-05-06 08:00:02.030816: step 82930, loss = 18.21 (3.2 examples/sec; 5.009 sec/batch)
2018-05-06 08:00:50.773159: step 82940, loss = 17.81 (3.2 examples/sec; 4.930 sec/batch)
2018-05-06 08:01:39.913223: step 82950, loss = 17.67 (3.2 examples/sec; 4.983 sec/batch)
2018-05-06 08:02:29.738402: step 82960, loss = 17.82 (3.3 examples/sec; 4.853 sec/batch)
2018-05-06 08:03:19.076555: step 82970, loss = 17.70 (3.3 examples/sec; 4.825 sec/batch)
2018-05-06 08:04:04.978180: step 82980, loss = 17.89 (3.3 examples/sec; 4.914 sec/batch)
2018-05-06 08:04:54.751357: step 82990, loss = 17.57 (3.2 examples/sec; 4.949 sec/batch)
2018-05-06 08:05:44.026008: step 83000, loss = 17.97 (3.2 examples/sec; 4.954 sec/batch)
2018-05-06 08:06:37.307429: step 83010, loss = 18.32 (3.3 examples/sec; 4.917 sec/batch)
2018-05-06 08:07:27.022835: step 83020, loss = 18.02 (3.2 examples/sec; 5.025 sec/batch)
2018-05-06 08:08:16.282435: step 83030, loss = 17.63 (3.3 examples/sec; 4.837 sec/batch)
2018-05-06 08:09:05.203856: step 83040, loss = 18.19 (3.2 examples/sec; 4.942 sec/batch)
2018-05-06 08:09:55.079528: step 83050, loss = 17.84 (3.3 examples/sec; 4.879 sec/batch)
2018-05-06 08:10:44.746405: step 83060, loss = 18.51 (3.2 examples/sec; 4.970 sec/batch)
2018-05-06 08:11:34.293881: step 83070, loss = 18.35 (3.3 examples/sec; 4.837 sec/batch)
2018-05-06 08:12:23.962524: step 83080, loss = 17.93 (3.3 examples/sec; 4.891 sec/batch)
2018-05-06 08:13:13.019504: step 83090, loss = 17.83 (3.2 examples/sec; 4.982 sec/batch)
2018-05-06 08:14:00.534008: step 83100, loss = 17.78 (4.2 examples/sec; 3.772 sec/batch)
2018-05-06 08:14:53.118790: step 83110, loss = 17.82 (3.2 examples/sec; 4.935 sec/batch)
2018-05-06 08:15:42.341869: step 83120, loss = 17.59 (3.2 examples/sec; 5.024 sec/batch)
2018-05-06 08:16:31.267779: step 83130, loss = 17.63 (3.3 examples/sec; 4.847 sec/batch)
2018-05-06 08:17:20.221791: step 83140, loss = 17.85 (3.2 examples/sec; 4.967 sec/batch)
2018-05-06 08:18:09.801028: step 83150, loss = 17.82 (3.2 examples/sec; 5.032 sec/batch)
2018-05-06 08:18:59.052780: step 83160, loss = 17.72 (3.1 examples/sec; 5.223 sec/batch)
2018-05-06 08:19:48.445123: step 83170, loss = 17.95 (3.3 examples/sec; 4.801 sec/batch)
2018-05-06 08:20:37.787396: step 83180, loss = 17.90 (3.3 examples/sec; 4.906 sec/batch)
2018-05-06 08:21:27.145811: step 83190, loss = 18.23 (3.2 examples/sec; 4.950 sec/batch)
2018-05-06 08:22:16.515855: step 83200, loss = 18.05 (3.2 examples/sec; 4.929 sec/batch)
2018-05-06 08:23:09.125548: step 83210, loss = 18.00 (3.1 examples/sec; 5.200 sec/batch)
2018-05-06 08:23:58.614404: step 83220, loss = 17.98 (3.3 examples/sec; 4.900 sec/batch)
2018-05-06 08:24:44.402104: step 83230, loss = 17.56 (3.2 examples/sec; 4.948 sec/batch)
2018-05-06 08:25:33.513790: step 83240, loss = 18.11 (3.4 examples/sec; 4.737 sec/batch)
2018-05-06 08:26:22.179699: step 83250, loss = 18.22 (3.3 examples/sec; 4.813 sec/batch)
2018-05-06 08:27:11.595489: step 83260, loss = 18.10 (3.3 examples/sec; 4.853 sec/batch)
2018-05-06 08:27:59.931384: step 83270, loss = 17.88 (3.4 examples/sec; 4.668 sec/batch)
2018-05-06 08:28:49.287478: step 83280, loss = 17.98 (3.3 examples/sec; 4.849 sec/batch)
2018-05-06 08:29:38.273006: step 83290, loss = 17.56 (3.2 examples/sec; 4.945 sec/batch)
2018-05-06 08:30:27.675865: step 83300, loss = 17.74 (3.4 examples/sec; 4.691 sec/batch)
2018-05-06 08:31:20.408410: step 83310, loss = 18.32 (3.1 examples/sec; 5.085 sec/batch)
2018-05-06 08:32:09.429282: step 83320, loss = 17.73 (3.3 examples/sec; 4.876 sec/batch)
2018-05-06 08:32:58.638308: step 83330, loss = 18.08 (3.2 examples/sec; 4.989 sec/batch)
2018-05-06 08:33:47.592075: step 83340, loss = 18.02 (3.2 examples/sec; 5.071 sec/batch)
2018-05-06 08:34:36.473677: step 83350, loss = 17.79 (4.0 examples/sec; 4.029 sec/batch)
2018-05-06 08:35:23.520471: step 83360, loss = 17.79 (3.2 examples/sec; 5.004 sec/batch)
2018-05-06 08:36:12.870402: step 83370, loss = 17.94 (3.3 examples/sec; 4.920 sec/batch)
2018-05-06 08:37:02.324004: step 83380, loss = 17.79 (3.3 examples/sec; 4.885 sec/batch)
2018-05-06 08:37:51.812405: step 83390, loss = 17.98 (3.2 examples/sec; 4.944 sec/batch)
2018-05-06 08:38:40.904801: step 83400, loss = 17.66 (3.2 examples/sec; 4.994 sec/batch)
2018-05-06 08:39:33.236049: step 83410, loss = 17.84 (3.3 examples/sec; 4.788 sec/batch)
2018-05-06 08:40:22.831209: step 83420, loss = 18.26 (3.2 examples/sec; 5.006 sec/batch)
2018-05-06 08:41:12.706730: step 83430, loss = 17.92 (3.3 examples/sec; 4.783 sec/batch)
2018-05-06 08:42:02.370264: step 83440, loss = 18.34 (3.2 examples/sec; 5.035 sec/batch)
2018-05-06 08:42:51.880008: step 83450, loss = 17.65 (3.2 examples/sec; 5.002 sec/batch)
2018-05-06 08:43:40.779811: step 83460, loss = 18.25 (3.2 examples/sec; 5.016 sec/batch)
2018-05-06 08:44:29.722865: step 83470, loss = 18.05 (3.3 examples/sec; 4.839 sec/batch)
2018-05-06 08:45:16.024280: step 83480, loss = 18.63 (3.2 examples/sec; 5.014 sec/batch)
2018-05-06 08:46:05.063508: step 83490, loss = 18.38 (3.3 examples/sec; 4.841 sec/batch)
2018-05-06 08:46:54.357581: step 83500, loss = 18.18 (3.3 examples/sec; 4.880 sec/batch)
2018-05-06 08:47:47.910453: step 83510, loss = 18.42 (3.3 examples/sec; 4.907 sec/batch)
2018-05-06 08:48:37.486058: step 83520, loss = 17.92 (3.3 examples/sec; 4.889 sec/batch)
2018-05-06 08:49:27.271518: step 83530, loss = 17.90 (3.2 examples/sec; 4.945 sec/batch)
2018-05-06 08:50:16.488834: step 83540, loss = 18.45 (3.1 examples/sec; 5.094 sec/batch)
2018-05-06 08:51:05.691043: step 83550, loss = 17.83 (3.2 examples/sec; 4.997 sec/batch)
2018-05-06 08:51:54.486036: step 83560, loss = 17.75 (3.3 examples/sec; 4.858 sec/batch)
2018-05-06 08:52:44.245821: step 83570, loss = 18.05 (3.4 examples/sec; 4.775 sec/batch)
2018-05-06 08:53:34.172396: step 83580, loss = 18.09 (3.2 examples/sec; 5.032 sec/batch)
2018-05-06 08:54:23.219246: step 83590, loss = 18.22 (3.3 examples/sec; 4.921 sec/batch)
2018-05-06 08:55:12.390426: step 83600, loss = 18.06 (3.2 examples/sec; 4.959 sec/batch)
2018-05-06 08:56:01.511155: step 83610, loss = 18.05 (3.3 examples/sec; 4.794 sec/batch)
2018-05-06 08:56:50.735712: step 83620, loss = 18.05 (3.2 examples/sec; 4.944 sec/batch)
2018-05-06 08:57:40.703129: step 83630, loss = 17.73 (3.1 examples/sec; 5.131 sec/batch)
2018-05-06 08:58:30.565608: step 83640, loss = 17.88 (3.2 examples/sec; 5.033 sec/batch)
2018-05-06 08:59:19.577411: step 83650, loss = 18.65 (3.2 examples/sec; 5.029 sec/batch)
2018-05-06 09:00:09.596251: step 83660, loss = 17.95 (3.1 examples/sec; 5.089 sec/batch)
2018-05-06 09:00:59.248796: step 83670, loss = 18.15 (3.2 examples/sec; 4.990 sec/batch)
2018-05-06 09:01:48.252902: step 83680, loss = 18.08 (3.3 examples/sec; 4.860 sec/batch)
2018-05-06 09:02:37.655225: step 83690, loss = 17.89 (3.3 examples/sec; 4.879 sec/batch)
2018-05-06 09:03:26.956937: step 83700, loss = 18.65 (3.2 examples/sec; 4.962 sec/batch)
2018-05-06 09:04:19.929138: step 83710, loss = 17.85 (3.1 examples/sec; 5.084 sec/batch)
2018-05-06 09:05:09.353935: step 83720, loss = 17.93 (3.3 examples/sec; 4.850 sec/batch)
2018-05-06 09:05:55.158559: step 83730, loss = 17.63 (3.2 examples/sec; 4.951 sec/batch)
2018-05-06 09:06:44.585962: step 83740, loss = 18.08 (3.3 examples/sec; 4.827 sec/batch)
2018-05-06 09:07:34.018253: step 83750, loss = 17.82 (3.3 examples/sec; 4.841 sec/batch)
2018-05-06 09:08:23.454894: step 83760, loss = 17.99 (3.1 examples/sec; 5.093 sec/batch)
2018-05-06 09:09:12.732158: step 83770, loss = 18.49 (3.4 examples/sec; 4.774 sec/batch)
2018-05-06 09:10:01.516159: step 83780, loss = 18.01 (3.3 examples/sec; 4.803 sec/batch)
2018-05-06 09:10:51.555842: step 83790, loss = 17.99 (3.2 examples/sec; 5.047 sec/batch)
2018-05-06 09:11:40.996232: step 83800, loss = 18.27 (3.2 examples/sec; 5.018 sec/batch)
2018-05-06 09:12:34.517692: step 83810, loss = 18.13 (3.2 examples/sec; 5.026 sec/batch)
2018-05-06 09:13:23.211940: step 83820, loss = 18.16 (3.2 examples/sec; 4.977 sec/batch)
2018-05-06 09:14:12.709065: step 83830, loss = 18.03 (3.3 examples/sec; 4.842 sec/batch)
2018-05-06 09:15:02.307546: step 83840, loss = 18.70 (3.2 examples/sec; 4.962 sec/batch)
2018-05-06 09:15:51.918701: step 83850, loss = 17.76 (3.3 examples/sec; 4.849 sec/batch)
2018-05-06 09:16:37.986609: step 83860, loss = 17.72 (3.2 examples/sec; 5.002 sec/batch)
2018-05-06 09:17:27.881699: step 83870, loss = 17.69 (3.2 examples/sec; 4.991 sec/batch)
2018-05-06 09:18:16.894498: step 83880, loss = 18.21 (3.3 examples/sec; 4.873 sec/batch)
2018-05-06 09:19:05.973140: step 83890, loss = 17.69 (3.3 examples/sec; 4.887 sec/batch)
2018-05-06 09:19:55.301181: step 83900, loss = 18.28 (3.2 examples/sec; 4.951 sec/batch)
2018-05-06 09:20:47.732662: step 83910, loss = 17.93 (3.3 examples/sec; 4.866 sec/batch)
2018-05-06 09:21:37.381470: step 83920, loss = 17.72 (3.2 examples/sec; 5.019 sec/batch)
2018-05-06 09:22:25.973221: step 83930, loss = 18.29 (3.3 examples/sec; 4.881 sec/batch)
2018-05-06 09:23:15.402017: step 83940, loss = 18.31 (3.2 examples/sec; 4.933 sec/batch)
2018-05-06 09:24:04.879648: step 83950, loss = 18.35 (3.2 examples/sec; 4.926 sec/batch)
2018-05-06 09:24:53.649012: step 83960, loss = 18.41 (3.3 examples/sec; 4.896 sec/batch)
2018-05-06 09:25:43.125240: step 83970, loss = 17.99 (3.3 examples/sec; 4.826 sec/batch)
2018-05-06 09:26:30.045053: step 83980, loss = 17.92 (4.2 examples/sec; 3.822 sec/batch)
2018-05-06 09:27:19.269491: step 83990, loss = 17.87 (3.2 examples/sec; 4.971 sec/batch)
2018-05-06 09:28:08.434056: step 84000, loss = 17.86 (3.4 examples/sec; 4.711 sec/batch)
2018-05-06 09:29:00.669532: step 84010, loss = 18.01 (3.3 examples/sec; 4.868 sec/batch)
2018-05-06 09:29:49.711157: step 84020, loss = 17.62 (3.4 examples/sec; 4.652 sec/batch)
2018-05-06 09:30:39.695373: step 84030, loss = 18.00 (3.1 examples/sec; 5.084 sec/batch)
2018-05-06 09:31:28.937991: step 84040, loss = 17.76 (3.3 examples/sec; 4.815 sec/batch)
2018-05-06 09:32:17.849126: step 84050, loss = 17.74 (3.5 examples/sec; 4.614 sec/batch)
2018-05-06 09:33:07.390706: step 84060, loss = 17.68 (3.2 examples/sec; 4.947 sec/batch)
2018-05-06 09:33:57.526983: step 84070, loss = 18.00 (3.3 examples/sec; 4.777 sec/batch)
2018-05-06 09:34:46.983738: step 84080, loss = 17.99 (3.2 examples/sec; 4.926 sec/batch)
2018-05-06 09:35:36.065710: step 84090, loss = 17.69 (3.3 examples/sec; 4.887 sec/batch)
2018-05-06 09:36:25.336619: step 84100, loss = 18.36 (3.3 examples/sec; 4.922 sec/batch)
2018-05-06 09:37:15.193522: step 84110, loss = 18.17 (3.2 examples/sec; 4.935 sec/batch)
2018-05-06 09:38:05.114342: step 84120, loss = 18.12 (3.2 examples/sec; 4.962 sec/batch)
2018-05-06 09:38:54.390844: step 84130, loss = 18.10 (3.2 examples/sec; 4.955 sec/batch)
2018-05-06 09:39:43.432627: step 84140, loss = 18.04 (3.3 examples/sec; 4.784 sec/batch)
2018-05-06 09:40:33.190207: step 84150, loss = 17.87 (3.2 examples/sec; 5.068 sec/batch)
2018-05-06 09:41:22.803961: step 84160, loss = 17.78 (3.3 examples/sec; 4.866 sec/batch)
2018-05-06 09:42:12.128828: step 84170, loss = 18.01 (3.2 examples/sec; 5.013 sec/batch)
2018-05-06 09:43:01.803422: step 84180, loss = 17.65 (3.2 examples/sec; 5.034 sec/batch)
2018-05-06 09:43:51.179672: step 84190, loss = 17.86 (3.3 examples/sec; 4.792 sec/batch)
2018-05-06 09:44:40.699365: step 84200, loss = 17.67 (3.3 examples/sec; 4.817 sec/batch)
2018-05-06 09:45:33.501656: step 84210, loss = 17.84 (3.3 examples/sec; 4.844 sec/batch)
2018-05-06 09:46:22.852615: step 84220, loss = 18.12 (3.1 examples/sec; 5.103 sec/batch)
2018-05-06 09:47:09.631814: step 84230, loss = 17.69 (4.3 examples/sec; 3.730 sec/batch)
2018-05-06 09:47:59.031630: step 84240, loss = 17.98 (3.2 examples/sec; 5.022 sec/batch)
2018-05-06 09:48:48.048015: step 84250, loss = 17.98 (3.3 examples/sec; 4.802 sec/batch)
2018-05-06 09:49:38.229364: step 84260, loss = 18.07 (3.2 examples/sec; 5.024 sec/batch)
2018-05-06 09:50:27.786430: step 84270, loss = 18.23 (3.3 examples/sec; 4.824 sec/batch)
2018-05-06 09:51:18.108625: step 84280, loss = 17.79 (3.2 examples/sec; 5.051 sec/batch)
2018-05-06 09:52:07.703160: step 84290, loss = 17.76 (3.1 examples/sec; 5.102 sec/batch)
2018-05-06 09:52:57.405531: step 84300, loss = 17.58 (3.3 examples/sec; 4.920 sec/batch)
2018-05-06 09:53:50.238056: step 84310, loss = 17.77 (3.3 examples/sec; 4.853 sec/batch)
2018-05-06 09:54:39.593373: step 84320, loss = 18.79 (3.3 examples/sec; 4.855 sec/batch)
2018-05-06 09:55:28.500007: step 84330, loss = 17.86 (3.3 examples/sec; 4.790 sec/batch)
2018-05-06 09:56:18.299954: step 84340, loss = 18.11 (3.2 examples/sec; 4.932 sec/batch)
2018-05-06 09:57:07.590473: step 84350, loss = 18.12 (3.3 examples/sec; 4.776 sec/batch)
2018-05-06 09:57:53.082531: step 84360, loss = 17.83 (3.4 examples/sec; 4.726 sec/batch)
2018-05-06 09:58:42.499899: step 84370, loss = 17.96 (3.2 examples/sec; 5.001 sec/batch)
2018-05-06 09:59:31.480004: step 84380, loss = 17.67 (3.3 examples/sec; 4.900 sec/batch)
2018-05-06 10:00:20.834536: step 84390, loss = 17.79 (3.3 examples/sec; 4.893 sec/batch)
2018-05-06 10:01:11.068612: step 84400, loss = 18.11 (3.2 examples/sec; 4.982 sec/batch)
2018-05-06 10:02:04.183571: step 84410, loss = 17.91 (3.3 examples/sec; 4.890 sec/batch)
2018-05-06 10:02:53.762584: step 84420, loss = 18.12 (3.3 examples/sec; 4.857 sec/batch)
2018-05-06 10:03:44.024823: step 84430, loss = 17.91 (3.2 examples/sec; 5.063 sec/batch)
2018-05-06 10:04:33.020510: step 84440, loss = 18.05 (3.3 examples/sec; 4.846 sec/batch)
2018-05-06 10:05:23.766388: step 84450, loss = 17.89 (2.9 examples/sec; 5.429 sec/batch)
2018-05-06 10:06:13.277719: step 84460, loss = 17.94 (3.3 examples/sec; 4.839 sec/batch)
2018-05-06 10:07:02.811166: step 84470, loss = 18.08 (3.3 examples/sec; 4.819 sec/batch)
2018-05-06 10:07:48.635472: step 84480, loss = 18.15 (4.2 examples/sec; 3.817 sec/batch)
2018-05-06 10:08:38.470680: step 84490, loss = 17.94 (3.4 examples/sec; 4.728 sec/batch)
2018-05-06 10:09:28.362788: step 84500, loss = 18.58 (3.2 examples/sec; 4.963 sec/batch)
2018-05-06 10:10:20.691950: step 84510, loss = 17.96 (3.3 examples/sec; 4.781 sec/batch)
2018-05-06 10:11:09.968739: step 84520, loss = 18.03 (3.2 examples/sec; 4.928 sec/batch)
2018-05-06 10:11:59.593117: step 84530, loss = 18.03 (3.2 examples/sec; 4.977 sec/batch)
2018-05-06 10:12:48.402387: step 84540, loss = 17.60 (3.3 examples/sec; 4.867 sec/batch)
2018-05-06 10:13:37.814585: step 84550, loss = 18.35 (3.2 examples/sec; 4.951 sec/batch)
2018-05-06 10:14:27.598946: step 84560, loss = 18.01 (3.2 examples/sec; 5.009 sec/batch)
2018-05-06 10:15:17.204356: step 84570, loss = 17.70 (3.2 examples/sec; 4.993 sec/batch)
2018-05-06 10:16:06.816933: step 84580, loss = 17.61 (3.2 examples/sec; 5.078 sec/batch)
2018-05-06 10:16:56.782389: step 84590, loss = 18.08 (3.2 examples/sec; 4.991 sec/batch)
2018-05-06 10:17:46.373111: step 84600, loss = 17.92 (3.3 examples/sec; 4.907 sec/batch)
2018-05-06 10:18:35.922665: step 84610, loss = 17.65 (3.3 examples/sec; 4.846 sec/batch)
2018-05-06 10:19:25.208474: step 84620, loss = 17.72 (3.2 examples/sec; 5.024 sec/batch)
2018-05-06 10:20:14.830641: step 84630, loss = 17.93 (3.2 examples/sec; 4.971 sec/batch)
2018-05-06 10:21:04.120034: step 84640, loss = 18.25 (3.4 examples/sec; 4.721 sec/batch)
2018-05-06 10:21:53.675221: step 84650, loss = 18.41 (3.1 examples/sec; 5.131 sec/batch)
2018-05-06 10:22:42.836257: step 84660, loss = 18.67 (3.2 examples/sec; 4.932 sec/batch)
2018-05-06 10:23:32.473679: step 84670, loss = 17.78 (3.3 examples/sec; 4.793 sec/batch)
2018-05-06 10:24:21.677159: step 84680, loss = 17.97 (3.3 examples/sec; 4.901 sec/batch)
2018-05-06 10:25:11.197828: step 84690, loss = 18.19 (3.3 examples/sec; 4.918 sec/batch)
2018-05-06 10:26:00.137854: step 84700, loss = 17.76 (3.2 examples/sec; 4.977 sec/batch)
2018-05-06 10:26:52.822974: step 84710, loss = 18.71 (3.3 examples/sec; 4.911 sec/batch)
2018-05-06 10:27:42.029753: step 84720, loss = 17.74 (3.3 examples/sec; 4.848 sec/batch)
2018-05-06 10:28:28.520374: step 84730, loss = 17.81 (4.3 examples/sec; 3.711 sec/batch)
2018-05-06 10:29:17.670518: step 84740, loss = 18.04 (3.1 examples/sec; 5.159 sec/batch)
2018-05-06 10:30:07.065750: step 84750, loss = 17.63 (3.2 examples/sec; 4.981 sec/batch)
2018-05-06 10:30:57.010422: step 84760, loss = 18.06 (3.2 examples/sec; 5.059 sec/batch)
2018-05-06 10:31:46.614469: step 84770, loss = 17.74 (3.3 examples/sec; 4.888 sec/batch)
2018-05-06 10:32:36.499370: step 84780, loss = 18.24 (3.3 examples/sec; 4.907 sec/batch)
2018-05-06 10:33:25.567472: step 84790, loss = 17.67 (3.2 examples/sec; 4.943 sec/batch)
2018-05-06 10:34:14.788411: step 84800, loss = 18.39 (3.2 examples/sec; 4.968 sec/batch)
2018-05-06 10:35:07.556716: step 84810, loss = 17.78 (3.4 examples/sec; 4.768 sec/batch)
2018-05-06 10:35:57.135916: step 84820, loss = 17.65 (3.2 examples/sec; 5.007 sec/batch)
2018-05-06 10:36:46.898357: step 84830, loss = 18.12 (3.2 examples/sec; 5.076 sec/batch)
2018-05-06 10:37:36.815142: step 84840, loss = 18.00 (3.2 examples/sec; 4.933 sec/batch)
2018-05-06 10:38:26.895626: step 84850, loss = 17.77 (3.2 examples/sec; 4.932 sec/batch)
2018-05-06 10:39:12.996811: step 84860, loss = 18.44 (3.2 examples/sec; 4.988 sec/batch)
2018-05-06 10:40:02.742472: step 84870, loss = 17.83 (3.3 examples/sec; 4.900 sec/batch)
2018-05-06 10:40:52.317369: step 84880, loss = 18.35 (3.2 examples/sec; 4.989 sec/batch)
2018-05-06 10:41:41.918927: step 84890, loss = 17.96 (3.2 examples/sec; 4.983 sec/batch)
2018-05-06 10:42:31.497655: step 84900, loss = 17.82 (3.2 examples/sec; 4.931 sec/batch)
2018-05-06 10:43:24.378856: step 84910, loss = 17.75 (3.2 examples/sec; 4.950 sec/batch)
2018-05-06 10:44:14.011065: step 84920, loss = 18.08 (3.2 examples/sec; 4.955 sec/batch)
2018-05-06 10:45:03.923175: step 84930, loss = 17.67 (3.2 examples/sec; 4.938 sec/batch)
2018-05-06 10:45:53.619267: step 84940, loss = 17.93 (3.2 examples/sec; 4.970 sec/batch)
2018-05-06 10:46:43.786559: step 84950, loss = 17.66 (3.1 examples/sec; 5.139 sec/batch)
2018-05-06 10:47:33.751399: step 84960, loss = 18.37 (3.2 examples/sec; 5.014 sec/batch)
2018-05-06 10:48:23.475279: step 84970, loss = 18.49 (3.3 examples/sec; 4.821 sec/batch)
2018-05-06 10:49:09.306712: step 84980, loss = 18.25 (4.1 examples/sec; 3.880 sec/batch)
2018-05-06 10:49:58.422223: step 84990, loss = 18.57 (3.4 examples/sec; 4.765 sec/batch)
2018-05-06 10:50:48.633270: step 85000, loss = 17.75 (3.3 examples/sec; 4.890 sec/batch)
2018-05-06 10:51:41.906960: step 85010, loss = 17.97 (3.3 examples/sec; 4.901 sec/batch)
2018-05-06 10:52:31.384138: step 85020, loss = 17.74 (3.2 examples/sec; 4.970 sec/batch)
2018-05-06 10:53:20.721670: step 85030, loss = 17.85 (3.2 examples/sec; 4.990 sec/batch)
2018-05-06 10:54:10.890561: step 85040, loss = 18.32 (3.2 examples/sec; 4.952 sec/batch)
2018-05-06 10:55:00.646915: step 85050, loss = 18.04 (3.1 examples/sec; 5.161 sec/batch)
2018-05-06 10:55:50.484141: step 85060, loss = 17.64 (3.2 examples/sec; 4.936 sec/batch)
2018-05-06 10:56:40.875050: step 85070, loss = 17.71 (3.1 examples/sec; 5.111 sec/batch)
2018-05-06 10:57:30.569597: step 85080, loss = 18.15 (3.2 examples/sec; 5.054 sec/batch)
2018-05-06 10:58:20.391270: step 85090, loss = 17.71 (3.1 examples/sec; 5.124 sec/batch)
2018-05-06 10:59:10.116761: step 85100, loss = 17.80 (3.2 examples/sec; 4.943 sec/batch)
2018-05-06 11:00:00.751867: step 85110, loss = 17.67 (3.1 examples/sec; 5.159 sec/batch)
2018-05-06 11:00:49.915010: step 85120, loss = 18.30 (3.2 examples/sec; 4.959 sec/batch)
2018-05-06 11:01:39.138851: step 85130, loss = 18.96 (3.3 examples/sec; 4.867 sec/batch)
2018-05-06 11:02:28.429290: step 85140, loss = 17.66 (3.2 examples/sec; 5.019 sec/batch)
2018-05-06 11:03:17.811092: step 85150, loss = 17.99 (3.2 examples/sec; 4.991 sec/batch)
2018-05-06 11:04:07.593693: step 85160, loss = 17.89 (3.4 examples/sec; 4.771 sec/batch)
2018-05-06 11:04:56.871549: step 85170, loss = 18.02 (3.2 examples/sec; 5.018 sec/batch)
2018-05-06 11:05:46.284583: step 85180, loss = 18.42 (3.4 examples/sec; 4.754 sec/batch)
2018-05-06 11:06:35.505785: step 85190, loss = 17.90 (3.3 examples/sec; 4.847 sec/batch)
2018-05-06 11:07:24.340396: step 85200, loss = 18.14 (3.3 examples/sec; 4.817 sec/batch)
2018-05-06 11:08:17.223740: step 85210, loss = 17.69 (3.3 examples/sec; 4.899 sec/batch)
2018-05-06 11:09:06.716245: step 85220, loss = 18.05 (3.3 examples/sec; 4.789 sec/batch)
2018-05-06 11:09:52.903482: step 85230, loss = 18.29 (3.3 examples/sec; 4.912 sec/batch)
2018-05-06 11:10:41.894438: step 85240, loss = 18.47 (3.4 examples/sec; 4.768 sec/batch)
2018-05-06 11:11:31.115467: step 85250, loss = 17.89 (3.2 examples/sec; 4.944 sec/batch)
2018-05-06 11:12:21.125841: step 85260, loss = 18.19 (3.1 examples/sec; 5.200 sec/batch)
2018-05-06 11:13:10.420783: step 85270, loss = 17.78 (3.3 examples/sec; 4.847 sec/batch)
2018-05-06 11:13:59.487193: step 85280, loss = 18.05 (3.3 examples/sec; 4.869 sec/batch)
2018-05-06 11:14:48.486136: step 85290, loss = 17.96 (3.2 examples/sec; 4.982 sec/batch)
2018-05-06 11:15:38.056798: step 85300, loss = 18.13 (3.1 examples/sec; 5.105 sec/batch)
2018-05-06 11:16:31.452939: step 85310, loss = 18.34 (3.2 examples/sec; 5.005 sec/batch)
2018-05-06 11:17:21.283197: step 85320, loss = 17.59 (3.3 examples/sec; 4.808 sec/batch)
2018-05-06 11:18:11.103893: step 85330, loss = 18.26 (3.2 examples/sec; 5.038 sec/batch)
2018-05-06 11:19:00.616931: step 85340, loss = 17.94 (3.3 examples/sec; 4.833 sec/batch)
2018-05-06 11:19:49.518917: step 85350, loss = 18.09 (4.1 examples/sec; 3.886 sec/batch)
2018-05-06 11:20:35.955460: step 85360, loss = 17.95 (3.3 examples/sec; 4.850 sec/batch)
2018-05-06 11:21:25.878077: step 85370, loss = 17.57 (3.2 examples/sec; 4.972 sec/batch)
2018-05-06 11:22:16.305186: step 85380, loss = 18.07 (3.0 examples/sec; 5.339 sec/batch)
2018-05-06 11:23:05.953913: step 85390, loss = 18.06 (3.3 examples/sec; 4.880 sec/batch)
2018-05-06 11:23:55.842542: step 85400, loss = 18.09 (3.3 examples/sec; 4.897 sec/batch)
2018-05-06 11:24:48.423411: step 85410, loss = 17.85 (3.3 examples/sec; 4.846 sec/batch)
2018-05-06 11:25:38.449320: step 85420, loss = 17.99 (3.3 examples/sec; 4.857 sec/batch)
2018-05-06 11:26:28.263049: step 85430, loss = 17.79 (3.1 examples/sec; 5.085 sec/batch)
2018-05-06 11:27:18.200393: step 85440, loss = 17.77 (3.2 examples/sec; 5.008 sec/batch)
2018-05-06 11:28:08.165773: step 85450, loss = 17.63 (3.2 examples/sec; 4.996 sec/batch)
2018-05-06 11:28:58.128739: step 85460, loss = 18.38 (3.2 examples/sec; 5.042 sec/batch)
2018-05-06 11:29:47.518345: step 85470, loss = 17.83 (3.3 examples/sec; 4.862 sec/batch)
2018-05-06 11:30:32.735709: step 85480, loss = 17.95 (3.2 examples/sec; 4.997 sec/batch)
2018-05-06 11:31:22.365596: step 85490, loss = 17.89 (3.2 examples/sec; 4.988 sec/batch)
2018-05-06 11:32:12.072771: step 85500, loss = 17.93 (3.3 examples/sec; 4.897 sec/batch)
2018-05-06 11:33:05.800210: step 85510, loss = 17.67 (3.0 examples/sec; 5.275 sec/batch)
2018-05-06 11:33:55.519238: step 85520, loss = 17.71 (3.3 examples/sec; 4.867 sec/batch)
2018-05-06 11:34:45.118455: step 85530, loss = 17.99 (3.3 examples/sec; 4.882 sec/batch)
2018-05-06 11:35:34.118487: step 85540, loss = 17.85 (3.3 examples/sec; 4.906 sec/batch)
2018-05-06 11:36:23.328137: step 85550, loss = 17.60 (3.3 examples/sec; 4.910 sec/batch)
2018-05-06 11:37:12.987159: step 85560, loss = 18.15 (3.4 examples/sec; 4.766 sec/batch)
2018-05-06 11:38:02.872379: step 85570, loss = 18.30 (3.2 examples/sec; 4.947 sec/batch)
2018-05-06 11:38:52.353220: step 85580, loss = 17.99 (3.1 examples/sec; 5.189 sec/batch)
2018-05-06 11:39:42.002586: step 85590, loss = 17.92 (3.1 examples/sec; 5.148 sec/batch)
2018-05-06 11:40:28.135347: step 85600, loss = 17.99 (4.3 examples/sec; 3.678 sec/batch)
2018-05-06 11:41:20.672810: step 85610, loss = 18.56 (3.3 examples/sec; 4.829 sec/batch)
2018-05-06 11:42:09.978186: step 85620, loss = 17.73 (3.3 examples/sec; 4.843 sec/batch)
2018-05-06 11:43:00.048723: step 85630, loss = 18.01 (3.0 examples/sec; 5.271 sec/batch)
2018-05-06 11:43:49.421680: step 85640, loss = 17.67 (3.3 examples/sec; 4.906 sec/batch)
2018-05-06 11:44:38.622694: step 85650, loss = 17.96 (3.3 examples/sec; 4.810 sec/batch)
2018-05-06 11:45:28.803173: step 85660, loss = 18.03 (3.1 examples/sec; 5.108 sec/batch)
2018-05-06 11:46:18.295593: step 85670, loss = 17.84 (3.3 examples/sec; 4.806 sec/batch)
2018-05-06 11:47:06.955520: step 85680, loss = 17.96 (3.2 examples/sec; 5.075 sec/batch)
2018-05-06 11:47:56.010973: step 85690, loss = 17.89 (3.3 examples/sec; 4.888 sec/batch)
2018-05-06 11:48:45.444335: step 85700, loss = 18.00 (3.3 examples/sec; 4.912 sec/batch)
2018-05-06 11:49:39.084303: step 85710, loss = 17.95 (3.3 examples/sec; 4.866 sec/batch)
2018-05-06 11:50:28.853091: step 85720, loss = 18.44 (3.3 examples/sec; 4.914 sec/batch)
2018-05-06 11:51:14.840461: step 85730, loss = 17.84 (3.2 examples/sec; 4.982 sec/batch)
2018-05-06 11:52:04.599401: step 85740, loss = 18.17 (3.2 examples/sec; 4.964 sec/batch)
2018-05-06 11:52:54.173952: step 85750, loss = 17.75 (3.2 examples/sec; 4.960 sec/batch)
2018-05-06 11:53:43.000923: step 85760, loss = 17.90 (3.4 examples/sec; 4.698 sec/batch)
2018-05-06 11:54:32.357548: step 85770, loss = 18.29 (3.3 examples/sec; 4.891 sec/batch)
2018-05-06 11:55:21.617582: step 85780, loss = 17.85 (3.2 examples/sec; 5.047 sec/batch)
2018-05-06 11:56:11.175918: step 85790, loss = 18.12 (3.3 examples/sec; 4.905 sec/batch)
2018-05-06 11:57:00.126311: step 85800, loss = 18.44 (3.2 examples/sec; 4.939 sec/batch)
2018-05-06 11:57:53.032251: step 85810, loss = 17.75 (3.2 examples/sec; 4.931 sec/batch)
2018-05-06 11:58:42.137928: step 85820, loss = 18.05 (3.4 examples/sec; 4.750 sec/batch)
2018-05-06 11:59:31.197501: step 85830, loss = 17.84 (3.2 examples/sec; 4.964 sec/batch)
2018-05-06 12:00:21.213525: step 85840, loss = 17.95 (3.2 examples/sec; 4.938 sec/batch)
2018-05-06 12:01:07.109489: step 85850, loss = 17.69 (4.2 examples/sec; 3.835 sec/batch)
2018-05-06 12:01:56.479995: step 85860, loss = 18.05 (3.2 examples/sec; 4.979 sec/batch)
2018-05-06 12:02:45.575349: step 85870, loss = 17.51 (3.3 examples/sec; 4.906 sec/batch)
2018-05-06 12:03:34.712914: step 85880, loss = 18.05 (3.3 examples/sec; 4.859 sec/batch)
2018-05-06 12:04:24.368771: step 85890, loss = 18.01 (3.2 examples/sec; 5.013 sec/batch)
2018-05-06 12:05:13.668709: step 85900, loss = 18.04 (3.2 examples/sec; 4.986 sec/batch)
2018-05-06 12:06:06.111016: step 85910, loss = 17.62 (3.4 examples/sec; 4.750 sec/batch)
2018-05-06 12:06:55.755120: step 85920, loss = 17.84 (3.2 examples/sec; 5.075 sec/batch)
2018-05-06 12:07:45.085733: step 85930, loss = 17.77 (3.3 examples/sec; 4.825 sec/batch)
2018-05-06 12:08:34.739675: step 85940, loss = 17.95 (3.3 examples/sec; 4.866 sec/batch)
2018-05-06 12:09:24.596579: step 85950, loss = 17.92 (3.3 examples/sec; 4.923 sec/batch)
2018-05-06 12:10:13.920067: step 85960, loss = 17.56 (3.3 examples/sec; 4.832 sec/batch)
2018-05-06 12:11:03.872009: step 85970, loss = 18.27 (3.3 examples/sec; 4.918 sec/batch)
2018-05-06 12:11:49.943559: step 85980, loss = 17.77 (3.2 examples/sec; 4.928 sec/batch)
2018-05-06 12:12:39.923122: step 85990, loss = 18.08 (3.1 examples/sec; 5.085 sec/batch)
2018-05-06 12:13:28.398364: step 86000, loss = 18.02 (3.3 examples/sec; 4.780 sec/batch)
2018-05-06 12:14:21.357796: step 86010, loss = 17.97 (3.2 examples/sec; 4.976 sec/batch)
2018-05-06 12:15:10.123454: step 86020, loss = 18.07 (3.3 examples/sec; 4.900 sec/batch)
2018-05-06 12:15:59.495998: step 86030, loss = 18.35 (3.3 examples/sec; 4.887 sec/batch)
2018-05-06 12:16:48.854779: step 86040, loss = 18.14 (3.3 examples/sec; 4.891 sec/batch)
2018-05-06 12:17:37.964967: step 86050, loss = 17.68 (3.2 examples/sec; 4.955 sec/batch)
2018-05-06 12:18:27.420619: step 86060, loss = 18.60 (3.2 examples/sec; 5.011 sec/batch)
2018-05-06 12:19:16.369233: step 86070, loss = 18.70 (3.3 examples/sec; 4.900 sec/batch)
2018-05-06 12:20:05.710121: step 86080, loss = 18.65 (3.2 examples/sec; 4.932 sec/batch)
2018-05-06 12:20:55.331271: step 86090, loss = 18.28 (3.2 examples/sec; 5.049 sec/batch)
2018-05-06 12:21:43.674457: step 86100, loss = 17.68 (4.2 examples/sec; 3.776 sec/batch)
2018-05-06 12:22:34.698668: step 86110, loss = 17.87 (3.2 examples/sec; 4.968 sec/batch)
2018-05-06 12:23:23.732857: step 86120, loss = 18.51 (3.3 examples/sec; 4.891 sec/batch)
2018-05-06 12:24:13.371082: step 86130, loss = 18.08 (3.2 examples/sec; 4.988 sec/batch)
2018-05-06 12:25:02.503192: step 86140, loss = 18.02 (3.3 examples/sec; 4.919 sec/batch)
2018-05-06 12:25:51.486643: step 86150, loss = 17.96 (3.2 examples/sec; 4.979 sec/batch)
2018-05-06 12:26:40.464454: step 86160, loss = 17.59 (3.3 examples/sec; 4.793 sec/batch)
2018-05-06 12:27:29.254324: step 86170, loss = 18.18 (3.2 examples/sec; 4.931 sec/batch)
2018-05-06 12:28:18.729269: step 86180, loss = 17.99 (3.2 examples/sec; 4.985 sec/batch)
2018-05-06 12:29:08.286824: step 86190, loss = 17.67 (3.2 examples/sec; 4.960 sec/batch)
2018-05-06 12:29:57.528279: step 86200, loss = 17.87 (3.2 examples/sec; 4.983 sec/batch)
2018-05-06 12:30:50.073516: step 86210, loss = 18.01 (3.4 examples/sec; 4.754 sec/batch)
2018-05-06 12:31:39.114611: step 86220, loss = 17.86 (3.2 examples/sec; 4.957 sec/batch)
2018-05-06 12:32:24.840862: step 86230, loss = 18.07 (3.2 examples/sec; 4.979 sec/batch)
2018-05-06 12:33:14.620589: step 86240, loss = 18.30 (3.2 examples/sec; 4.988 sec/batch)
2018-05-06 12:34:04.685354: step 86250, loss = 18.08 (3.2 examples/sec; 4.973 sec/batch)
2018-05-06 12:34:54.009002: step 86260, loss = 17.90 (3.2 examples/sec; 5.009 sec/batch)
2018-05-06 12:35:43.588921: step 86270, loss = 18.46 (3.3 examples/sec; 4.829 sec/batch)
2018-05-06 12:36:32.847605: step 86280, loss = 18.28 (3.2 examples/sec; 5.053 sec/batch)
2018-05-06 12:37:21.310180: step 86290, loss = 17.92 (3.2 examples/sec; 4.978 sec/batch)
2018-05-06 12:38:10.527171: step 86300, loss = 18.02 (3.3 examples/sec; 4.866 sec/batch)
2018-05-06 12:39:03.165148: step 86310, loss = 18.26 (3.3 examples/sec; 4.798 sec/batch)
2018-05-06 12:39:52.945519: step 86320, loss = 18.03 (3.2 examples/sec; 4.953 sec/batch)
2018-05-06 12:40:42.032272: step 86330, loss = 18.18 (3.2 examples/sec; 4.970 sec/batch)
2018-05-06 12:41:31.066700: step 86340, loss = 18.17 (3.3 examples/sec; 4.916 sec/batch)
2018-05-06 12:42:20.171554: step 86350, loss = 17.63 (3.3 examples/sec; 4.816 sec/batch)
2018-05-06 12:43:05.868128: step 86360, loss = 17.91 (3.2 examples/sec; 4.971 sec/batch)
2018-05-06 12:43:55.457144: step 86370, loss = 18.22 (3.3 examples/sec; 4.904 sec/batch)
2018-05-06 12:44:44.839267: step 86380, loss = 18.09 (3.2 examples/sec; 4.997 sec/batch)
2018-05-06 12:45:33.950063: step 86390, loss = 17.95 (3.2 examples/sec; 4.951 sec/batch)
2018-05-06 12:46:22.994951: step 86400, loss = 17.71 (3.2 examples/sec; 4.950 sec/batch)
2018-05-06 12:47:15.863541: step 86410, loss = 17.91 (3.2 examples/sec; 5.003 sec/batch)
2018-05-06 12:48:06.140313: step 86420, loss = 17.83 (3.3 examples/sec; 4.803 sec/batch)
2018-05-06 12:48:55.259391: step 86430, loss = 17.97 (3.3 examples/sec; 4.899 sec/batch)
2018-05-06 12:49:44.735155: step 86440, loss = 18.54 (3.1 examples/sec; 5.112 sec/batch)
2018-05-06 12:50:34.516822: step 86450, loss = 18.12 (3.1 examples/sec; 5.102 sec/batch)
2018-05-06 12:51:23.817478: step 86460, loss = 17.99 (3.2 examples/sec; 5.026 sec/batch)
2018-05-06 12:52:13.602849: step 86470, loss = 18.31 (3.2 examples/sec; 5.005 sec/batch)
2018-05-06 12:52:59.725587: step 86480, loss = 17.88 (3.4 examples/sec; 4.689 sec/batch)
2018-05-06 12:53:49.205412: step 86490, loss = 17.82 (3.2 examples/sec; 5.004 sec/batch)
2018-05-06 12:54:38.353331: step 86500, loss = 17.99 (3.4 examples/sec; 4.732 sec/batch)
2018-05-06 12:55:31.245619: step 86510, loss = 17.64 (3.2 examples/sec; 5.064 sec/batch)
2018-05-06 12:56:20.316894: step 86520, loss = 18.14 (3.2 examples/sec; 4.942 sec/batch)
2018-05-06 12:57:10.072614: step 86530, loss = 17.75 (3.2 examples/sec; 4.964 sec/batch)
2018-05-06 12:58:00.775246: step 86540, loss = 18.07 (3.1 examples/sec; 5.128 sec/batch)
2018-05-06 12:58:50.537778: step 86550, loss = 18.17 (3.3 examples/sec; 4.897 sec/batch)
2018-05-06 12:59:39.870017: step 86560, loss = 17.79 (3.2 examples/sec; 4.961 sec/batch)
2018-05-06 13:00:28.880392: step 86570, loss = 17.61 (3.3 examples/sec; 4.909 sec/batch)
2018-05-06 13:01:18.416974: step 86580, loss = 17.85 (3.3 examples/sec; 4.914 sec/batch)
2018-05-06 13:02:08.478768: step 86590, loss = 18.12 (3.2 examples/sec; 4.962 sec/batch)
2018-05-06 13:02:58.072366: step 86600, loss = 18.27 (3.3 examples/sec; 4.874 sec/batch)
2018-05-06 13:03:47.624344: step 86610, loss = 18.16 (3.2 examples/sec; 4.964 sec/batch)
2018-05-06 13:04:36.987333: step 86620, loss = 18.20 (3.2 examples/sec; 4.935 sec/batch)
2018-05-06 13:05:26.090550: step 86630, loss = 18.00 (3.2 examples/sec; 4.992 sec/batch)
2018-05-06 13:06:15.977051: step 86640, loss = 17.93 (3.3 examples/sec; 4.881 sec/batch)
2018-05-06 13:07:05.608223: step 86650, loss = 17.66 (3.3 examples/sec; 4.879 sec/batch)
2018-05-06 13:07:54.713004: step 86660, loss = 18.05 (3.2 examples/sec; 4.941 sec/batch)
2018-05-06 13:08:43.239300: step 86670, loss = 17.96 (3.3 examples/sec; 4.917 sec/batch)
2018-05-06 13:09:32.599352: step 86680, loss = 17.71 (3.3 examples/sec; 4.799 sec/batch)
2018-05-06 13:10:21.523945: step 86690, loss = 18.94 (3.4 examples/sec; 4.750 sec/batch)
2018-05-06 13:11:11.059295: step 86700, loss = 18.24 (3.3 examples/sec; 4.870 sec/batch)
2018-05-06 13:12:04.000726: step 86710, loss = 17.91 (3.2 examples/sec; 4.944 sec/batch)
2018-05-06 13:12:53.279985: step 86720, loss = 17.96 (3.4 examples/sec; 4.667 sec/batch)
2018-05-06 13:13:39.046453: step 86730, loss = 17.87 (4.1 examples/sec; 3.933 sec/batch)
2018-05-06 13:14:28.290183: step 86740, loss = 17.96 (3.2 examples/sec; 5.005 sec/batch)
2018-05-06 13:15:17.390018: step 86750, loss = 17.78 (3.3 examples/sec; 4.864 sec/batch)
2018-05-06 13:16:07.128020: step 86760, loss = 17.63 (3.1 examples/sec; 5.181 sec/batch)
2018-05-06 13:16:56.891324: step 86770, loss = 17.96 (3.2 examples/sec; 5.066 sec/batch)
2018-05-06 13:17:45.941840: step 86780, loss = 17.66 (3.2 examples/sec; 5.043 sec/batch)
2018-05-06 13:18:34.879174: step 86790, loss = 18.01 (3.2 examples/sec; 4.985 sec/batch)
2018-05-06 13:19:24.216670: step 86800, loss = 17.65 (3.3 examples/sec; 4.908 sec/batch)
2018-05-06 13:20:17.069658: step 86810, loss = 17.84 (3.2 examples/sec; 4.951 sec/batch)
2018-05-06 13:21:06.847183: step 86820, loss = 18.12 (3.2 examples/sec; 4.944 sec/batch)
2018-05-06 13:21:56.021013: step 86830, loss = 18.84 (3.4 examples/sec; 4.700 sec/batch)
2018-05-06 13:22:44.622618: step 86840, loss = 18.03 (3.3 examples/sec; 4.839 sec/batch)
2018-05-06 13:23:33.622230: step 86850, loss = 17.95 (3.2 examples/sec; 4.952 sec/batch)
2018-05-06 13:24:19.660948: step 86860, loss = 17.61 (3.2 examples/sec; 4.955 sec/batch)
2018-05-06 13:25:09.068316: step 86870, loss = 18.35 (3.3 examples/sec; 4.851 sec/batch)
2018-05-06 13:25:58.373005: step 86880, loss = 18.02 (3.3 examples/sec; 4.858 sec/batch)
2018-05-06 13:26:47.331087: step 86890, loss = 17.72 (3.2 examples/sec; 4.969 sec/batch)
2018-05-06 13:27:36.499337: step 86900, loss = 17.66 (3.3 examples/sec; 4.877 sec/batch)
2018-05-06 13:28:29.841846: step 86910, loss = 18.79 (3.2 examples/sec; 5.051 sec/batch)
2018-05-06 13:29:19.569040: step 86920, loss = 17.76 (3.3 examples/sec; 4.834 sec/batch)
2018-05-06 13:30:08.817428: step 86930, loss = 18.05 (3.3 examples/sec; 4.901 sec/batch)
2018-05-06 13:30:58.498233: step 86940, loss = 17.89 (3.1 examples/sec; 5.235 sec/batch)
2018-05-06 13:31:47.888586: step 86950, loss = 18.03 (3.2 examples/sec; 4.987 sec/batch)
2018-05-06 13:32:37.752460: step 86960, loss = 18.05 (3.2 examples/sec; 4.970 sec/batch)
2018-05-06 13:33:27.573770: step 86970, loss = 17.94 (3.4 examples/sec; 4.770 sec/batch)
2018-05-06 13:34:15.080422: step 86980, loss = 18.03 (4.2 examples/sec; 3.771 sec/batch)
2018-05-06 13:35:03.666121: step 86990, loss = 17.55 (3.3 examples/sec; 4.885 sec/batch)
2018-05-06 13:35:53.214967: step 87000, loss = 17.91 (3.3 examples/sec; 4.847 sec/batch)
2018-05-06 13:36:46.582507: step 87010, loss = 17.81 (3.4 examples/sec; 4.754 sec/batch)
2018-05-06 13:37:36.140345: step 87020, loss = 17.68 (3.2 examples/sec; 4.933 sec/batch)
2018-05-06 13:38:25.201781: step 87030, loss = 17.92 (3.2 examples/sec; 4.968 sec/batch)
2018-05-06 13:39:14.929339: step 87040, loss = 18.28 (3.3 examples/sec; 4.871 sec/batch)
2018-05-06 13:40:04.717013: step 87050, loss = 17.89 (3.1 examples/sec; 5.139 sec/batch)
2018-05-06 13:40:54.321629: step 87060, loss = 17.82 (3.2 examples/sec; 4.947 sec/batch)
2018-05-06 13:41:44.112525: step 87070, loss = 17.97 (3.2 examples/sec; 4.981 sec/batch)
2018-05-06 13:42:33.433953: step 87080, loss = 18.38 (3.2 examples/sec; 4.928 sec/batch)
2018-05-06 13:43:22.445065: step 87090, loss = 17.58 (3.3 examples/sec; 4.796 sec/batch)
2018-05-06 13:44:12.948855: step 87100, loss = 18.66 (3.2 examples/sec; 4.957 sec/batch)
2018-05-06 13:45:03.115749: step 87110, loss = 17.57 (3.3 examples/sec; 4.913 sec/batch)
2018-05-06 13:45:52.315342: step 87120, loss = 18.10 (3.3 examples/sec; 4.893 sec/batch)
2018-05-06 13:46:41.242646: step 87130, loss = 18.26 (3.3 examples/sec; 4.830 sec/batch)
2018-05-06 13:47:30.655675: step 87140, loss = 17.80 (3.2 examples/sec; 4.936 sec/batch)
2018-05-06 13:48:19.964165: step 87150, loss = 17.98 (3.3 examples/sec; 4.887 sec/batch)
2018-05-06 13:49:09.207470: step 87160, loss = 18.30 (3.3 examples/sec; 4.821 sec/batch)
2018-05-06 13:49:58.580221: step 87170, loss = 17.81 (3.3 examples/sec; 4.903 sec/batch)
2018-05-06 13:50:47.865736: step 87180, loss = 18.00 (3.2 examples/sec; 5.020 sec/batch)
2018-05-06 13:51:36.952849: step 87190, loss = 17.64 (3.3 examples/sec; 4.897 sec/batch)
2018-05-06 13:52:27.082273: step 87200, loss = 17.71 (3.2 examples/sec; 5.007 sec/batch)
2018-05-06 13:53:19.871261: step 87210, loss = 17.81 (3.3 examples/sec; 4.912 sec/batch)
2018-05-06 13:54:09.397809: step 87220, loss = 18.44 (3.3 examples/sec; 4.869 sec/batch)
2018-05-06 13:54:55.660508: step 87230, loss = 17.62 (4.1 examples/sec; 3.888 sec/batch)
2018-05-06 13:55:45.507680: step 87240, loss = 17.98 (3.1 examples/sec; 5.083 sec/batch)
2018-05-06 13:56:34.719280: step 87250, loss = 17.88 (3.2 examples/sec; 4.965 sec/batch)
2018-05-06 13:57:24.163864: step 87260, loss = 17.63 (3.2 examples/sec; 4.999 sec/batch)
2018-05-06 13:58:14.173759: step 87270, loss = 18.64 (3.1 examples/sec; 5.126 sec/batch)
2018-05-06 13:59:03.814391: step 87280, loss = 17.71 (3.2 examples/sec; 4.994 sec/batch)
2018-05-06 13:59:53.270805: step 87290, loss = 17.59 (3.3 examples/sec; 4.883 sec/batch)
2018-05-06 14:00:42.486222: step 87300, loss = 17.58 (3.2 examples/sec; 5.010 sec/batch)
2018-05-06 14:01:35.529098: step 87310, loss = 18.78 (3.2 examples/sec; 4.930 sec/batch)
2018-05-06 14:02:24.798051: step 87320, loss = 18.12 (3.2 examples/sec; 5.032 sec/batch)
2018-05-06 14:03:14.366230: step 87330, loss = 17.95 (3.3 examples/sec; 4.866 sec/batch)
2018-05-06 14:04:04.077202: step 87340, loss = 17.99 (3.2 examples/sec; 4.936 sec/batch)
2018-05-06 14:04:53.959727: step 87350, loss = 17.86 (3.3 examples/sec; 4.919 sec/batch)
2018-05-06 14:05:40.180728: step 87360, loss = 18.15 (3.2 examples/sec; 5.035 sec/batch)
2018-05-06 14:06:29.619991: step 87370, loss = 17.56 (3.3 examples/sec; 4.888 sec/batch)
2018-05-06 14:07:19.053017: step 87380, loss = 18.36 (3.3 examples/sec; 4.820 sec/batch)
2018-05-06 14:08:08.824520: step 87390, loss = 17.92 (3.2 examples/sec; 5.024 sec/batch)
2018-05-06 14:08:58.248784: step 87400, loss = 18.14 (3.2 examples/sec; 4.923 sec/batch)
2018-05-06 14:09:51.569556: step 87410, loss = 17.91 (3.3 examples/sec; 4.896 sec/batch)
2018-05-06 14:10:41.415652: step 87420, loss = 17.69 (3.2 examples/sec; 4.968 sec/batch)
2018-05-06 14:11:30.974230: step 87430, loss = 17.98 (3.3 examples/sec; 4.841 sec/batch)
2018-05-06 14:12:19.956021: step 87440, loss = 18.01 (3.3 examples/sec; 4.841 sec/batch)
2018-05-06 14:13:08.952772: step 87450, loss = 17.78 (3.3 examples/sec; 4.921 sec/batch)
2018-05-06 14:13:59.012263: step 87460, loss = 17.99 (3.1 examples/sec; 5.114 sec/batch)
2018-05-06 14:14:48.579391: step 87470, loss = 17.59 (3.3 examples/sec; 4.883 sec/batch)
2018-05-06 14:15:34.644061: step 87480, loss = 18.71 (3.4 examples/sec; 4.693 sec/batch)
2018-05-06 14:16:24.258115: step 87490, loss = 17.81 (3.2 examples/sec; 5.076 sec/batch)
2018-05-06 14:17:13.645962: step 87500, loss = 18.01 (3.3 examples/sec; 4.886 sec/batch)
2018-05-06 14:18:06.679303: step 87510, loss = 17.63 (3.4 examples/sec; 4.753 sec/batch)
2018-05-06 14:18:56.313614: step 87520, loss = 17.78 (3.2 examples/sec; 4.963 sec/batch)
2018-05-06 14:19:45.380009: step 87530, loss = 17.66 (3.4 examples/sec; 4.740 sec/batch)
2018-05-06 14:20:35.081655: step 87540, loss = 18.09 (3.2 examples/sec; 5.053 sec/batch)
2018-05-06 14:21:25.154647: step 87550, loss = 17.95 (3.2 examples/sec; 5.005 sec/batch)
2018-05-06 14:22:14.740679: step 87560, loss = 18.11 (3.2 examples/sec; 4.975 sec/batch)
2018-05-06 14:23:03.915373: step 87570, loss = 18.24 (3.2 examples/sec; 5.022 sec/batch)
2018-05-06 14:23:53.279003: step 87580, loss = 17.94 (3.3 examples/sec; 4.922 sec/batch)
2018-05-06 14:24:42.584294: step 87590, loss = 18.75 (3.3 examples/sec; 4.853 sec/batch)
2018-05-06 14:25:32.748323: step 87600, loss = 18.04 (3.2 examples/sec; 4.980 sec/batch)
2018-05-06 14:26:22.175626: step 87610, loss = 17.99 (3.2 examples/sec; 5.001 sec/batch)
2018-05-06 14:27:11.741143: step 87620, loss = 18.30 (3.3 examples/sec; 4.875 sec/batch)
2018-05-06 14:28:01.073083: step 87630, loss = 17.72 (3.2 examples/sec; 5.044 sec/batch)
2018-05-06 14:28:50.466472: step 87640, loss = 17.86 (3.2 examples/sec; 4.950 sec/batch)
2018-05-06 14:29:39.986528: step 87650, loss = 17.59 (3.2 examples/sec; 5.028 sec/batch)
2018-05-06 14:30:29.454850: step 87660, loss = 17.77 (3.2 examples/sec; 4.934 sec/batch)
2018-05-06 14:31:18.861426: step 87670, loss = 17.57 (3.2 examples/sec; 5.061 sec/batch)
2018-05-06 14:32:08.808267: step 87680, loss = 17.80 (3.1 examples/sec; 5.198 sec/batch)
2018-05-06 14:32:58.732969: step 87690, loss = 18.49 (3.2 examples/sec; 4.932 sec/batch)
2018-05-06 14:33:48.308491: step 87700, loss = 17.93 (3.2 examples/sec; 5.021 sec/batch)
